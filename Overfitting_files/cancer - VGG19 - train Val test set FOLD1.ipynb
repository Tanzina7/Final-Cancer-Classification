{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23f7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0069a6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import keras\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372b2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model for the dogs vs cats dataset\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# baseline model with dropout for the dogs vs cats dataset\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88c23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "#importing other required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import Sequential\n",
    "from keras.applications import VGG19 #For Transfer Learning\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f262394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651076d9",
   "metadata": {},
   "source": [
    "# fold-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d13ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('pannuke\\\\Fold_1\\\\images\\\\images.npy')\n",
    "y = np.load('pannuke\\\\Fold_1\\\\images\\\\types.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540a9216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2257, 256, 256, 3), (399, 256, 256, 3), (2257,), (399,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xx_train, x_test, yy_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "\n",
    "xx_train.shape, x_test.shape, yy_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a11a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1805, 256, 256, 3), (452, 256, 256, 3), (1805,), (452,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(xx_train, yy_train, test_size=0.20, random_state=42)\n",
    "\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0029c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1805, 256, 256, 3),\n",
       " (452, 256, 256, 3),\n",
       " (399, 256, 256, 3),\n",
       " (1805,),\n",
       " (452,),\n",
       " (399,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a52dfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  9 11 ...  5  3  6]\n",
      "['Breast' 'Liver' 'Ovarian' ... 'Colon' 'Breast' 'Esophagus']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder2 = LabelEncoder()\n",
    "integer_encoded2 = label_encoder2.fit_transform(y_train)\n",
    "print(integer_encoded2)\n",
    "\n",
    "print(y_train)\n",
    "# binary encode\n",
    "onehot_encoder2 = OneHotEncoder(sparse=False)\n",
    "integer_encoded2 = integer_encoded2.reshape(len(integer_encoded2), 1)\n",
    "onehot_encoded2 = onehot_encoder2.fit_transform(integer_encoded2)\n",
    "print(onehot_encoded2)\n",
    "\n",
    "y_train = onehot_encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7b15a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  9  5  3  5  3  3  8  0  2  6  3  3  3  5  6 12 11  5  3  5  0  4  3\n",
      "  8  3 12  5  3  5  3 10  3  3  5  0  6 10 15 13  0  4 15  5  7  3  5  3\n",
      " 11  5  0  3  5  3 16  3  3  3  3  5  5  6  1  0 13  3  3  5  3  3  7  4\n",
      "  3  3  3  9 11  3  3  5  1 17 14  3  5  4  1 18  1  5  1  3  5  4  4  7\n",
      "  5  3  3 17 17  5  3 17  5  6  3  3 11  3  1 13  3  0  3  1  3 18 10  5\n",
      "  3  9  3 12  3 11  3  5  5  1  5  6  6  6  6  4 17  3  3  1  9 11  3  2\n",
      "  3  5 12  3  5  3  5  3 10  3  3 11  1  6  0  2 10  9  6  4  4  5 17  5\n",
      "  3  3  9  3  0  5  5  5 10  3  5  0  3  0 18  3 17  3 12  3  3  5  3  5\n",
      "  3  7  4  5 17 17 12  6 13 13 16  0 16  1 12 11 11  6 10  5  1  3  6  5\n",
      "  4  5  3 16  1  3  3  4  1 17  6  8  5  4  0  1 16  4  6  0 14  3  3 12\n",
      "  5 18  3  6 15  0 14 15 17  3  3  3  3  6  4  6 16  3  3 14  5  3  4  5\n",
      "  6  5  5  3  0  3  0 14  7  5 12  8  5  0 14  3  1  0  3  0  5  3  5  3\n",
      " 15  6  5  6  3  4  5 14  0  5  3  5  9  3  3  0 14  3  1 17 11  3  3  3\n",
      "  4  0  3  1  1  4  5 13  0  3 14  0  3  9  3  3 17  6  5 12  5  5  3  6\n",
      "  4  1 15  3  6  0  5  7  8  5  6  4 10  3  8 13  0  3  4  3  5  6 10  0\n",
      "  3  3  3  3  6  3  0  3  2  5 14  3  1  3 15  3  3  6  3 10  5 13  5  3\n",
      "  3  1  7  3  3  5  0  3 14  7  9  3 12 16  1  3  3 12  3  4  2  3  7 13\n",
      "  5 17  6  4  3  1 10  4  6  3  3 16 12 15  5  3  5  4  4  3 10  4  0  3\n",
      " 10  1  3  1  0  8 10 16  5  3  4 14 14  0 17  6  3  7  6  3]\n",
      "['Breast' 'Liver' 'Colon' 'Breast' 'Colon' 'Breast' 'Breast' 'Kidney'\n",
      " 'Adrenal_gland' 'Bladder' 'Esophagus' 'Breast' 'Breast' 'Breast' 'Colon'\n",
      " 'Esophagus' 'Pancreatic' 'Ovarian' 'Colon' 'Breast' 'Colon'\n",
      " 'Adrenal_gland' 'Cervix' 'Breast' 'Kidney' 'Breast' 'Pancreatic' 'Colon'\n",
      " 'Breast' 'Colon' 'Breast' 'Lung' 'Breast' 'Breast' 'Colon'\n",
      " 'Adrenal_gland' 'Esophagus' 'Lung' 'Stomach' 'Prostate' 'Adrenal_gland'\n",
      " 'Cervix' 'Stomach' 'Colon' 'HeadNeck' 'Breast' 'Colon' 'Breast' 'Ovarian'\n",
      " 'Colon' 'Adrenal_gland' 'Breast' 'Colon' 'Breast' 'Testis' 'Breast'\n",
      " 'Breast' 'Breast' 'Breast' 'Colon' 'Colon' 'Esophagus' 'Bile-duct'\n",
      " 'Adrenal_gland' 'Prostate' 'Breast' 'Breast' 'Colon' 'Breast' 'Breast'\n",
      " 'HeadNeck' 'Cervix' 'Breast' 'Breast' 'Breast' 'Liver' 'Ovarian' 'Breast'\n",
      " 'Breast' 'Colon' 'Bile-duct' 'Thyroid' 'Skin' 'Breast' 'Colon' 'Cervix'\n",
      " 'Bile-duct' 'Uterus' 'Bile-duct' 'Colon' 'Bile-duct' 'Breast' 'Colon'\n",
      " 'Cervix' 'Cervix' 'HeadNeck' 'Colon' 'Breast' 'Breast' 'Thyroid'\n",
      " 'Thyroid' 'Colon' 'Breast' 'Thyroid' 'Colon' 'Esophagus' 'Breast'\n",
      " 'Breast' 'Ovarian' 'Breast' 'Bile-duct' 'Prostate' 'Breast'\n",
      " 'Adrenal_gland' 'Breast' 'Bile-duct' 'Breast' 'Uterus' 'Lung' 'Colon'\n",
      " 'Breast' 'Liver' 'Breast' 'Pancreatic' 'Breast' 'Ovarian' 'Breast'\n",
      " 'Colon' 'Colon' 'Bile-duct' 'Colon' 'Esophagus' 'Esophagus' 'Esophagus'\n",
      " 'Esophagus' 'Cervix' 'Thyroid' 'Breast' 'Breast' 'Bile-duct' 'Liver'\n",
      " 'Ovarian' 'Breast' 'Bladder' 'Breast' 'Colon' 'Pancreatic' 'Breast'\n",
      " 'Colon' 'Breast' 'Colon' 'Breast' 'Lung' 'Breast' 'Breast' 'Ovarian'\n",
      " 'Bile-duct' 'Esophagus' 'Adrenal_gland' 'Bladder' 'Lung' 'Liver'\n",
      " 'Esophagus' 'Cervix' 'Cervix' 'Colon' 'Thyroid' 'Colon' 'Breast' 'Breast'\n",
      " 'Liver' 'Breast' 'Adrenal_gland' 'Colon' 'Colon' 'Colon' 'Lung' 'Breast'\n",
      " 'Colon' 'Adrenal_gland' 'Breast' 'Adrenal_gland' 'Uterus' 'Breast'\n",
      " 'Thyroid' 'Breast' 'Pancreatic' 'Breast' 'Breast' 'Colon' 'Breast'\n",
      " 'Colon' 'Breast' 'HeadNeck' 'Cervix' 'Colon' 'Thyroid' 'Thyroid'\n",
      " 'Pancreatic' 'Esophagus' 'Prostate' 'Prostate' 'Testis' 'Adrenal_gland'\n",
      " 'Testis' 'Bile-duct' 'Pancreatic' 'Ovarian' 'Ovarian' 'Esophagus' 'Lung'\n",
      " 'Colon' 'Bile-duct' 'Breast' 'Esophagus' 'Colon' 'Cervix' 'Colon'\n",
      " 'Breast' 'Testis' 'Bile-duct' 'Breast' 'Breast' 'Cervix' 'Bile-duct'\n",
      " 'Thyroid' 'Esophagus' 'Kidney' 'Colon' 'Cervix' 'Adrenal_gland'\n",
      " 'Bile-duct' 'Testis' 'Cervix' 'Esophagus' 'Adrenal_gland' 'Skin' 'Breast'\n",
      " 'Breast' 'Pancreatic' 'Colon' 'Uterus' 'Breast' 'Esophagus' 'Stomach'\n",
      " 'Adrenal_gland' 'Skin' 'Stomach' 'Thyroid' 'Breast' 'Breast' 'Breast'\n",
      " 'Breast' 'Esophagus' 'Cervix' 'Esophagus' 'Testis' 'Breast' 'Breast'\n",
      " 'Skin' 'Colon' 'Breast' 'Cervix' 'Colon' 'Esophagus' 'Colon' 'Colon'\n",
      " 'Breast' 'Adrenal_gland' 'Breast' 'Adrenal_gland' 'Skin' 'HeadNeck'\n",
      " 'Colon' 'Pancreatic' 'Kidney' 'Colon' 'Adrenal_gland' 'Skin' 'Breast'\n",
      " 'Bile-duct' 'Adrenal_gland' 'Breast' 'Adrenal_gland' 'Colon' 'Breast'\n",
      " 'Colon' 'Breast' 'Stomach' 'Esophagus' 'Colon' 'Esophagus' 'Breast'\n",
      " 'Cervix' 'Colon' 'Skin' 'Adrenal_gland' 'Colon' 'Breast' 'Colon' 'Liver'\n",
      " 'Breast' 'Breast' 'Adrenal_gland' 'Skin' 'Breast' 'Bile-duct' 'Thyroid'\n",
      " 'Ovarian' 'Breast' 'Breast' 'Breast' 'Cervix' 'Adrenal_gland' 'Breast'\n",
      " 'Bile-duct' 'Bile-duct' 'Cervix' 'Colon' 'Prostate' 'Adrenal_gland'\n",
      " 'Breast' 'Skin' 'Adrenal_gland' 'Breast' 'Liver' 'Breast' 'Breast'\n",
      " 'Thyroid' 'Esophagus' 'Colon' 'Pancreatic' 'Colon' 'Colon' 'Breast'\n",
      " 'Esophagus' 'Cervix' 'Bile-duct' 'Stomach' 'Breast' 'Esophagus'\n",
      " 'Adrenal_gland' 'Colon' 'HeadNeck' 'Kidney' 'Colon' 'Esophagus' 'Cervix'\n",
      " 'Lung' 'Breast' 'Kidney' 'Prostate' 'Adrenal_gland' 'Breast' 'Cervix'\n",
      " 'Breast' 'Colon' 'Esophagus' 'Lung' 'Adrenal_gland' 'Breast' 'Breast'\n",
      " 'Breast' 'Breast' 'Esophagus' 'Breast' 'Adrenal_gland' 'Breast' 'Bladder'\n",
      " 'Colon' 'Skin' 'Breast' 'Bile-duct' 'Breast' 'Stomach' 'Breast' 'Breast'\n",
      " 'Esophagus' 'Breast' 'Lung' 'Colon' 'Prostate' 'Colon' 'Breast' 'Breast'\n",
      " 'Bile-duct' 'HeadNeck' 'Breast' 'Breast' 'Colon' 'Adrenal_gland' 'Breast'\n",
      " 'Skin' 'HeadNeck' 'Liver' 'Breast' 'Pancreatic' 'Testis' 'Bile-duct'\n",
      " 'Breast' 'Breast' 'Pancreatic' 'Breast' 'Cervix' 'Bladder' 'Breast'\n",
      " 'HeadNeck' 'Prostate' 'Colon' 'Thyroid' 'Esophagus' 'Cervix' 'Breast'\n",
      " 'Bile-duct' 'Lung' 'Cervix' 'Esophagus' 'Breast' 'Breast' 'Testis'\n",
      " 'Pancreatic' 'Stomach' 'Colon' 'Breast' 'Colon' 'Cervix' 'Cervix'\n",
      " 'Breast' 'Lung' 'Cervix' 'Adrenal_gland' 'Breast' 'Lung' 'Bile-duct'\n",
      " 'Breast' 'Bile-duct' 'Adrenal_gland' 'Kidney' 'Lung' 'Testis' 'Colon'\n",
      " 'Breast' 'Cervix' 'Skin' 'Skin' 'Adrenal_gland' 'Thyroid' 'Esophagus'\n",
      " 'Breast' 'HeadNeck' 'Esophagus' 'Breast']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder2 = LabelEncoder()\n",
    "integer_encoded2 = label_encoder2.fit_transform(y_val)\n",
    "print(integer_encoded2)\n",
    "\n",
    "print(y_val)\n",
    "# binary encode\n",
    "onehot_encoder2 = OneHotEncoder(sparse=False)\n",
    "integer_encoded2 = integer_encoded2.reshape(len(integer_encoded2), 1)\n",
    "onehot_encoded2 = onehot_encoder2.fit_transform(integer_encoded2)\n",
    "print(onehot_encoded2)\n",
    "\n",
    "y_val = onehot_encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4deb48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  3  1  1  3  3  5  5  3  3  0  3  3  4  3  3  3  5  1  4  5  1 10  7\n",
      " 17 10  3  8 11  5  0  3  1  3  5  5  4  3  5  3  5  3  3  1 12 16  3  3\n",
      "  3  5  4  4  8  3  3  3  1  3  5 11  4  1  3 13  3  3  3  5  3 13  5  7\n",
      " 14  6  4  3  5  9  8  5  5  6  1  5  0 15  5  3  3  0  5 15 15 14  3  3\n",
      "  1  6  1  3 11  3 17 10 13  6  3  5  4  5  3  3 16  3  8  5  1  6  5 18\n",
      "  0  6  5 17 10  3  5  3  8  3  2 17  3  1  3  3  3 12  1  3 11  5 13  1\n",
      "  7  4  3  4  7 14  5 14  3 15  6  8  5  3 17 15  1  5  5 17 13  5  3  5\n",
      "  5  3  3  3 16  3  6  5  1  3  5  0  4  4  0  9  4  6 14  3  5  5  3  4\n",
      "  0  5  0  3  7 15  0  5  3 11  5  5 10  3  3  3  5  9  2  1  3 17  5 16\n",
      "  9  1 15 11  5 14  5  3  3 14  1 12  6  3  6  5  0  4 10  3  3  3  1  4\n",
      "  5 17 13  5  6  7 17 17 13  3 13 10  3 13 15  7  5  9 12  6  5  5 16 10\n",
      "  3 12  3  3  7  3  2  3 16  1 14  3  3 16 11 17  5  3  4  4  5  5  5  5\n",
      "  3  3  3  4  3  3  1  6  1 17  3  9 14  6 13  3  7 12  0  5  3  3  3  5\n",
      "  1 18 10 12  3  0  6  0  5  5  3 12 12  3  3  7  6  3 17 10  3  3  5  3\n",
      "  5  6  5  1 13  5  3  3  3  7  5  5  5  0  9 13  5  5  3  3  3 11  3  3\n",
      "  1  3  5  5  3 12 12  0  4  3 13  5  3  3  1  8  1  1  5  1  5  3 10  3\n",
      " 16  1  0 10  3  3  3  4  3  5  3  1  3  9 17]\n",
      "['Colon' 'Breast' 'Bile-duct' 'Bile-duct' 'Breast' 'Breast' 'Colon'\n",
      " 'Colon' 'Breast' 'Breast' 'Adrenal_gland' 'Breast' 'Breast' 'Cervix'\n",
      " 'Breast' 'Breast' 'Breast' 'Colon' 'Bile-duct' 'Cervix' 'Colon'\n",
      " 'Bile-duct' 'Lung' 'HeadNeck' 'Thyroid' 'Lung' 'Breast' 'Kidney'\n",
      " 'Ovarian' 'Colon' 'Adrenal_gland' 'Breast' 'Bile-duct' 'Breast' 'Colon'\n",
      " 'Colon' 'Cervix' 'Breast' 'Colon' 'Breast' 'Colon' 'Breast' 'Breast'\n",
      " 'Bile-duct' 'Pancreatic' 'Testis' 'Breast' 'Breast' 'Breast' 'Colon'\n",
      " 'Cervix' 'Cervix' 'Kidney' 'Breast' 'Breast' 'Breast' 'Bile-duct'\n",
      " 'Breast' 'Colon' 'Ovarian' 'Cervix' 'Bile-duct' 'Breast' 'Prostate'\n",
      " 'Breast' 'Breast' 'Breast' 'Colon' 'Breast' 'Prostate' 'Colon' 'HeadNeck'\n",
      " 'Skin' 'Esophagus' 'Cervix' 'Breast' 'Colon' 'Liver' 'Kidney' 'Colon'\n",
      " 'Colon' 'Esophagus' 'Bile-duct' 'Colon' 'Adrenal_gland' 'Stomach' 'Colon'\n",
      " 'Breast' 'Breast' 'Adrenal_gland' 'Colon' 'Stomach' 'Stomach' 'Skin'\n",
      " 'Breast' 'Breast' 'Bile-duct' 'Esophagus' 'Bile-duct' 'Breast' 'Ovarian'\n",
      " 'Breast' 'Thyroid' 'Lung' 'Prostate' 'Esophagus' 'Breast' 'Colon'\n",
      " 'Cervix' 'Colon' 'Breast' 'Breast' 'Testis' 'Breast' 'Kidney' 'Colon'\n",
      " 'Bile-duct' 'Esophagus' 'Colon' 'Uterus' 'Adrenal_gland' 'Esophagus'\n",
      " 'Colon' 'Thyroid' 'Lung' 'Breast' 'Colon' 'Breast' 'Kidney' 'Breast'\n",
      " 'Bladder' 'Thyroid' 'Breast' 'Bile-duct' 'Breast' 'Breast' 'Breast'\n",
      " 'Pancreatic' 'Bile-duct' 'Breast' 'Ovarian' 'Colon' 'Prostate'\n",
      " 'Bile-duct' 'HeadNeck' 'Cervix' 'Breast' 'Cervix' 'HeadNeck' 'Skin'\n",
      " 'Colon' 'Skin' 'Breast' 'Stomach' 'Esophagus' 'Kidney' 'Colon' 'Breast'\n",
      " 'Thyroid' 'Stomach' 'Bile-duct' 'Colon' 'Colon' 'Thyroid' 'Prostate'\n",
      " 'Colon' 'Breast' 'Colon' 'Colon' 'Breast' 'Breast' 'Breast' 'Testis'\n",
      " 'Breast' 'Esophagus' 'Colon' 'Bile-duct' 'Breast' 'Colon' 'Adrenal_gland'\n",
      " 'Cervix' 'Cervix' 'Adrenal_gland' 'Liver' 'Cervix' 'Esophagus' 'Skin'\n",
      " 'Breast' 'Colon' 'Colon' 'Breast' 'Cervix' 'Adrenal_gland' 'Colon'\n",
      " 'Adrenal_gland' 'Breast' 'HeadNeck' 'Stomach' 'Adrenal_gland' 'Colon'\n",
      " 'Breast' 'Ovarian' 'Colon' 'Colon' 'Lung' 'Breast' 'Breast' 'Breast'\n",
      " 'Colon' 'Liver' 'Bladder' 'Bile-duct' 'Breast' 'Thyroid' 'Colon' 'Testis'\n",
      " 'Liver' 'Bile-duct' 'Stomach' 'Ovarian' 'Colon' 'Skin' 'Colon' 'Breast'\n",
      " 'Breast' 'Skin' 'Bile-duct' 'Pancreatic' 'Esophagus' 'Breast' 'Esophagus'\n",
      " 'Colon' 'Adrenal_gland' 'Cervix' 'Lung' 'Breast' 'Breast' 'Breast'\n",
      " 'Bile-duct' 'Cervix' 'Colon' 'Thyroid' 'Prostate' 'Colon' 'Esophagus'\n",
      " 'HeadNeck' 'Thyroid' 'Thyroid' 'Prostate' 'Breast' 'Prostate' 'Lung'\n",
      " 'Breast' 'Prostate' 'Stomach' 'HeadNeck' 'Colon' 'Liver' 'Pancreatic'\n",
      " 'Esophagus' 'Colon' 'Colon' 'Testis' 'Lung' 'Breast' 'Pancreatic'\n",
      " 'Breast' 'Breast' 'HeadNeck' 'Breast' 'Bladder' 'Breast' 'Testis'\n",
      " 'Bile-duct' 'Skin' 'Breast' 'Breast' 'Testis' 'Ovarian' 'Thyroid' 'Colon'\n",
      " 'Breast' 'Cervix' 'Cervix' 'Colon' 'Colon' 'Colon' 'Colon' 'Breast'\n",
      " 'Breast' 'Breast' 'Cervix' 'Breast' 'Breast' 'Bile-duct' 'Esophagus'\n",
      " 'Bile-duct' 'Thyroid' 'Breast' 'Liver' 'Skin' 'Esophagus' 'Prostate'\n",
      " 'Breast' 'HeadNeck' 'Pancreatic' 'Adrenal_gland' 'Colon' 'Breast'\n",
      " 'Breast' 'Breast' 'Colon' 'Bile-duct' 'Uterus' 'Lung' 'Pancreatic'\n",
      " 'Breast' 'Adrenal_gland' 'Esophagus' 'Adrenal_gland' 'Colon' 'Colon'\n",
      " 'Breast' 'Pancreatic' 'Pancreatic' 'Breast' 'Breast' 'HeadNeck'\n",
      " 'Esophagus' 'Breast' 'Thyroid' 'Lung' 'Breast' 'Breast' 'Colon' 'Breast'\n",
      " 'Colon' 'Esophagus' 'Colon' 'Bile-duct' 'Prostate' 'Colon' 'Breast'\n",
      " 'Breast' 'Breast' 'HeadNeck' 'Colon' 'Colon' 'Colon' 'Adrenal_gland'\n",
      " 'Liver' 'Prostate' 'Colon' 'Colon' 'Breast' 'Breast' 'Breast' 'Ovarian'\n",
      " 'Breast' 'Breast' 'Bile-duct' 'Breast' 'Colon' 'Colon' 'Breast'\n",
      " 'Pancreatic' 'Pancreatic' 'Adrenal_gland' 'Cervix' 'Breast' 'Prostate'\n",
      " 'Colon' 'Breast' 'Breast' 'Bile-duct' 'Kidney' 'Bile-duct' 'Bile-duct'\n",
      " 'Colon' 'Bile-duct' 'Colon' 'Breast' 'Lung' 'Breast' 'Testis' 'Bile-duct'\n",
      " 'Adrenal_gland' 'Lung' 'Breast' 'Breast' 'Breast' 'Cervix' 'Breast'\n",
      " 'Colon' 'Breast' 'Bile-duct' 'Breast' 'Liver' 'Thyroid']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder2 = LabelEncoder()\n",
    "integer_encoded2 = label_encoder2.fit_transform(y_test)\n",
    "print(integer_encoded2)\n",
    "\n",
    "print(y_test)\n",
    "# binary encode\n",
    "onehot_encoder2 = OneHotEncoder(sparse=False)\n",
    "integer_encoded2 = integer_encoded2.reshape(len(integer_encoded2), 1)\n",
    "onehot_encoded2 = onehot_encoder2.fit_transform(integer_encoded2)\n",
    "print(onehot_encoded2)\n",
    "\n",
    "y_test = onehot_encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "843b2d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1805, 256, 256, 3),\n",
       " (452, 256, 256, 3),\n",
       " (399, 256, 256, 3),\n",
       " (1805, 19),\n",
       " (452, 19),\n",
       " (399, 19))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4062d95",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc183e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562637f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 256, 3)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(19, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# define model\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3bfe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2031 samples, validate on 359 samples\n",
      "Epoch 1/20\n",
      "2031/2031 [==============================] - 9s 4ms/step - loss: 20521.7386 - accuracy: 0.2787 - val_loss: 2.9160 - val_accuracy: 0.3287\n",
      "Epoch 2/20\n",
      "2031/2031 [==============================] - 5s 2ms/step - loss: 2.8952 - accuracy: 0.3127 - val_loss: 2.8689 - val_accuracy: 0.3287\n",
      "Epoch 3/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.8502 - accuracy: 0.3127 - val_loss: 2.8228 - val_accuracy: 0.3287\n",
      "Epoch 4/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.8071 - accuracy: 0.3127 - val_loss: 2.7779 - val_accuracy: 0.3287\n",
      "Epoch 5/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.7620 - accuracy: 0.3127 - val_loss: 2.7295 - val_accuracy: 0.3287\n",
      "Epoch 6/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.7166 - accuracy: 0.3127 - val_loss: 2.6835 - val_accuracy: 0.3287\n",
      "Epoch 7/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.6672 - accuracy: 0.3127 - val_loss: 2.6301 - val_accuracy: 0.3287\n",
      "Epoch 8/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.6179 - accuracy: 0.3127 - val_loss: 2.5820 - val_accuracy: 0.3287\n",
      "Epoch 9/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.5757 - accuracy: 0.3127 - val_loss: 2.5425 - val_accuracy: 0.3287\n",
      "Epoch 10/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.5410 - accuracy: 0.3127 - val_loss: 2.5095 - val_accuracy: 0.3287\n",
      "Epoch 11/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.5124 - accuracy: 0.3127 - val_loss: 2.4823 - val_accuracy: 0.3287\n",
      "Epoch 12/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.4874 - accuracy: 0.3131 - val_loss: 2.4613 - val_accuracy: 0.3287\n",
      "Epoch 13/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.4688 - accuracy: 0.3131 - val_loss: 2.4429 - val_accuracy: 0.3287\n",
      "Epoch 14/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.4526 - accuracy: 0.3131 - val_loss: 2.4295 - val_accuracy: 0.3287\n",
      "Epoch 15/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.4383 - accuracy: 0.3131 - val_loss: 2.4133 - val_accuracy: 0.3287\n",
      "Epoch 16/20\n",
      "2031/2031 [==============================] - 5s 2ms/step - loss: 2.4221 - accuracy: 0.3131 - val_loss: 2.3881 - val_accuracy: 0.3287\n",
      "Epoch 17/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.3869 - accuracy: 0.3161 - val_loss: 2.3441 - val_accuracy: 0.3315\n",
      "Epoch 18/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.4057 - accuracy: 0.3220 - val_loss: 2.3991 - val_accuracy: 0.3287\n",
      "Epoch 19/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.4176 - accuracy: 0.3127 - val_loss: 2.3940 - val_accuracy: 0.3287\n",
      "Epoch 20/20\n",
      "2031/2031 [==============================] - 5s 3ms/step - loss: 2.4131 - accuracy: 0.3127 - val_loss: 2.3900 - val_accuracy: 0.3287\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=20, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea323ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - 1s 2ms/step\n",
      "Evaluation result on Test Data : Loss = 2.4577671997529222, accuracy = 0.27819550037384033\n"
     ]
    }
   ],
   "source": [
    "[test_loss, test_acc] = model.evaluate(x_test, y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d7b4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - 0s 1ms/step\n",
      "> 27.820\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e710e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53758595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2031, 256, 256, 3),\n",
       " (359, 256, 256, 3),\n",
       " (266, 256, 256, 3),\n",
       " (2031, 19),\n",
       " (359, 19),\n",
       " (266, 19))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25116477",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd8fa26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2031 samples, validate on 359 samples\n",
      "Epoch 1/20\n",
      "2031/2031 [==============================] - 41s 20ms/step - loss: 6.1076 - accuracy: 0.2595 - val_loss: 2.5444 - val_accuracy: 0.3677\n",
      "Epoch 2/20\n",
      "2031/2031 [==============================] - 22s 11ms/step - loss: 2.0229 - accuracy: 0.4717 - val_loss: 2.1320 - val_accuracy: 0.4457\n",
      "Epoch 3/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 1.3201 - accuracy: 0.6027 - val_loss: 2.0110 - val_accuracy: 0.4540\n",
      "Epoch 4/20\n",
      "2031/2031 [==============================] - 22s 11ms/step - loss: 0.9662 - accuracy: 0.7036 - val_loss: 1.9896 - val_accuracy: 0.4763\n",
      "Epoch 5/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.7315 - accuracy: 0.7770 - val_loss: 1.9686 - val_accuracy: 0.4986\n",
      "Epoch 6/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.5678 - accuracy: 0.8380 - val_loss: 1.9760 - val_accuracy: 0.4958\n",
      "Epoch 7/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.4539 - accuracy: 0.8680 - val_loss: 1.9820 - val_accuracy: 0.5125\n",
      "Epoch 8/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.3843 - accuracy: 0.8941 - val_loss: 1.9926 - val_accuracy: 0.5125\n",
      "Epoch 9/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.3169 - accuracy: 0.9257 - val_loss: 2.0170 - val_accuracy: 0.5237\n",
      "Epoch 10/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.2569 - accuracy: 0.9404 - val_loss: 2.0123 - val_accuracy: 0.5292\n",
      "Epoch 11/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.2146 - accuracy: 0.9567 - val_loss: 2.0448 - val_accuracy: 0.5265\n",
      "Epoch 12/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.2081 - accuracy: 0.9562 - val_loss: 2.0538 - val_accuracy: 0.5265\n",
      "Epoch 13/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1819 - accuracy: 0.9641 - val_loss: 2.0790 - val_accuracy: 0.5348\n",
      "Epoch 14/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1773 - accuracy: 0.9596 - val_loss: 2.0699 - val_accuracy: 0.5265\n",
      "Epoch 15/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1433 - accuracy: 0.9769 - val_loss: 2.0871 - val_accuracy: 0.5209\n",
      "Epoch 16/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1327 - accuracy: 0.9744 - val_loss: 2.0762 - val_accuracy: 0.5209\n",
      "Epoch 17/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1236 - accuracy: 0.9783 - val_loss: 2.1176 - val_accuracy: 0.5209\n",
      "Epoch 18/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1302 - accuracy: 0.9749 - val_loss: 2.0785 - val_accuracy: 0.5320\n",
      "Epoch 19/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1060 - accuracy: 0.9823 - val_loss: 2.0818 - val_accuracy: 0.5292\n",
      "Epoch 20/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1026 - accuracy: 0.9833 - val_loss: 2.1015 - val_accuracy: 0.5237\n",
      "266/266 [==============================] - 8s 30ms/step\n",
      "> 50.000\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(256, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass2 = Dense(128, activation='relu')(class1)\n",
    "\tclass3 = Dropout(0.2)(class2)\n",
    "\toutput = Dense(19, activation='softmax')(class3)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=64, epochs=20, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6270e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot.inputs.keras import PlotLossesCallback\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "# ModelCheckpoint callback - save best weights\n",
    "tl_checkpoint_1 = ModelCheckpoint(filepath='newest model.h5',\n",
    "                                  save_best_only=True,\n",
    "                                  mode='max',\n",
    "                                  verbose=1)\n",
    "\n",
    "# EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef3d20cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "\tvalidation       \t (min:    0.357, max:    0.504, cur:    0.499)\n",
      "\ttraining         \t (min:    0.258, max:    0.948, cur:    0.948)\n",
      "Loss\n",
      "\tvalidation       \t (min:    2.165, max:    2.689, cur:    2.330)\n",
      "\ttraining         \t (min:    0.223, max:    5.654, cur:    0.223)\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=64, epochs=20, verbose=1, validation_data=(x_val, y_val), \n",
    "                        callbacks=[tl_checkpoint_1, early_stop, plot_loss_1])\n",
    "\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eff25fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 3s 10ms/sample - loss: 2.6315 - accuracy: 0.3383\n",
      "> 33.835\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('newest model.h5')\n",
    "\n",
    "# evaluate model\n",
    "_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e85172fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint('newest model2.h5', monitor='accuracy', \n",
    "                                                save_best_only=True, mode='max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "112508cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2031 samples, validate on 359 samples\n",
      "Epoch 1/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 6.2467 - accuracy: 0.2629 - val_loss: 2.4685 - val_accuracy: 0.3955\n",
      "Epoch 2/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 2.0864 - accuracy: 0.4594 - val_loss: 2.0040 - val_accuracy: 0.4652\n",
      "Epoch 3/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 1.4284 - accuracy: 0.5771 - val_loss: 1.9642 - val_accuracy: 0.4652\n",
      "Epoch 4/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 1.0311 - accuracy: 0.6745 - val_loss: 1.9430 - val_accuracy: 0.4735\n",
      "Epoch 5/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.7973 - accuracy: 0.7597 - val_loss: 1.9283 - val_accuracy: 0.4903\n",
      "Epoch 6/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.6326 - accuracy: 0.8144 - val_loss: 1.9261 - val_accuracy: 0.5070\n",
      "Epoch 7/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.5013 - accuracy: 0.8602 - val_loss: 1.9340 - val_accuracy: 0.5209\n",
      "Epoch 8/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.4278 - accuracy: 0.8927 - val_loss: 1.9367 - val_accuracy: 0.5125\n",
      "Epoch 9/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.3496 - accuracy: 0.9163 - val_loss: 1.9879 - val_accuracy: 0.5042\n",
      "Epoch 10/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.3080 - accuracy: 0.9266 - val_loss: 1.9507 - val_accuracy: 0.5153\n",
      "Epoch 11/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.2447 - accuracy: 0.9488 - val_loss: 1.9946 - val_accuracy: 0.5042\n",
      "Epoch 12/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.2118 - accuracy: 0.9552 - val_loss: 1.9710 - val_accuracy: 0.5209\n",
      "Epoch 13/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.2065 - accuracy: 0.9557 - val_loss: 1.9918 - val_accuracy: 0.5153\n",
      "Epoch 14/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1737 - accuracy: 0.9714 - val_loss: 2.0036 - val_accuracy: 0.5097\n",
      "Epoch 15/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1650 - accuracy: 0.9650 - val_loss: 2.0203 - val_accuracy: 0.4986\n",
      "Epoch 16/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1437 - accuracy: 0.9724 - val_loss: 2.0257 - val_accuracy: 0.5014\n",
      "Epoch 17/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1360 - accuracy: 0.9744 - val_loss: 2.0186 - val_accuracy: 0.5209\n",
      "Epoch 18/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1322 - accuracy: 0.9759 - val_loss: 2.0313 - val_accuracy: 0.5181\n",
      "Epoch 19/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1144 - accuracy: 0.9803 - val_loss: 2.0677 - val_accuracy: 0.5209\n",
      "Epoch 20/20\n",
      "2031/2031 [==============================] - 23s 11ms/step - loss: 0.1091 - accuracy: 0.9828 - val_loss: 2.0532 - val_accuracy: 0.5237\n"
     ]
    }
   ],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=64, epochs=20, verbose=1, validation_data=(x_val, y_val), \n",
    "                        callbacks=callbacks)\n",
    "\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329c907b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 9ms/sample - loss: 2.6708 - accuracy: 0.4699\n",
      "> 46.992\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('newest model2.h5')\n",
    "\n",
    "# evaluate model\n",
    "_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0424b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a554cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint('newest model5.h5', monitor='accuracy', \n",
    "                                                save_best_only=True, mode='max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4605a96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 34s 19ms/step - loss: 7.0163 - accuracy: 0.1967 - val_loss: 2.2175 - val_accuracy: 0.3982\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 3.2280 - accuracy: 0.2953 - val_loss: 2.1051 - val_accuracy: 0.4226\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 2.5000 - accuracy: 0.3634 - val_loss: 1.9893 - val_accuracy: 0.4469\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 2.1314 - accuracy: 0.4144 - val_loss: 1.8655 - val_accuracy: 0.4558\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.9185 - accuracy: 0.4537 - val_loss: 1.8322 - val_accuracy: 0.4934\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.8133 - accuracy: 0.4787 - val_loss: 1.8122 - val_accuracy: 0.4624\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.7247 - accuracy: 0.5058 - val_loss: 1.7779 - val_accuracy: 0.4757\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.5730 - accuracy: 0.5402 - val_loss: 1.7417 - val_accuracy: 0.4912\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.5199 - accuracy: 0.5551 - val_loss: 1.7297 - val_accuracy: 0.4956\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.3947 - accuracy: 0.5856 - val_loss: 1.7328 - val_accuracy: 0.4757\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.3247 - accuracy: 0.6066 - val_loss: 1.6946 - val_accuracy: 0.4889\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.2567 - accuracy: 0.6227 - val_loss: 1.6728 - val_accuracy: 0.4978\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.1988 - accuracy: 0.6488 - val_loss: 1.7002 - val_accuracy: 0.4889\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.1669 - accuracy: 0.6493 - val_loss: 1.6533 - val_accuracy: 0.5199\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.1234 - accuracy: 0.6626 - val_loss: 1.6737 - val_accuracy: 0.4823\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.0130 - accuracy: 0.6864 - val_loss: 1.6263 - val_accuracy: 0.5022\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9673 - accuracy: 0.6920 - val_loss: 1.6315 - val_accuracy: 0.5177\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9248 - accuracy: 0.7064 - val_loss: 1.6635 - val_accuracy: 0.5111\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9359 - accuracy: 0.7219 - val_loss: 1.6537 - val_accuracy: 0.5044\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.8355 - accuracy: 0.7479 - val_loss: 1.6320 - val_accuracy: 0.5199\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.8126 - accuracy: 0.7446 - val_loss: 1.6226 - val_accuracy: 0.5243\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.7379 - accuracy: 0.7729 - val_loss: 1.6455 - val_accuracy: 0.5199\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.7216 - accuracy: 0.7734 - val_loss: 1.6606 - val_accuracy: 0.5111\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6470 - accuracy: 0.7978 - val_loss: 1.6855 - val_accuracy: 0.5288\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6394 - accuracy: 0.8028 - val_loss: 1.6283 - val_accuracy: 0.5265\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5694 - accuracy: 0.8271 - val_loss: 1.6415 - val_accuracy: 0.5310\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5809 - accuracy: 0.8305 - val_loss: 1.6354 - val_accuracy: 0.5111\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5542 - accuracy: 0.8288 - val_loss: 1.6285 - val_accuracy: 0.5199\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.4900 - accuracy: 0.8554 - val_loss: 1.6354 - val_accuracy: 0.5243\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.4664 - accuracy: 0.8560 - val_loss: 1.6750 - val_accuracy: 0.5265\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.4583 - accuracy: 0.8537 - val_loss: 1.6434 - val_accuracy: 0.5265\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.4426 - accuracy: 0.8632 - val_loss: 1.6483 - val_accuracy: 0.5133\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.4095 - accuracy: 0.8765 - val_loss: 1.6348 - val_accuracy: 0.5310\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.4206 - accuracy: 0.8709 - val_loss: 1.6562 - val_accuracy: 0.5310\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.3773 - accuracy: 0.8859 - val_loss: 1.6872 - val_accuracy: 0.5221\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.3434 - accuracy: 0.8997 - val_loss: 1.7029 - val_accuracy: 0.5376\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.3634 - accuracy: 0.8864 - val_loss: 1.6915 - val_accuracy: 0.5310\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.3085 - accuracy: 0.9125 - val_loss: 1.7049 - val_accuracy: 0.5376\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.3256 - accuracy: 0.9030 - val_loss: 1.7219 - val_accuracy: 0.5310\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.3137 - accuracy: 0.9030 - val_loss: 1.7101 - val_accuracy: 0.5398\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2731 - accuracy: 0.9152 - val_loss: 1.7149 - val_accuracy: 0.5155\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2520 - accuracy: 0.9291 - val_loss: 1.7170 - val_accuracy: 0.5310\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2619 - accuracy: 0.9175 - val_loss: 1.7201 - val_accuracy: 0.5465\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2596 - accuracy: 0.9219 - val_loss: 1.7832 - val_accuracy: 0.5310\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2055 - accuracy: 0.9424 - val_loss: 1.7255 - val_accuracy: 0.5442\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2250 - accuracy: 0.9269 - val_loss: 1.7173 - val_accuracy: 0.5509\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2055 - accuracy: 0.9452 - val_loss: 1.7406 - val_accuracy: 0.5509\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2208 - accuracy: 0.9374 - val_loss: 1.7392 - val_accuracy: 0.5509\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1939 - accuracy: 0.9435 - val_loss: 1.7553 - val_accuracy: 0.5442\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.2005 - accuracy: 0.9352 - val_loss: 1.7772 - val_accuracy: 0.5420\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1619 - accuracy: 0.9540 - val_loss: 1.7652 - val_accuracy: 0.5575\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1735 - accuracy: 0.9468 - val_loss: 1.7519 - val_accuracy: 0.5442\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1780 - accuracy: 0.9446 - val_loss: 1.7534 - val_accuracy: 0.5398\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1566 - accuracy: 0.9557 - val_loss: 1.7339 - val_accuracy: 0.5420\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1569 - accuracy: 0.9529 - val_loss: 1.7782 - val_accuracy: 0.5465\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1544 - accuracy: 0.9579 - val_loss: 1.7757 - val_accuracy: 0.5442\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1524 - accuracy: 0.9562 - val_loss: 1.7776 - val_accuracy: 0.5575\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1463 - accuracy: 0.9557 - val_loss: 1.7481 - val_accuracy: 0.5619\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1419 - accuracy: 0.9573 - val_loss: 1.7489 - val_accuracy: 0.5597\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1265 - accuracy: 0.9695 - val_loss: 1.8158 - val_accuracy: 0.5442\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1212 - accuracy: 0.9629 - val_loss: 1.8324 - val_accuracy: 0.5531\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1233 - accuracy: 0.9612 - val_loss: 1.7582 - val_accuracy: 0.5509\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1193 - accuracy: 0.9657 - val_loss: 1.7659 - val_accuracy: 0.5531\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1191 - accuracy: 0.9651 - val_loss: 1.8098 - val_accuracy: 0.5553\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1160 - accuracy: 0.9673 - val_loss: 1.8376 - val_accuracy: 0.5509\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1012 - accuracy: 0.9756 - val_loss: 1.8576 - val_accuracy: 0.5509\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1172 - accuracy: 0.9645 - val_loss: 1.7632 - val_accuracy: 0.5509\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1119 - accuracy: 0.9651 - val_loss: 1.8437 - val_accuracy: 0.5243\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0863 - accuracy: 0.9773 - val_loss: 1.7971 - val_accuracy: 0.5509\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0804 - accuracy: 0.9773 - val_loss: 1.8559 - val_accuracy: 0.5509\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0978 - accuracy: 0.9717 - val_loss: 1.8500 - val_accuracy: 0.5531\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0950 - accuracy: 0.9762 - val_loss: 1.8890 - val_accuracy: 0.5531\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0826 - accuracy: 0.9801 - val_loss: 1.8724 - val_accuracy: 0.5487\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0724 - accuracy: 0.9795 - val_loss: 1.9250 - val_accuracy: 0.5575\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0955 - accuracy: 0.9690 - val_loss: 1.8446 - val_accuracy: 0.5752\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1005 - accuracy: 0.9695 - val_loss: 1.8986 - val_accuracy: 0.5575\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0890 - accuracy: 0.9734 - val_loss: 1.9376 - val_accuracy: 0.5420\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0850 - accuracy: 0.9751 - val_loss: 1.9393 - val_accuracy: 0.5465\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.1050 - accuracy: 0.9657 - val_loss: 1.9071 - val_accuracy: 0.5531\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0721 - accuracy: 0.9767 - val_loss: 1.9019 - val_accuracy: 0.5442\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0754 - accuracy: 0.9762 - val_loss: 1.9900 - val_accuracy: 0.5465\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0866 - accuracy: 0.9756 - val_loss: 1.9025 - val_accuracy: 0.5420\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 1.8203 - val_accuracy: 0.5575\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0740 - accuracy: 0.9795 - val_loss: 1.8794 - val_accuracy: 0.5575\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0735 - accuracy: 0.9801 - val_loss: 1.8912 - val_accuracy: 0.5509\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0607 - accuracy: 0.9856 - val_loss: 1.8855 - val_accuracy: 0.5442\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0556 - accuracy: 0.9867 - val_loss: 1.9510 - val_accuracy: 0.5531\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0633 - accuracy: 0.9784 - val_loss: 1.9020 - val_accuracy: 0.5487\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0761 - accuracy: 0.9740 - val_loss: 1.9661 - val_accuracy: 0.5487\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0553 - accuracy: 0.9828 - val_loss: 1.9587 - val_accuracy: 0.5398\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0583 - accuracy: 0.9828 - val_loss: 1.8935 - val_accuracy: 0.5708\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0582 - accuracy: 0.9828 - val_loss: 1.9605 - val_accuracy: 0.5531\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0655 - accuracy: 0.9806 - val_loss: 1.9323 - val_accuracy: 0.5642\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0615 - accuracy: 0.9806 - val_loss: 2.0055 - val_accuracy: 0.5465\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0473 - accuracy: 0.9850 - val_loss: 2.0132 - val_accuracy: 0.5465\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0567 - accuracy: 0.9817 - val_loss: 1.9564 - val_accuracy: 0.5509\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0470 - accuracy: 0.9856 - val_loss: 1.9595 - val_accuracy: 0.5509\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0461 - accuracy: 0.9856 - val_loss: 1.9565 - val_accuracy: 0.5597\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0561 - accuracy: 0.9828 - val_loss: 1.9614 - val_accuracy: 0.5442\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.0489 - accuracy: 0.9850 - val_loss: 1.9223 - val_accuracy: 0.5575\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(2048, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.5)(class1)        \n",
    "\tclass1 = Dense(512, activation='relu')(class1)   \n",
    "\tclass1 = Dense(256, activation='relu')(class1)   \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val), callbacks=callbacks)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b5060fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "399/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 6s 16ms/sample - loss: 1.8491 - accuracy: 0.5514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 55.138\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('newest model5.h5')\n",
    "\n",
    "# evaluate model\n",
    "_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d0726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f16a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/20\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 5.6717 - accuracy: 0.2216 - val_loss: 2.4738 - val_accuracy: 0.3916\n",
      "Epoch 2/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 2.4392 - accuracy: 0.3994 - val_loss: 2.0462 - val_accuracy: 0.4292\n",
      "Epoch 3/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.8739 - accuracy: 0.4848 - val_loss: 1.9318 - val_accuracy: 0.4602\n",
      "Epoch 4/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.5860 - accuracy: 0.5313 - val_loss: 1.8710 - val_accuracy: 0.4845\n",
      "Epoch 5/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.3590 - accuracy: 0.5767 - val_loss: 1.8364 - val_accuracy: 0.4757\n",
      "Epoch 6/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.1574 - accuracy: 0.6443 - val_loss: 1.8759 - val_accuracy: 0.4801\n",
      "Epoch 7/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.9829 - accuracy: 0.6914 - val_loss: 1.8401 - val_accuracy: 0.4735\n",
      "Epoch 8/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.9201 - accuracy: 0.7130 - val_loss: 1.7981 - val_accuracy: 0.4845\n",
      "Epoch 9/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.8057 - accuracy: 0.7540 - val_loss: 1.7636 - val_accuracy: 0.4934\n",
      "Epoch 10/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.7299 - accuracy: 0.7756 - val_loss: 1.8197 - val_accuracy: 0.5022\n",
      "Epoch 11/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.6709 - accuracy: 0.7839 - val_loss: 1.8331 - val_accuracy: 0.5177\n",
      "Epoch 12/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.5893 - accuracy: 0.8222 - val_loss: 1.8216 - val_accuracy: 0.5133\n",
      "Epoch 13/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.5357 - accuracy: 0.8332 - val_loss: 1.7928 - val_accuracy: 0.5133\n",
      "Epoch 14/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.5205 - accuracy: 0.8366 - val_loss: 1.8146 - val_accuracy: 0.5022\n",
      "Epoch 15/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.4586 - accuracy: 0.8576 - val_loss: 1.7994 - val_accuracy: 0.5155\n",
      "Epoch 16/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.4016 - accuracy: 0.8831 - val_loss: 1.8165 - val_accuracy: 0.5243\n",
      "Epoch 17/20\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4220 - accuracy: 0.8731 - val_loss: 1.8029 - val_accuracy: 0.5088\n",
      "Epoch 18/20\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.3505 - accuracy: 0.8920 - val_loss: 1.8326 - val_accuracy: 0.5199\n",
      "Epoch 19/20\n",
      "1805/1805 [==============================] - 20s 11ms/step - loss: 0.3511 - accuracy: 0.8920 - val_loss: 1.8010 - val_accuracy: 0.5199\n",
      "Epoch 20/20\n",
      "1805/1805 [==============================] - 20s 11ms/step - loss: 0.3096 - accuracy: 0.9014 - val_loss: 1.8491 - val_accuracy: 0.5199\n",
      "399/399 [==============================] - 6s 14ms/step\n",
      "> 53.383\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.4)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=20, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d4abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/30\n",
      "1805/1805 [==============================] - 27s 15ms/step - loss: 8.1074 - accuracy: 0.1751 - val_loss: 2.3003 - val_accuracy: 0.3783\n",
      "Epoch 2/30\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 3.3074 - accuracy: 0.2831 - val_loss: 2.1019 - val_accuracy: 0.4181\n",
      "Epoch 3/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.7389 - accuracy: 0.3380 - val_loss: 1.9932 - val_accuracy: 0.4270\n",
      "Epoch 4/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.3468 - accuracy: 0.3828 - val_loss: 1.9404 - val_accuracy: 0.4469\n",
      "Epoch 5/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.1206 - accuracy: 0.4244 - val_loss: 1.8814 - val_accuracy: 0.4580\n",
      "Epoch 6/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.0167 - accuracy: 0.4532 - val_loss: 1.8601 - val_accuracy: 0.4668\n",
      "Epoch 7/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.8277 - accuracy: 0.4776 - val_loss: 1.8239 - val_accuracy: 0.4580\n",
      "Epoch 8/30\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.8290 - accuracy: 0.4781 - val_loss: 1.8528 - val_accuracy: 0.4558\n",
      "Epoch 9/30\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 1.7087 - accuracy: 0.5042 - val_loss: 1.8157 - val_accuracy: 0.4712\n",
      "Epoch 10/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.6914 - accuracy: 0.5136 - val_loss: 1.7837 - val_accuracy: 0.4535\n",
      "Epoch 11/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.6024 - accuracy: 0.5219 - val_loss: 1.7825 - val_accuracy: 0.4580\n",
      "Epoch 12/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.5261 - accuracy: 0.5396 - val_loss: 1.7527 - val_accuracy: 0.4801\n",
      "Epoch 13/30\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 1.5322 - accuracy: 0.5463 - val_loss: 1.7628 - val_accuracy: 0.4735\n",
      "Epoch 14/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.4620 - accuracy: 0.5640 - val_loss: 1.7495 - val_accuracy: 0.4823\n",
      "Epoch 15/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3860 - accuracy: 0.5756 - val_loss: 1.7168 - val_accuracy: 0.5066\n",
      "Epoch 16/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.4384 - accuracy: 0.5668 - val_loss: 1.7638 - val_accuracy: 0.4801\n",
      "Epoch 17/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3861 - accuracy: 0.5828 - val_loss: 1.7144 - val_accuracy: 0.5000\n",
      "Epoch 18/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3134 - accuracy: 0.6078 - val_loss: 1.6967 - val_accuracy: 0.4956\n",
      "Epoch 19/30\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 1.3201 - accuracy: 0.5873 - val_loss: 1.6812 - val_accuracy: 0.5199\n",
      "Epoch 20/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2843 - accuracy: 0.6127 - val_loss: 1.6774 - val_accuracy: 0.5044\n",
      "Epoch 21/30\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 1.2289 - accuracy: 0.6249 - val_loss: 1.6686 - val_accuracy: 0.5265\n",
      "Epoch 22/30\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 1.1856 - accuracy: 0.6355 - val_loss: 1.6964 - val_accuracy: 0.5066\n",
      "Epoch 23/30\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 1.1768 - accuracy: 0.6421 - val_loss: 1.6807 - val_accuracy: 0.5022\n",
      "Epoch 24/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1317 - accuracy: 0.6476 - val_loss: 1.6425 - val_accuracy: 0.5243\n",
      "Epoch 25/30\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 1.1306 - accuracy: 0.6587 - val_loss: 1.6410 - val_accuracy: 0.5332\n",
      "Epoch 26/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0569 - accuracy: 0.6753 - val_loss: 1.6535 - val_accuracy: 0.5199\n",
      "Epoch 27/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1013 - accuracy: 0.6659 - val_loss: 1.6426 - val_accuracy: 0.5177\n",
      "Epoch 28/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0172 - accuracy: 0.6975 - val_loss: 1.6607 - val_accuracy: 0.5177\n",
      "Epoch 29/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9830 - accuracy: 0.6947 - val_loss: 1.6377 - val_accuracy: 0.5265\n",
      "Epoch 30/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9868 - accuracy: 0.6970 - val_loss: 1.6455 - val_accuracy: 0.5177\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 54.386\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1072, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=30, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34df65d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 7.3465 - accuracy: 0.1579 - val_loss: 2.5442 - val_accuracy: 0.3164\n",
      "Epoch 2/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 3.1249 - accuracy: 0.2848 - val_loss: 2.1937 - val_accuracy: 0.4115\n",
      "Epoch 3/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.5033 - accuracy: 0.3701 - val_loss: 2.0332 - val_accuracy: 0.4403\n",
      "Epoch 4/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.1586 - accuracy: 0.3983 - val_loss: 1.9894 - val_accuracy: 0.4469\n",
      "Epoch 5/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.9326 - accuracy: 0.4460 - val_loss: 1.9440 - val_accuracy: 0.4624\n",
      "Epoch 6/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.8047 - accuracy: 0.4776 - val_loss: 1.9095 - val_accuracy: 0.4469\n",
      "Epoch 7/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.7565 - accuracy: 0.5003 - val_loss: 1.8944 - val_accuracy: 0.4535\n",
      "Epoch 8/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.6052 - accuracy: 0.5224 - val_loss: 1.8671 - val_accuracy: 0.4646\n",
      "Epoch 9/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.6014 - accuracy: 0.5219 - val_loss: 1.8482 - val_accuracy: 0.4867\n",
      "Epoch 10/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.5256 - accuracy: 0.5385 - val_loss: 1.8287 - val_accuracy: 0.4823\n",
      "Epoch 11/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.4873 - accuracy: 0.5540 - val_loss: 1.8230 - val_accuracy: 0.4735\n",
      "Epoch 12/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.4274 - accuracy: 0.5762 - val_loss: 1.8435 - val_accuracy: 0.4823\n",
      "Epoch 13/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3937 - accuracy: 0.5789 - val_loss: 1.8309 - val_accuracy: 0.4912\n",
      "Epoch 14/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3524 - accuracy: 0.5867 - val_loss: 1.8112 - val_accuracy: 0.4934\n",
      "Epoch 15/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2781 - accuracy: 0.6111 - val_loss: 1.8064 - val_accuracy: 0.4889\n",
      "Epoch 16/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2997 - accuracy: 0.5994 - val_loss: 1.8073 - val_accuracy: 0.4845\n",
      "Epoch 17/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2089 - accuracy: 0.6255 - val_loss: 1.7644 - val_accuracy: 0.5022\n",
      "Epoch 18/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1945 - accuracy: 0.6216 - val_loss: 1.7941 - val_accuracy: 0.4912\n",
      "Epoch 19/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1763 - accuracy: 0.6321 - val_loss: 1.7932 - val_accuracy: 0.5066\n",
      "Epoch 20/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1782 - accuracy: 0.6305 - val_loss: 1.7643 - val_accuracy: 0.5177\n",
      "Epoch 21/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1003 - accuracy: 0.6510 - val_loss: 1.7896 - val_accuracy: 0.5133\n",
      "Epoch 22/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0644 - accuracy: 0.6620 - val_loss: 1.8247 - val_accuracy: 0.4978\n",
      "Epoch 23/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0476 - accuracy: 0.6792 - val_loss: 1.7646 - val_accuracy: 0.5000\n",
      "Epoch 24/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0535 - accuracy: 0.6742 - val_loss: 1.7490 - val_accuracy: 0.5066\n",
      "Epoch 25/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0117 - accuracy: 0.6886 - val_loss: 1.7845 - val_accuracy: 0.5066\n",
      "Epoch 26/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9800 - accuracy: 0.6909 - val_loss: 1.7714 - val_accuracy: 0.5155\n",
      "Epoch 27/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9266 - accuracy: 0.7058 - val_loss: 1.7863 - val_accuracy: 0.5199\n",
      "Epoch 28/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9181 - accuracy: 0.7169 - val_loss: 1.7565 - val_accuracy: 0.5111\n",
      "Epoch 29/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.8892 - accuracy: 0.7296 - val_loss: 1.7640 - val_accuracy: 0.5354\n",
      "Epoch 30/30\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.8960 - accuracy: 0.7224 - val_loss: 1.7931 - val_accuracy: 0.5133\n",
      "399/399 [==============================] - 4s 11ms/step\n",
      "> 54.637\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1072, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=30, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e99760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/50\n",
      "1805/1805 [==============================] - 28s 16ms/step - loss: 7.4575 - accuracy: 0.1596 - val_loss: 2.4220 - val_accuracy: 0.3385\n",
      "Epoch 2/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 3.3114 - accuracy: 0.2299 - val_loss: 2.2957 - val_accuracy: 0.3562\n",
      "Epoch 3/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.7187 - accuracy: 0.3136 - val_loss: 2.1764 - val_accuracy: 0.4071\n",
      "Epoch 4/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.4749 - accuracy: 0.3374 - val_loss: 2.0736 - val_accuracy: 0.4403\n",
      "Epoch 5/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.3137 - accuracy: 0.3751 - val_loss: 2.0164 - val_accuracy: 0.4358\n",
      "Epoch 6/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.1216 - accuracy: 0.4116 - val_loss: 1.9784 - val_accuracy: 0.4336\n",
      "Epoch 7/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.9787 - accuracy: 0.4266 - val_loss: 1.9480 - val_accuracy: 0.4513\n",
      "Epoch 8/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.9317 - accuracy: 0.4465 - val_loss: 1.9033 - val_accuracy: 0.4580\n",
      "Epoch 9/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.8614 - accuracy: 0.4626 - val_loss: 1.8633 - val_accuracy: 0.4491\n",
      "Epoch 10/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.7531 - accuracy: 0.4931 - val_loss: 1.8444 - val_accuracy: 0.4535\n",
      "Epoch 11/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.6906 - accuracy: 0.5125 - val_loss: 1.8229 - val_accuracy: 0.4779\n",
      "Epoch 12/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.6318 - accuracy: 0.5186 - val_loss: 1.8229 - val_accuracy: 0.4757\n",
      "Epoch 13/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.6199 - accuracy: 0.5230 - val_loss: 1.7990 - val_accuracy: 0.4712\n",
      "Epoch 14/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.5722 - accuracy: 0.5235 - val_loss: 1.7912 - val_accuracy: 0.4867\n",
      "Epoch 15/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.5101 - accuracy: 0.5546 - val_loss: 1.8048 - val_accuracy: 0.4668\n",
      "Epoch 16/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4773 - accuracy: 0.5596 - val_loss: 1.7943 - val_accuracy: 0.4845\n",
      "Epoch 17/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4496 - accuracy: 0.5623 - val_loss: 1.7803 - val_accuracy: 0.4867\n",
      "Epoch 18/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4228 - accuracy: 0.5795 - val_loss: 1.7472 - val_accuracy: 0.5044\n",
      "Epoch 19/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4031 - accuracy: 0.5706 - val_loss: 1.7426 - val_accuracy: 0.4934\n",
      "Epoch 20/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3353 - accuracy: 0.6000 - val_loss: 1.7699 - val_accuracy: 0.5177\n",
      "Epoch 21/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3146 - accuracy: 0.6072 - val_loss: 1.7506 - val_accuracy: 0.5088\n",
      "Epoch 22/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2943 - accuracy: 0.6078 - val_loss: 1.7540 - val_accuracy: 0.5088\n",
      "Epoch 23/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2915 - accuracy: 0.6105 - val_loss: 1.7134 - val_accuracy: 0.5133\n",
      "Epoch 24/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2247 - accuracy: 0.6260 - val_loss: 1.7123 - val_accuracy: 0.5133\n",
      "Epoch 25/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2174 - accuracy: 0.6299 - val_loss: 1.7141 - val_accuracy: 0.5177\n",
      "Epoch 26/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1520 - accuracy: 0.6526 - val_loss: 1.7193 - val_accuracy: 0.5199\n",
      "Epoch 27/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1412 - accuracy: 0.6465 - val_loss: 1.7285 - val_accuracy: 0.5155\n",
      "Epoch 28/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1173 - accuracy: 0.6543 - val_loss: 1.7011 - val_accuracy: 0.5155\n",
      "Epoch 29/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1040 - accuracy: 0.6781 - val_loss: 1.6872 - val_accuracy: 0.5332\n",
      "Epoch 30/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0505 - accuracy: 0.6837 - val_loss: 1.7347 - val_accuracy: 0.5288\n",
      "Epoch 31/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0661 - accuracy: 0.6737 - val_loss: 1.6823 - val_accuracy: 0.5376\n",
      "Epoch 32/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0151 - accuracy: 0.6820 - val_loss: 1.7430 - val_accuracy: 0.5354\n",
      "Epoch 33/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0119 - accuracy: 0.6970 - val_loss: 1.6839 - val_accuracy: 0.5288\n",
      "Epoch 34/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9360 - accuracy: 0.7091 - val_loss: 1.7093 - val_accuracy: 0.5221\n",
      "Epoch 35/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8997 - accuracy: 0.7235 - val_loss: 1.6854 - val_accuracy: 0.5310\n",
      "Epoch 36/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9266 - accuracy: 0.7208 - val_loss: 1.6986 - val_accuracy: 0.5487\n",
      "Epoch 37/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8350 - accuracy: 0.7413 - val_loss: 1.7184 - val_accuracy: 0.5398\n",
      "Epoch 38/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8230 - accuracy: 0.7535 - val_loss: 1.6836 - val_accuracy: 0.5332\n",
      "Epoch 39/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8394 - accuracy: 0.7418 - val_loss: 1.6588 - val_accuracy: 0.5509\n",
      "Epoch 40/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8011 - accuracy: 0.7479 - val_loss: 1.6651 - val_accuracy: 0.5575\n",
      "Epoch 41/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7661 - accuracy: 0.7596 - val_loss: 1.7060 - val_accuracy: 0.5509\n",
      "Epoch 42/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7665 - accuracy: 0.7673 - val_loss: 1.6324 - val_accuracy: 0.5664\n",
      "Epoch 43/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7480 - accuracy: 0.7734 - val_loss: 1.6635 - val_accuracy: 0.5398\n",
      "Epoch 44/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7121 - accuracy: 0.7801 - val_loss: 1.6874 - val_accuracy: 0.5531\n",
      "Epoch 45/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7248 - accuracy: 0.7817 - val_loss: 1.7212 - val_accuracy: 0.5575\n",
      "Epoch 46/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6488 - accuracy: 0.7978 - val_loss: 1.6959 - val_accuracy: 0.5509\n",
      "Epoch 47/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6680 - accuracy: 0.7828 - val_loss: 1.6881 - val_accuracy: 0.5509\n",
      "Epoch 48/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6077 - accuracy: 0.8116 - val_loss: 1.7030 - val_accuracy: 0.5553\n",
      "Epoch 49/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6315 - accuracy: 0.8066 - val_loss: 1.6895 - val_accuracy: 0.5730\n",
      "Epoch 50/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5836 - accuracy: 0.8211 - val_loss: 1.7534 - val_accuracy: 0.5597\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 55.890\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1072, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=50, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51112f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/50\n",
      "1805/1805 [==============================] - 30s 16ms/step - loss: 7.3732 - accuracy: 0.1518 - val_loss: 2.1489 - val_accuracy: 0.3827\n",
      "Epoch 2/50\n",
      "1805/1805 [==============================] - 20s 11ms/step - loss: 2.9610 - accuracy: 0.3025 - val_loss: 2.0582 - val_accuracy: 0.4181\n",
      "Epoch 3/50\n",
      "1805/1805 [==============================] - 20s 11ms/step - loss: 2.3727 - accuracy: 0.3740 - val_loss: 1.9625 - val_accuracy: 0.4535\n",
      "Epoch 4/50\n",
      "1805/1805 [==============================] - 20s 11ms/step - loss: 2.1322 - accuracy: 0.4150 - val_loss: 1.8992 - val_accuracy: 0.4558\n",
      "Epoch 5/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.9605 - accuracy: 0.4438 - val_loss: 1.8811 - val_accuracy: 0.4801\n",
      "Epoch 6/50\n",
      "1805/1805 [==============================] - 20s 11ms/step - loss: 1.8497 - accuracy: 0.4571 - val_loss: 1.8559 - val_accuracy: 0.4757\n",
      "Epoch 7/50\n",
      "1805/1805 [==============================] - 20s 11ms/step - loss: 1.7491 - accuracy: 0.4925 - val_loss: 1.8274 - val_accuracy: 0.4956\n",
      "Epoch 8/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.6533 - accuracy: 0.5136 - val_loss: 1.8076 - val_accuracy: 0.4934\n",
      "Epoch 9/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.6060 - accuracy: 0.5368 - val_loss: 1.8063 - val_accuracy: 0.4912\n",
      "Epoch 10/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.4808 - accuracy: 0.5601 - val_loss: 1.7810 - val_accuracy: 0.4889\n",
      "Epoch 11/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.4785 - accuracy: 0.5662 - val_loss: 1.7257 - val_accuracy: 0.4867\n",
      "Epoch 12/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.4517 - accuracy: 0.5701 - val_loss: 1.7570 - val_accuracy: 0.4978\n",
      "Epoch 13/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.3736 - accuracy: 0.5806 - val_loss: 1.7333 - val_accuracy: 0.4912\n",
      "Epoch 14/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.3044 - accuracy: 0.6017 - val_loss: 1.7475 - val_accuracy: 0.4978\n",
      "Epoch 15/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.2690 - accuracy: 0.6155 - val_loss: 1.7235 - val_accuracy: 0.5022\n",
      "Epoch 16/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.2449 - accuracy: 0.6244 - val_loss: 1.7132 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.2271 - accuracy: 0.6255 - val_loss: 1.7038 - val_accuracy: 0.5066\n",
      "Epoch 18/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.1745 - accuracy: 0.6393 - val_loss: 1.6786 - val_accuracy: 0.5111\n",
      "Epoch 19/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.1392 - accuracy: 0.6482 - val_loss: 1.6919 - val_accuracy: 0.5133\n",
      "Epoch 20/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0880 - accuracy: 0.6681 - val_loss: 1.7068 - val_accuracy: 0.5155\n",
      "Epoch 21/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0359 - accuracy: 0.6825 - val_loss: 1.6786 - val_accuracy: 0.5088\n",
      "Epoch 22/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0314 - accuracy: 0.6825 - val_loss: 1.7357 - val_accuracy: 0.4978\n",
      "Epoch 23/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9660 - accuracy: 0.7014 - val_loss: 1.7592 - val_accuracy: 0.5221\n",
      "Epoch 24/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9608 - accuracy: 0.6925 - val_loss: 1.7005 - val_accuracy: 0.5221\n",
      "Epoch 25/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.9604 - accuracy: 0.6981 - val_loss: 1.6799 - val_accuracy: 0.5288\n",
      "Epoch 26/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.9105 - accuracy: 0.7247 - val_loss: 1.7462 - val_accuracy: 0.5177\n",
      "Epoch 27/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.9026 - accuracy: 0.7119 - val_loss: 1.7339 - val_accuracy: 0.5199\n",
      "Epoch 28/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.8306 - accuracy: 0.7479 - val_loss: 1.6885 - val_accuracy: 0.5354\n",
      "Epoch 29/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.8488 - accuracy: 0.7435 - val_loss: 1.7201 - val_accuracy: 0.5354\n",
      "Epoch 30/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.8064 - accuracy: 0.7507 - val_loss: 1.6906 - val_accuracy: 0.5376\n",
      "Epoch 31/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7761 - accuracy: 0.7546 - val_loss: 1.7276 - val_accuracy: 0.5465\n",
      "Epoch 32/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.7778 - accuracy: 0.7651 - val_loss: 1.7408 - val_accuracy: 0.5398\n",
      "Epoch 33/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.7562 - accuracy: 0.7706 - val_loss: 1.7108 - val_accuracy: 0.5442\n",
      "Epoch 34/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.7320 - accuracy: 0.7717 - val_loss: 1.6925 - val_accuracy: 0.5354\n",
      "Epoch 35/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.7315 - accuracy: 0.7756 - val_loss: 1.7210 - val_accuracy: 0.5487\n",
      "Epoch 36/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.7095 - accuracy: 0.7812 - val_loss: 1.7251 - val_accuracy: 0.5531\n",
      "Epoch 37/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.6876 - accuracy: 0.7873 - val_loss: 1.7162 - val_accuracy: 0.5509\n",
      "Epoch 38/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.6588 - accuracy: 0.7945 - val_loss: 1.6870 - val_accuracy: 0.5420\n",
      "Epoch 39/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.6286 - accuracy: 0.8011 - val_loss: 1.6751 - val_accuracy: 0.5288\n",
      "Epoch 40/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6426 - accuracy: 0.8061 - val_loss: 1.6786 - val_accuracy: 0.5354\n",
      "Epoch 41/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6112 - accuracy: 0.8144 - val_loss: 1.7542 - val_accuracy: 0.5442\n",
      "Epoch 42/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6027 - accuracy: 0.8122 - val_loss: 1.7051 - val_accuracy: 0.5442\n",
      "Epoch 43/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6178 - accuracy: 0.8177 - val_loss: 1.6979 - val_accuracy: 0.5531\n",
      "Epoch 44/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.6051 - accuracy: 0.8144 - val_loss: 1.7482 - val_accuracy: 0.5354\n",
      "Epoch 45/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5446 - accuracy: 0.8316 - val_loss: 1.7097 - val_accuracy: 0.5642\n",
      "Epoch 46/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.5091 - accuracy: 0.8343 - val_loss: 1.7905 - val_accuracy: 0.5465\n",
      "Epoch 47/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5144 - accuracy: 0.8432 - val_loss: 1.7257 - val_accuracy: 0.5487\n",
      "Epoch 48/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.5036 - accuracy: 0.8465 - val_loss: 1.7425 - val_accuracy: 0.5487\n",
      "Epoch 49/50\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4874 - accuracy: 0.8488 - val_loss: 1.7156 - val_accuracy: 0.5442\n",
      "Epoch 50/50\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.4550 - accuracy: 0.8626 - val_loss: 1.7147 - val_accuracy: 0.5509\n",
      "399/399 [==============================] - 6s 16ms/step\n",
      "> 55.890\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1072, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=50, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec2d04a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 7.4433 - accuracy: 0.1657 - val_loss: 2.3870 - val_accuracy: 0.3097\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 3.3324 - accuracy: 0.2465 - val_loss: 2.1870 - val_accuracy: 0.3429\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.6620 - accuracy: 0.2997 - val_loss: 2.0813 - val_accuracy: 0.3650\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.4186 - accuracy: 0.3285 - val_loss: 2.0679 - val_accuracy: 0.4049\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.2101 - accuracy: 0.3823 - val_loss: 2.0089 - val_accuracy: 0.3695\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.0610 - accuracy: 0.4139 - val_loss: 1.9400 - val_accuracy: 0.4181\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.9710 - accuracy: 0.4416 - val_loss: 1.9169 - val_accuracy: 0.4469\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.9008 - accuracy: 0.4521 - val_loss: 1.8567 - val_accuracy: 0.4668\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.8004 - accuracy: 0.4709 - val_loss: 1.8366 - val_accuracy: 0.4624\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.7427 - accuracy: 0.4981 - val_loss: 1.8313 - val_accuracy: 0.4602\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.6896 - accuracy: 0.5042 - val_loss: 1.7905 - val_accuracy: 0.4867\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.6248 - accuracy: 0.5191 - val_loss: 1.7650 - val_accuracy: 0.4867\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.5538 - accuracy: 0.5380 - val_loss: 1.7551 - val_accuracy: 0.4934\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.5187 - accuracy: 0.5474 - val_loss: 1.7624 - val_accuracy: 0.4978\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4463 - accuracy: 0.5568 - val_loss: 1.7485 - val_accuracy: 0.5022\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4035 - accuracy: 0.5817 - val_loss: 1.7554 - val_accuracy: 0.4934\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3433 - accuracy: 0.6022 - val_loss: 1.7437 - val_accuracy: 0.4956\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3681 - accuracy: 0.5989 - val_loss: 1.7123 - val_accuracy: 0.5088\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2942 - accuracy: 0.6139 - val_loss: 1.6879 - val_accuracy: 0.5088\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2447 - accuracy: 0.6260 - val_loss: 1.7430 - val_accuracy: 0.5022\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2383 - accuracy: 0.6305 - val_loss: 1.6911 - val_accuracy: 0.5066\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1912 - accuracy: 0.6399 - val_loss: 1.7060 - val_accuracy: 0.5044\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1881 - accuracy: 0.6371 - val_loss: 1.7065 - val_accuracy: 0.5111\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1412 - accuracy: 0.6460 - val_loss: 1.6784 - val_accuracy: 0.5243\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0542 - accuracy: 0.6753 - val_loss: 1.6879 - val_accuracy: 0.5133\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0457 - accuracy: 0.6859 - val_loss: 1.6481 - val_accuracy: 0.5265\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0394 - accuracy: 0.6920 - val_loss: 1.6988 - val_accuracy: 0.5044\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0301 - accuracy: 0.6842 - val_loss: 1.6722 - val_accuracy: 0.5133\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9773 - accuracy: 0.7008 - val_loss: 1.6811 - val_accuracy: 0.5265\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9106 - accuracy: 0.7230 - val_loss: 1.7162 - val_accuracy: 0.5111\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9330 - accuracy: 0.7191 - val_loss: 1.6878 - val_accuracy: 0.5243\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8398 - accuracy: 0.7468 - val_loss: 1.6656 - val_accuracy: 0.5288\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8252 - accuracy: 0.7496 - val_loss: 1.6679 - val_accuracy: 0.5177\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8183 - accuracy: 0.7446 - val_loss: 1.6519 - val_accuracy: 0.5332\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8246 - accuracy: 0.7562 - val_loss: 1.6878 - val_accuracy: 0.5177\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8007 - accuracy: 0.7551 - val_loss: 1.6583 - val_accuracy: 0.5465\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7021 - accuracy: 0.7839 - val_loss: 1.6475 - val_accuracy: 0.5376\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6963 - accuracy: 0.7706 - val_loss: 1.6588 - val_accuracy: 0.5221\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6837 - accuracy: 0.7934 - val_loss: 1.7101 - val_accuracy: 0.5332\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7039 - accuracy: 0.7828 - val_loss: 1.6702 - val_accuracy: 0.5398\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6668 - accuracy: 0.7911 - val_loss: 1.6603 - val_accuracy: 0.5332\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6401 - accuracy: 0.8066 - val_loss: 1.6829 - val_accuracy: 0.5332\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6168 - accuracy: 0.8044 - val_loss: 1.6705 - val_accuracy: 0.5398\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5430 - accuracy: 0.8432 - val_loss: 1.7131 - val_accuracy: 0.5376\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5769 - accuracy: 0.8249 - val_loss: 1.7041 - val_accuracy: 0.5420\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5236 - accuracy: 0.8349 - val_loss: 1.6872 - val_accuracy: 0.5531\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5247 - accuracy: 0.8343 - val_loss: 1.7561 - val_accuracy: 0.5420\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5232 - accuracy: 0.8343 - val_loss: 1.6872 - val_accuracy: 0.5487\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5131 - accuracy: 0.8388 - val_loss: 1.6973 - val_accuracy: 0.5465\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4851 - accuracy: 0.8576 - val_loss: 1.7004 - val_accuracy: 0.5465\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4273 - accuracy: 0.8693 - val_loss: 1.7070 - val_accuracy: 0.5597\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4252 - accuracy: 0.8609 - val_loss: 1.7081 - val_accuracy: 0.5398\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4052 - accuracy: 0.8720 - val_loss: 1.6944 - val_accuracy: 0.5619\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4203 - accuracy: 0.8715 - val_loss: 1.7246 - val_accuracy: 0.5420\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4316 - accuracy: 0.8693 - val_loss: 1.7124 - val_accuracy: 0.5553\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4033 - accuracy: 0.8742 - val_loss: 1.8365 - val_accuracy: 0.5442\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3711 - accuracy: 0.8870 - val_loss: 1.8263 - val_accuracy: 0.5288\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3685 - accuracy: 0.8870 - val_loss: 1.7284 - val_accuracy: 0.5487\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3566 - accuracy: 0.8903 - val_loss: 1.7223 - val_accuracy: 0.5509\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3723 - accuracy: 0.8859 - val_loss: 1.7657 - val_accuracy: 0.5531\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3629 - accuracy: 0.8859 - val_loss: 1.7085 - val_accuracy: 0.5553\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3372 - accuracy: 0.8953 - val_loss: 1.7395 - val_accuracy: 0.5553\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3162 - accuracy: 0.9058 - val_loss: 1.7275 - val_accuracy: 0.5531\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3378 - accuracy: 0.8920 - val_loss: 1.7790 - val_accuracy: 0.5288\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3183 - accuracy: 0.9108 - val_loss: 1.8055 - val_accuracy: 0.5487\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3100 - accuracy: 0.9058 - val_loss: 1.8045 - val_accuracy: 0.5442\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2536 - accuracy: 0.9269 - val_loss: 1.7531 - val_accuracy: 0.5642\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2624 - accuracy: 0.9224 - val_loss: 1.7561 - val_accuracy: 0.5531\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2544 - accuracy: 0.9208 - val_loss: 1.7879 - val_accuracy: 0.5531\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2648 - accuracy: 0.9169 - val_loss: 1.8103 - val_accuracy: 0.5376\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2459 - accuracy: 0.9247 - val_loss: 1.8431 - val_accuracy: 0.5442\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2445 - accuracy: 0.9224 - val_loss: 1.8323 - val_accuracy: 0.5465\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2156 - accuracy: 0.9280 - val_loss: 1.8917 - val_accuracy: 0.5487\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2507 - accuracy: 0.9302 - val_loss: 1.8186 - val_accuracy: 0.5465\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2381 - accuracy: 0.9285 - val_loss: 1.8229 - val_accuracy: 0.5509\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2099 - accuracy: 0.9380 - val_loss: 1.8574 - val_accuracy: 0.5553\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2132 - accuracy: 0.9357 - val_loss: 1.8539 - val_accuracy: 0.5597\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2106 - accuracy: 0.9319 - val_loss: 1.8039 - val_accuracy: 0.5597\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2168 - accuracy: 0.9346 - val_loss: 1.7508 - val_accuracy: 0.5619\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2067 - accuracy: 0.9385 - val_loss: 1.8335 - val_accuracy: 0.5597\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1882 - accuracy: 0.9440 - val_loss: 1.8973 - val_accuracy: 0.5487\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2045 - accuracy: 0.9424 - val_loss: 1.8796 - val_accuracy: 0.5597\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1682 - accuracy: 0.9496 - val_loss: 1.7944 - val_accuracy: 0.5664\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1854 - accuracy: 0.9424 - val_loss: 1.8241 - val_accuracy: 0.5642\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1664 - accuracy: 0.9479 - val_loss: 1.8652 - val_accuracy: 0.5642\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1585 - accuracy: 0.9490 - val_loss: 1.8361 - val_accuracy: 0.5708\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1585 - accuracy: 0.9496 - val_loss: 1.9134 - val_accuracy: 0.5597\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1863 - accuracy: 0.9396 - val_loss: 1.9460 - val_accuracy: 0.5553\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1526 - accuracy: 0.9535 - val_loss: 1.8929 - val_accuracy: 0.5531\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1674 - accuracy: 0.9479 - val_loss: 1.9264 - val_accuracy: 0.5553\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1465 - accuracy: 0.9524 - val_loss: 1.8739 - val_accuracy: 0.5597\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1328 - accuracy: 0.9584 - val_loss: 1.8411 - val_accuracy: 0.5575\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1496 - accuracy: 0.9518 - val_loss: 1.9268 - val_accuracy: 0.5642\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1433 - accuracy: 0.9590 - val_loss: 1.9253 - val_accuracy: 0.5642\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1347 - accuracy: 0.9579 - val_loss: 1.9452 - val_accuracy: 0.5575\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1367 - accuracy: 0.9557 - val_loss: 1.9182 - val_accuracy: 0.5531\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1521 - accuracy: 0.9512 - val_loss: 1.9118 - val_accuracy: 0.5487\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1470 - accuracy: 0.9524 - val_loss: 1.9952 - val_accuracy: 0.5575\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1164 - accuracy: 0.9618 - val_loss: 1.9260 - val_accuracy: 0.5642\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1377 - accuracy: 0.9568 - val_loss: 1.9626 - val_accuracy: 0.5642\n",
      "399/399 [==============================] - 6s 14ms/step\n",
      "> 57.895\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1024, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513cd4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d088229d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc70387a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 32s 18ms/step - loss: 6.7415 - accuracy: 0.1507 - val_loss: 2.3487 - val_accuracy: 0.3429\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 3.2124 - accuracy: 0.2410 - val_loss: 2.2056 - val_accuracy: 0.4159\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.6332 - accuracy: 0.3158 - val_loss: 2.0956 - val_accuracy: 0.4226\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.4810 - accuracy: 0.3612 - val_loss: 1.9969 - val_accuracy: 0.4447\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.2080 - accuracy: 0.3939 - val_loss: 1.9629 - val_accuracy: 0.4425\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.0639 - accuracy: 0.4227 - val_loss: 1.8835 - val_accuracy: 0.4646\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.9754 - accuracy: 0.4493 - val_loss: 1.8555 - val_accuracy: 0.4668\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.9262 - accuracy: 0.4687 - val_loss: 1.8584 - val_accuracy: 0.4580\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.8152 - accuracy: 0.4698 - val_loss: 1.8252 - val_accuracy: 0.4779\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.7378 - accuracy: 0.5053 - val_loss: 1.7899 - val_accuracy: 0.4845\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.6892 - accuracy: 0.5130 - val_loss: 1.8002 - val_accuracy: 0.4646\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.5863 - accuracy: 0.5296 - val_loss: 1.7444 - val_accuracy: 0.4934\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.5914 - accuracy: 0.5341 - val_loss: 1.7563 - val_accuracy: 0.4912\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.5075 - accuracy: 0.5596 - val_loss: 1.7325 - val_accuracy: 0.5111\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 1.5006 - accuracy: 0.5507 - val_loss: 1.7247 - val_accuracy: 0.5199\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 1.4333 - accuracy: 0.5767 - val_loss: 1.7298 - val_accuracy: 0.4978\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.4161 - accuracy: 0.5828 - val_loss: 1.7059 - val_accuracy: 0.5044\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3981 - accuracy: 0.5773 - val_loss: 1.6881 - val_accuracy: 0.5111\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3235 - accuracy: 0.5961 - val_loss: 1.7042 - val_accuracy: 0.5088\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.3042 - accuracy: 0.6061 - val_loss: 1.6843 - val_accuracy: 0.5221\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2553 - accuracy: 0.6277 - val_loss: 1.6755 - val_accuracy: 0.5066\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2483 - accuracy: 0.6199 - val_loss: 1.6706 - val_accuracy: 0.5243\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2559 - accuracy: 0.6172 - val_loss: 1.6571 - val_accuracy: 0.5155\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1769 - accuracy: 0.6427 - val_loss: 1.6782 - val_accuracy: 0.5221\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1225 - accuracy: 0.6681 - val_loss: 1.6787 - val_accuracy: 0.5243\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.1100 - accuracy: 0.6593 - val_loss: 1.6456 - val_accuracy: 0.5133\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0350 - accuracy: 0.6831 - val_loss: 1.6743 - val_accuracy: 0.5243\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0656 - accuracy: 0.6715 - val_loss: 1.7026 - val_accuracy: 0.5177\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0429 - accuracy: 0.6853 - val_loss: 1.6635 - val_accuracy: 0.5354\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9900 - accuracy: 0.7042 - val_loss: 1.6703 - val_accuracy: 0.5376\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9767 - accuracy: 0.7025 - val_loss: 1.6615 - val_accuracy: 0.5265\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9033 - accuracy: 0.7230 - val_loss: 1.6177 - val_accuracy: 0.5243\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.8923 - accuracy: 0.7219 - val_loss: 1.6426 - val_accuracy: 0.5332\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.8703 - accuracy: 0.7313 - val_loss: 1.6287 - val_accuracy: 0.5354\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.8578 - accuracy: 0.7313 - val_loss: 1.6575 - val_accuracy: 0.5310\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.8407 - accuracy: 0.7429 - val_loss: 1.6641 - val_accuracy: 0.5288\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.7874 - accuracy: 0.7579 - val_loss: 1.6228 - val_accuracy: 0.5465\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.7668 - accuracy: 0.7695 - val_loss: 1.6850 - val_accuracy: 0.5199\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.7154 - accuracy: 0.7756 - val_loss: 1.6532 - val_accuracy: 0.5531\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.7723 - accuracy: 0.7657 - val_loss: 1.7050 - val_accuracy: 0.5310\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.6892 - accuracy: 0.7906 - val_loss: 1.7068 - val_accuracy: 0.5376\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.6689 - accuracy: 0.7878 - val_loss: 1.7365 - val_accuracy: 0.5265\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.6494 - accuracy: 0.7989 - val_loss: 1.6616 - val_accuracy: 0.5354\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.6524 - accuracy: 0.7978 - val_loss: 1.6337 - val_accuracy: 0.5575\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.6281 - accuracy: 0.7983 - val_loss: 1.6421 - val_accuracy: 0.5509\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.6079 - accuracy: 0.8155 - val_loss: 1.6809 - val_accuracy: 0.5354\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.5697 - accuracy: 0.8222 - val_loss: 1.6820 - val_accuracy: 0.5420\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.5493 - accuracy: 0.8321 - val_loss: 1.6756 - val_accuracy: 0.5487\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.5213 - accuracy: 0.8371 - val_loss: 1.7348 - val_accuracy: 0.5420\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.5408 - accuracy: 0.8327 - val_loss: 1.7170 - val_accuracy: 0.5420\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.5181 - accuracy: 0.8316 - val_loss: 1.7459 - val_accuracy: 0.5265\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4859 - accuracy: 0.8482 - val_loss: 1.7502 - val_accuracy: 0.5487\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4965 - accuracy: 0.8499 - val_loss: 1.6896 - val_accuracy: 0.5442\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4472 - accuracy: 0.8582 - val_loss: 1.6942 - val_accuracy: 0.5288\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4470 - accuracy: 0.8548 - val_loss: 1.7182 - val_accuracy: 0.5376\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4419 - accuracy: 0.8565 - val_loss: 1.7383 - val_accuracy: 0.5442\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3956 - accuracy: 0.8809 - val_loss: 1.7468 - val_accuracy: 0.5531\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4163 - accuracy: 0.8720 - val_loss: 1.6937 - val_accuracy: 0.5354\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3921 - accuracy: 0.8787 - val_loss: 1.6840 - val_accuracy: 0.5553\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3621 - accuracy: 0.8809 - val_loss: 1.7541 - val_accuracy: 0.5509\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3592 - accuracy: 0.8909 - val_loss: 1.6999 - val_accuracy: 0.5487\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3682 - accuracy: 0.8875 - val_loss: 1.7757 - val_accuracy: 0.5332\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3506 - accuracy: 0.8892 - val_loss: 1.7063 - val_accuracy: 0.5553\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3400 - accuracy: 0.8898 - val_loss: 1.7643 - val_accuracy: 0.5509\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3180 - accuracy: 0.9014 - val_loss: 1.7696 - val_accuracy: 0.5575\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3199 - accuracy: 0.9064 - val_loss: 1.7521 - val_accuracy: 0.5420\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3039 - accuracy: 0.9091 - val_loss: 1.7044 - val_accuracy: 0.5642\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3170 - accuracy: 0.9003 - val_loss: 1.7924 - val_accuracy: 0.5531\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2801 - accuracy: 0.9086 - val_loss: 1.7651 - val_accuracy: 0.5531\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3032 - accuracy: 0.9130 - val_loss: 1.7694 - val_accuracy: 0.5465\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2946 - accuracy: 0.9130 - val_loss: 1.7367 - val_accuracy: 0.5553\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2736 - accuracy: 0.9152 - val_loss: 1.7772 - val_accuracy: 0.5509\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2583 - accuracy: 0.9180 - val_loss: 1.8105 - val_accuracy: 0.5553\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2516 - accuracy: 0.9202 - val_loss: 1.7733 - val_accuracy: 0.5553\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2335 - accuracy: 0.9296 - val_loss: 1.9295 - val_accuracy: 0.5487\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2328 - accuracy: 0.9269 - val_loss: 1.8027 - val_accuracy: 0.5465\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2355 - accuracy: 0.9324 - val_loss: 1.7922 - val_accuracy: 0.5509\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2628 - accuracy: 0.9208 - val_loss: 1.8545 - val_accuracy: 0.5553\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2384 - accuracy: 0.9274 - val_loss: 1.8441 - val_accuracy: 0.5487\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2126 - accuracy: 0.9402 - val_loss: 1.8050 - val_accuracy: 0.5619\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2250 - accuracy: 0.9285 - val_loss: 1.7683 - val_accuracy: 0.5553\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1939 - accuracy: 0.9402 - val_loss: 1.7882 - val_accuracy: 0.5531\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1893 - accuracy: 0.9407 - val_loss: 1.8554 - val_accuracy: 0.5597\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1864 - accuracy: 0.9429 - val_loss: 1.8680 - val_accuracy: 0.5686\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1744 - accuracy: 0.9402 - val_loss: 1.9374 - val_accuracy: 0.5487\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2126 - accuracy: 0.9352 - val_loss: 1.8263 - val_accuracy: 0.5752\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1843 - accuracy: 0.9418 - val_loss: 1.8510 - val_accuracy: 0.5642\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1721 - accuracy: 0.9490 - val_loss: 1.9491 - val_accuracy: 0.5509\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1884 - accuracy: 0.9446 - val_loss: 1.8961 - val_accuracy: 0.5575\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1734 - accuracy: 0.9479 - val_loss: 1.8617 - val_accuracy: 0.5619\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1557 - accuracy: 0.9524 - val_loss: 1.8906 - val_accuracy: 0.5553\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1450 - accuracy: 0.9573 - val_loss: 1.8785 - val_accuracy: 0.5642\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1525 - accuracy: 0.9551 - val_loss: 1.8597 - val_accuracy: 0.5642\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1496 - accuracy: 0.9596 - val_loss: 1.9432 - val_accuracy: 0.5642\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1666 - accuracy: 0.9501 - val_loss: 1.8174 - val_accuracy: 0.5796\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1444 - accuracy: 0.9557 - val_loss: 1.8859 - val_accuracy: 0.5686\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1281 - accuracy: 0.9651 - val_loss: 1.8707 - val_accuracy: 0.5774\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1570 - accuracy: 0.9518 - val_loss: 1.8977 - val_accuracy: 0.5708\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1351 - accuracy: 0.9596 - val_loss: 1.9072 - val_accuracy: 0.5642\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1568 - accuracy: 0.9485 - val_loss: 1.9609 - val_accuracy: 0.5442\n",
      "399/399 [==============================] - 6s 16ms/step\n",
      "> 57.143\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1072, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ac4bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 30s 16ms/step - loss: 5.2578 - accuracy: 0.1856 - val_loss: 2.3754 - val_accuracy: 0.3407\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 2.5648 - accuracy: 0.3169 - val_loss: 2.1594 - val_accuracy: 0.4004\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.0765 - accuracy: 0.4205 - val_loss: 1.9855 - val_accuracy: 0.4292\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.8437 - accuracy: 0.4759 - val_loss: 1.8717 - val_accuracy: 0.4712\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.7210 - accuracy: 0.5014 - val_loss: 1.8469 - val_accuracy: 0.4558\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.5445 - accuracy: 0.5562 - val_loss: 1.8036 - val_accuracy: 0.4668\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4437 - accuracy: 0.5634 - val_loss: 1.7972 - val_accuracy: 0.4735\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3239 - accuracy: 0.5889 - val_loss: 1.7668 - val_accuracy: 0.4646\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2221 - accuracy: 0.6260 - val_loss: 1.7429 - val_accuracy: 0.5044\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1515 - accuracy: 0.6465 - val_loss: 1.7767 - val_accuracy: 0.4801\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0768 - accuracy: 0.6620 - val_loss: 1.7266 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0286 - accuracy: 0.6886 - val_loss: 1.7268 - val_accuracy: 0.5111\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9214 - accuracy: 0.7042 - val_loss: 1.6942 - val_accuracy: 0.5044\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8536 - accuracy: 0.7357 - val_loss: 1.6920 - val_accuracy: 0.5155\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7877 - accuracy: 0.7524 - val_loss: 1.7148 - val_accuracy: 0.4867\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7675 - accuracy: 0.7507 - val_loss: 1.6432 - val_accuracy: 0.5288\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6658 - accuracy: 0.7978 - val_loss: 1.6669 - val_accuracy: 0.5199\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6291 - accuracy: 0.8028 - val_loss: 1.6768 - val_accuracy: 0.5288\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5863 - accuracy: 0.8271 - val_loss: 1.6999 - val_accuracy: 0.5288\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5099 - accuracy: 0.8432 - val_loss: 1.7494 - val_accuracy: 0.5265\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5134 - accuracy: 0.8427 - val_loss: 1.7229 - val_accuracy: 0.5354\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4596 - accuracy: 0.8598 - val_loss: 1.7047 - val_accuracy: 0.5354\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4326 - accuracy: 0.8648 - val_loss: 1.6989 - val_accuracy: 0.5420\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4001 - accuracy: 0.8792 - val_loss: 1.7014 - val_accuracy: 0.5288\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3507 - accuracy: 0.8881 - val_loss: 1.7298 - val_accuracy: 0.5155\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3793 - accuracy: 0.8809 - val_loss: 1.7214 - val_accuracy: 0.5398\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3026 - accuracy: 0.9075 - val_loss: 1.7825 - val_accuracy: 0.5221\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3195 - accuracy: 0.9030 - val_loss: 1.7599 - val_accuracy: 0.5265\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2907 - accuracy: 0.9097 - val_loss: 1.7458 - val_accuracy: 0.5398\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2707 - accuracy: 0.9269 - val_loss: 1.7595 - val_accuracy: 0.5310\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2733 - accuracy: 0.9102 - val_loss: 1.7789 - val_accuracy: 0.5288\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2454 - accuracy: 0.9258 - val_loss: 1.8357 - val_accuracy: 0.5221\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2129 - accuracy: 0.9324 - val_loss: 1.7926 - val_accuracy: 0.5398\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2075 - accuracy: 0.9368 - val_loss: 1.8242 - val_accuracy: 0.5310\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2200 - accuracy: 0.9396 - val_loss: 1.8154 - val_accuracy: 0.5199\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2055 - accuracy: 0.9402 - val_loss: 1.8060 - val_accuracy: 0.5398\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1935 - accuracy: 0.9418 - val_loss: 1.8352 - val_accuracy: 0.5376\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1727 - accuracy: 0.9452 - val_loss: 1.7863 - val_accuracy: 0.5332\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1780 - accuracy: 0.9463 - val_loss: 1.7762 - val_accuracy: 0.5310\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1590 - accuracy: 0.9512 - val_loss: 1.7971 - val_accuracy: 0.5376\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1480 - accuracy: 0.9557 - val_loss: 1.7913 - val_accuracy: 0.5265\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1596 - accuracy: 0.9501 - val_loss: 1.7702 - val_accuracy: 0.5442\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1591 - accuracy: 0.9485 - val_loss: 1.8369 - val_accuracy: 0.5597\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1380 - accuracy: 0.9584 - val_loss: 1.8111 - val_accuracy: 0.5442\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1385 - accuracy: 0.9562 - val_loss: 1.9081 - val_accuracy: 0.5310\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1292 - accuracy: 0.9634 - val_loss: 1.8457 - val_accuracy: 0.5465\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1186 - accuracy: 0.9695 - val_loss: 1.8328 - val_accuracy: 0.5465\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1271 - accuracy: 0.9607 - val_loss: 1.9039 - val_accuracy: 0.5288\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1179 - accuracy: 0.9634 - val_loss: 1.9426 - val_accuracy: 0.5354\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1188 - accuracy: 0.9645 - val_loss: 2.0494 - val_accuracy: 0.5420\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1143 - accuracy: 0.9662 - val_loss: 1.9821 - val_accuracy: 0.5332\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0979 - accuracy: 0.9673 - val_loss: 1.9349 - val_accuracy: 0.5553\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1008 - accuracy: 0.9734 - val_loss: 2.0051 - val_accuracy: 0.5465\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1052 - accuracy: 0.9684 - val_loss: 2.0256 - val_accuracy: 0.5531\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1074 - accuracy: 0.9662 - val_loss: 1.9587 - val_accuracy: 0.5442\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0947 - accuracy: 0.9734 - val_loss: 1.9851 - val_accuracy: 0.5398\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0914 - accuracy: 0.9717 - val_loss: 1.9664 - val_accuracy: 0.5398\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0913 - accuracy: 0.9712 - val_loss: 2.0241 - val_accuracy: 0.5398\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1173 - accuracy: 0.9612 - val_loss: 1.9641 - val_accuracy: 0.5376\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0945 - accuracy: 0.9734 - val_loss: 1.8818 - val_accuracy: 0.5420\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0748 - accuracy: 0.9745 - val_loss: 1.9896 - val_accuracy: 0.5310\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0944 - accuracy: 0.9706 - val_loss: 1.9542 - val_accuracy: 0.5354\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0735 - accuracy: 0.9817 - val_loss: 1.9658 - val_accuracy: 0.5398\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0742 - accuracy: 0.9789 - val_loss: 1.9314 - val_accuracy: 0.5420\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0685 - accuracy: 0.9828 - val_loss: 2.0090 - val_accuracy: 0.5442\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0638 - accuracy: 0.9812 - val_loss: 2.0074 - val_accuracy: 0.5420\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0721 - accuracy: 0.9828 - val_loss: 2.0914 - val_accuracy: 0.5420\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0728 - accuracy: 0.9789 - val_loss: 1.9745 - val_accuracy: 0.5531\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0683 - accuracy: 0.9812 - val_loss: 1.9845 - val_accuracy: 0.5420\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0662 - accuracy: 0.9823 - val_loss: 1.9761 - val_accuracy: 0.5465\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0616 - accuracy: 0.9839 - val_loss: 2.0464 - val_accuracy: 0.5420\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0592 - accuracy: 0.9845 - val_loss: 1.9287 - val_accuracy: 0.5487\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0643 - accuracy: 0.9773 - val_loss: 2.0645 - val_accuracy: 0.5265\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0687 - accuracy: 0.9823 - val_loss: 2.0086 - val_accuracy: 0.5332\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0611 - accuracy: 0.9828 - val_loss: 2.1010 - val_accuracy: 0.5354\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0626 - accuracy: 0.9812 - val_loss: 2.0174 - val_accuracy: 0.5531\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0577 - accuracy: 0.9828 - val_loss: 2.0602 - val_accuracy: 0.5310\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0587 - accuracy: 0.9817 - val_loss: 2.0952 - val_accuracy: 0.5398\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0617 - accuracy: 0.9812 - val_loss: 2.0892 - val_accuracy: 0.5288\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0601 - accuracy: 0.9817 - val_loss: 2.0738 - val_accuracy: 0.5310\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0525 - accuracy: 0.9834 - val_loss: 2.1663 - val_accuracy: 0.5354\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0581 - accuracy: 0.9823 - val_loss: 2.0168 - val_accuracy: 0.5398\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0441 - accuracy: 0.9884 - val_loss: 2.0222 - val_accuracy: 0.5332\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0518 - accuracy: 0.9861 - val_loss: 2.0773 - val_accuracy: 0.5310\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0464 - accuracy: 0.9856 - val_loss: 2.0509 - val_accuracy: 0.5442\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0594 - accuracy: 0.9828 - val_loss: 2.1309 - val_accuracy: 0.5465\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0407 - accuracy: 0.9928 - val_loss: 2.0644 - val_accuracy: 0.5509\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0404 - accuracy: 0.9873 - val_loss: 2.1542 - val_accuracy: 0.5442\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0478 - accuracy: 0.9845 - val_loss: 2.0524 - val_accuracy: 0.5487\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0462 - accuracy: 0.9856 - val_loss: 2.0573 - val_accuracy: 0.5420\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0518 - accuracy: 0.9850 - val_loss: 2.1345 - val_accuracy: 0.5420\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0447 - accuracy: 0.9867 - val_loss: 2.1391 - val_accuracy: 0.5398\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0376 - accuracy: 0.9911 - val_loss: 2.0927 - val_accuracy: 0.5398\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0404 - accuracy: 0.9895 - val_loss: 1.9946 - val_accuracy: 0.5398\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0371 - accuracy: 0.9889 - val_loss: 2.0784 - val_accuracy: 0.5376\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0405 - accuracy: 0.9873 - val_loss: 2.0956 - val_accuracy: 0.5376\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0353 - accuracy: 0.9906 - val_loss: 2.1361 - val_accuracy: 0.5442\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0338 - accuracy: 0.9906 - val_loss: 2.2692 - val_accuracy: 0.5354\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0329 - accuracy: 0.9928 - val_loss: 2.1242 - val_accuracy: 0.5310\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.0362 - accuracy: 0.9895 - val_loss: 2.2977 - val_accuracy: 0.5288\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 53.133\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1072, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657829f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "505b411f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 7.2074 - accuracy: 0.1889 - val_loss: 2.3616 - val_accuracy: 0.3717\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 3.1826 - accuracy: 0.2814 - val_loss: 2.1976 - val_accuracy: 0.3916\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.4598 - accuracy: 0.3662 - val_loss: 2.0635 - val_accuracy: 0.4403\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 2.2375 - accuracy: 0.4127 - val_loss: 1.9668 - val_accuracy: 0.4624\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.9623 - accuracy: 0.4532 - val_loss: 1.8980 - val_accuracy: 0.4602\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.8470 - accuracy: 0.4803 - val_loss: 1.9042 - val_accuracy: 0.4735\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.7132 - accuracy: 0.5102 - val_loss: 1.8414 - val_accuracy: 0.4735\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.6430 - accuracy: 0.5313 - val_loss: 1.8142 - val_accuracy: 0.4889\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.5658 - accuracy: 0.5429 - val_loss: 1.7788 - val_accuracy: 0.4912\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4874 - accuracy: 0.5529 - val_loss: 1.8030 - val_accuracy: 0.4845\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.4320 - accuracy: 0.5745 - val_loss: 1.7909 - val_accuracy: 0.4912\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3689 - accuracy: 0.5828 - val_loss: 1.7415 - val_accuracy: 0.4912\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3476 - accuracy: 0.6000 - val_loss: 1.7185 - val_accuracy: 0.5066\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.3150 - accuracy: 0.6061 - val_loss: 1.7449 - val_accuracy: 0.4912\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2555 - accuracy: 0.6238 - val_loss: 1.7040 - val_accuracy: 0.5199\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.2159 - accuracy: 0.6321 - val_loss: 1.7106 - val_accuracy: 0.5111\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1764 - accuracy: 0.6399 - val_loss: 1.7361 - val_accuracy: 0.5111\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.1057 - accuracy: 0.6526 - val_loss: 1.7468 - val_accuracy: 0.4956\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 1.0777 - accuracy: 0.6626 - val_loss: 1.7290 - val_accuracy: 0.5133\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.0999 - accuracy: 0.6604 - val_loss: 1.6796 - val_accuracy: 0.5243\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 1.0378 - accuracy: 0.6831 - val_loss: 1.7288 - val_accuracy: 0.5133\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.9790 - accuracy: 0.6903 - val_loss: 1.7138 - val_accuracy: 0.5265\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 21s 11ms/step - loss: 0.9604 - accuracy: 0.7075 - val_loss: 1.7082 - val_accuracy: 0.5265\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9528 - accuracy: 0.7086 - val_loss: 1.7486 - val_accuracy: 0.5155\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.9167 - accuracy: 0.7169 - val_loss: 1.7021 - val_accuracy: 0.5155\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8794 - accuracy: 0.7302 - val_loss: 1.6979 - val_accuracy: 0.5243\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8266 - accuracy: 0.7391 - val_loss: 1.6997 - val_accuracy: 0.5133\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8302 - accuracy: 0.7352 - val_loss: 1.7017 - val_accuracy: 0.5310\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.8138 - accuracy: 0.7468 - val_loss: 1.6600 - val_accuracy: 0.5354\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7750 - accuracy: 0.7529 - val_loss: 1.6809 - val_accuracy: 0.5354\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7799 - accuracy: 0.7573 - val_loss: 1.6969 - val_accuracy: 0.5332\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7358 - accuracy: 0.7712 - val_loss: 1.6962 - val_accuracy: 0.5177\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.7277 - accuracy: 0.7756 - val_loss: 1.7070 - val_accuracy: 0.5332\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6779 - accuracy: 0.7989 - val_loss: 1.7536 - val_accuracy: 0.5243\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6916 - accuracy: 0.7917 - val_loss: 1.6818 - val_accuracy: 0.5398\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6750 - accuracy: 0.7945 - val_loss: 1.6713 - val_accuracy: 0.5354\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6489 - accuracy: 0.7994 - val_loss: 1.7372 - val_accuracy: 0.5376\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6046 - accuracy: 0.8072 - val_loss: 1.7051 - val_accuracy: 0.5310\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.6136 - accuracy: 0.8100 - val_loss: 1.7570 - val_accuracy: 0.5199\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5901 - accuracy: 0.8161 - val_loss: 1.7421 - val_accuracy: 0.5265\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5675 - accuracy: 0.8216 - val_loss: 1.8071 - val_accuracy: 0.5354\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5959 - accuracy: 0.8094 - val_loss: 1.6941 - val_accuracy: 0.5243\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5482 - accuracy: 0.8332 - val_loss: 1.7650 - val_accuracy: 0.5177\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5323 - accuracy: 0.8366 - val_loss: 1.7280 - val_accuracy: 0.5376\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5684 - accuracy: 0.8382 - val_loss: 1.7327 - val_accuracy: 0.5376\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5193 - accuracy: 0.8476 - val_loss: 1.7921 - val_accuracy: 0.5310\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5002 - accuracy: 0.8460 - val_loss: 1.7537 - val_accuracy: 0.5199\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.5146 - accuracy: 0.8404 - val_loss: 1.7723 - val_accuracy: 0.5332\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4935 - accuracy: 0.8454 - val_loss: 1.7381 - val_accuracy: 0.5310\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4502 - accuracy: 0.8543 - val_loss: 1.7865 - val_accuracy: 0.5487\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4881 - accuracy: 0.8488 - val_loss: 1.7422 - val_accuracy: 0.5354\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4424 - accuracy: 0.8626 - val_loss: 1.6962 - val_accuracy: 0.5465\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4470 - accuracy: 0.8615 - val_loss: 1.6763 - val_accuracy: 0.5575\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4290 - accuracy: 0.8565 - val_loss: 1.8435 - val_accuracy: 0.5354\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4300 - accuracy: 0.8626 - val_loss: 1.7336 - val_accuracy: 0.5465\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3953 - accuracy: 0.8787 - val_loss: 1.7994 - val_accuracy: 0.5310\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.4096 - accuracy: 0.8776 - val_loss: 1.7811 - val_accuracy: 0.5575\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3861 - accuracy: 0.8759 - val_loss: 1.8097 - val_accuracy: 0.5553\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3789 - accuracy: 0.8753 - val_loss: 1.7729 - val_accuracy: 0.5553\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3723 - accuracy: 0.8859 - val_loss: 1.8578 - val_accuracy: 0.5354\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3513 - accuracy: 0.8864 - val_loss: 1.8352 - val_accuracy: 0.5354\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3589 - accuracy: 0.8870 - val_loss: 1.8165 - val_accuracy: 0.5442\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3590 - accuracy: 0.8853 - val_loss: 1.7583 - val_accuracy: 0.5420\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3396 - accuracy: 0.8947 - val_loss: 1.8229 - val_accuracy: 0.5487\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3311 - accuracy: 0.8970 - val_loss: 1.7840 - val_accuracy: 0.5442\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3591 - accuracy: 0.8909 - val_loss: 1.8788 - val_accuracy: 0.5376\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3229 - accuracy: 0.9008 - val_loss: 1.8219 - val_accuracy: 0.5354\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3533 - accuracy: 0.8859 - val_loss: 1.8088 - val_accuracy: 0.5487\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2877 - accuracy: 0.9102 - val_loss: 1.8645 - val_accuracy: 0.5354\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3066 - accuracy: 0.9030 - val_loss: 1.8324 - val_accuracy: 0.5354\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3168 - accuracy: 0.9042 - val_loss: 1.8539 - val_accuracy: 0.5398\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2878 - accuracy: 0.9075 - val_loss: 1.9251 - val_accuracy: 0.5332\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2939 - accuracy: 0.9047 - val_loss: 1.8995 - val_accuracy: 0.5487\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3320 - accuracy: 0.8925 - val_loss: 1.8299 - val_accuracy: 0.5442\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3149 - accuracy: 0.8981 - val_loss: 1.7903 - val_accuracy: 0.5465\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3044 - accuracy: 0.9047 - val_loss: 1.7735 - val_accuracy: 0.5442\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3002 - accuracy: 0.9042 - val_loss: 1.8929 - val_accuracy: 0.5398\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.3037 - accuracy: 0.9119 - val_loss: 1.8834 - val_accuracy: 0.5619\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2699 - accuracy: 0.9086 - val_loss: 1.8064 - val_accuracy: 0.5509\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2846 - accuracy: 0.9130 - val_loss: 1.8963 - val_accuracy: 0.5619\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2639 - accuracy: 0.9180 - val_loss: 1.8857 - val_accuracy: 0.5664\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2509 - accuracy: 0.9213 - val_loss: 1.8421 - val_accuracy: 0.5465\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2559 - accuracy: 0.9219 - val_loss: 1.8064 - val_accuracy: 0.5597\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2296 - accuracy: 0.9269 - val_loss: 1.8549 - val_accuracy: 0.5575\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2406 - accuracy: 0.9269 - val_loss: 1.8541 - val_accuracy: 0.5531\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2350 - accuracy: 0.9285 - val_loss: 1.7952 - val_accuracy: 0.5752\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2492 - accuracy: 0.9269 - val_loss: 1.8514 - val_accuracy: 0.5531\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2657 - accuracy: 0.9163 - val_loss: 1.8556 - val_accuracy: 0.5465\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2308 - accuracy: 0.9285 - val_loss: 1.7862 - val_accuracy: 0.5597\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2098 - accuracy: 0.9352 - val_loss: 1.8231 - val_accuracy: 0.5642\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2175 - accuracy: 0.9258 - val_loss: 1.9331 - val_accuracy: 0.5442\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2163 - accuracy: 0.9319 - val_loss: 1.9986 - val_accuracy: 0.5420\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1866 - accuracy: 0.9429 - val_loss: 1.8981 - val_accuracy: 0.5553\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2178 - accuracy: 0.9363 - val_loss: 1.8958 - val_accuracy: 0.5487\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2082 - accuracy: 0.9291 - val_loss: 1.8547 - val_accuracy: 0.5664\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2141 - accuracy: 0.9319 - val_loss: 1.8479 - val_accuracy: 0.5642\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2189 - accuracy: 0.9324 - val_loss: 1.9662 - val_accuracy: 0.5509\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.2046 - accuracy: 0.9385 - val_loss: 1.9615 - val_accuracy: 0.5597\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1946 - accuracy: 0.9357 - val_loss: 1.8784 - val_accuracy: 0.5642\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 0.1858 - accuracy: 0.9413 - val_loss: 1.8563 - val_accuracy: 0.5686\n",
      "399/399 [==============================] - 4s 9ms/step\n",
      "> 54.887\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(1072, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb74eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 30s 17ms/step - loss: 7.0344 - accuracy: 0.1391 - val_loss: 2.3156 - val_accuracy: 0.3429\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 3.2548 - accuracy: 0.2310 - val_loss: 2.2940 - val_accuracy: 0.3341\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 2.8220 - accuracy: 0.2515 - val_loss: 2.2742 - val_accuracy: 0.3850\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 2.5864 - accuracy: 0.3069 - val_loss: 2.1330 - val_accuracy: 0.4004\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 2.4038 - accuracy: 0.3391 - val_loss: 2.0948 - val_accuracy: 0.4159\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 2.2340 - accuracy: 0.3723 - val_loss: 2.0304 - val_accuracy: 0.4270\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 2.1383 - accuracy: 0.3956 - val_loss: 1.9759 - val_accuracy: 0.4358\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 2.0741 - accuracy: 0.4150 - val_loss: 1.9045 - val_accuracy: 0.4558\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 2.0069 - accuracy: 0.4349 - val_loss: 1.8689 - val_accuracy: 0.4558\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.8898 - accuracy: 0.4648 - val_loss: 1.8382 - val_accuracy: 0.4889\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.8177 - accuracy: 0.4709 - val_loss: 1.8393 - val_accuracy: 0.4712\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7533 - accuracy: 0.4870 - val_loss: 1.7939 - val_accuracy: 0.4447\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7416 - accuracy: 0.5014 - val_loss: 1.7598 - val_accuracy: 0.4779\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6817 - accuracy: 0.5125 - val_loss: 1.7588 - val_accuracy: 0.4779\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6707 - accuracy: 0.5163 - val_loss: 1.7587 - val_accuracy: 0.4912\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6379 - accuracy: 0.5114 - val_loss: 1.7330 - val_accuracy: 0.4867\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5904 - accuracy: 0.5407 - val_loss: 1.7282 - val_accuracy: 0.4956\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5627 - accuracy: 0.5302 - val_loss: 1.7208 - val_accuracy: 0.4934\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5485 - accuracy: 0.5435 - val_loss: 1.7265 - val_accuracy: 0.4845\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5171 - accuracy: 0.5330 - val_loss: 1.7303 - val_accuracy: 0.4801\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 1.4896 - accuracy: 0.5501 - val_loss: 1.7103 - val_accuracy: 0.4867\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4805 - accuracy: 0.5607 - val_loss: 1.7054 - val_accuracy: 0.5044\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4393 - accuracy: 0.5701 - val_loss: 1.7268 - val_accuracy: 0.5022\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4209 - accuracy: 0.5812 - val_loss: 1.7192 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3881 - accuracy: 0.5828 - val_loss: 1.7056 - val_accuracy: 0.5111\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3842 - accuracy: 0.5795 - val_loss: 1.7022 - val_accuracy: 0.5022\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 1.3455 - accuracy: 0.5989 - val_loss: 1.6843 - val_accuracy: 0.5133\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 1.3322 - accuracy: 0.5950 - val_loss: 1.7098 - val_accuracy: 0.5088\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 1.3098 - accuracy: 0.6083 - val_loss: 1.6951 - val_accuracy: 0.5199\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3097 - accuracy: 0.5967 - val_loss: 1.6812 - val_accuracy: 0.5155\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2649 - accuracy: 0.6155 - val_loss: 1.6865 - val_accuracy: 0.5177\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2610 - accuracy: 0.6155 - val_loss: 1.6717 - val_accuracy: 0.5066\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2400 - accuracy: 0.6238 - val_loss: 1.6639 - val_accuracy: 0.5221\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2330 - accuracy: 0.6161 - val_loss: 1.6597 - val_accuracy: 0.5199\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1900 - accuracy: 0.6355 - val_loss: 1.6294 - val_accuracy: 0.5354\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1995 - accuracy: 0.6338 - val_loss: 1.6407 - val_accuracy: 0.5177\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1531 - accuracy: 0.6432 - val_loss: 1.6233 - val_accuracy: 0.5310\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1170 - accuracy: 0.6560 - val_loss: 1.6365 - val_accuracy: 0.5265\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1188 - accuracy: 0.6598 - val_loss: 1.6477 - val_accuracy: 0.5243\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0990 - accuracy: 0.6576 - val_loss: 1.6238 - val_accuracy: 0.5398\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0597 - accuracy: 0.6753 - val_loss: 1.6241 - val_accuracy: 0.5354\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0488 - accuracy: 0.6687 - val_loss: 1.6122 - val_accuracy: 0.5332\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0056 - accuracy: 0.7030 - val_loss: 1.6131 - val_accuracy: 0.5531\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.0217 - accuracy: 0.6809 - val_loss: 1.6019 - val_accuracy: 0.5553\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9888 - accuracy: 0.6864 - val_loss: 1.6163 - val_accuracy: 0.5597\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9640 - accuracy: 0.6970 - val_loss: 1.5967 - val_accuracy: 0.5487\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9305 - accuracy: 0.7147 - val_loss: 1.6524 - val_accuracy: 0.5288\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9063 - accuracy: 0.7197 - val_loss: 1.5891 - val_accuracy: 0.5332\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9149 - accuracy: 0.7163 - val_loss: 1.5813 - val_accuracy: 0.5509\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8789 - accuracy: 0.7247 - val_loss: 1.5976 - val_accuracy: 0.5420\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8648 - accuracy: 0.7247 - val_loss: 1.6137 - val_accuracy: 0.5509\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8260 - accuracy: 0.7407 - val_loss: 1.5778 - val_accuracy: 0.5420\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8439 - accuracy: 0.7324 - val_loss: 1.5785 - val_accuracy: 0.5509\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8002 - accuracy: 0.7540 - val_loss: 1.5927 - val_accuracy: 0.5465\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8066 - accuracy: 0.7568 - val_loss: 1.5774 - val_accuracy: 0.5619\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7669 - accuracy: 0.7618 - val_loss: 1.5608 - val_accuracy: 0.5332\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7478 - accuracy: 0.7623 - val_loss: 1.5679 - val_accuracy: 0.5465\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6871 - accuracy: 0.7839 - val_loss: 1.5135 - val_accuracy: 0.5509\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6860 - accuracy: 0.7922 - val_loss: 1.5441 - val_accuracy: 0.5553\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6688 - accuracy: 0.7917 - val_loss: 1.5644 - val_accuracy: 0.5575\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6417 - accuracy: 0.7967 - val_loss: 1.6216 - val_accuracy: 0.5376\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6276 - accuracy: 0.7922 - val_loss: 1.5629 - val_accuracy: 0.5442\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5847 - accuracy: 0.8161 - val_loss: 1.5669 - val_accuracy: 0.5553\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5750 - accuracy: 0.8238 - val_loss: 1.6033 - val_accuracy: 0.5487\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5728 - accuracy: 0.8338 - val_loss: 1.5438 - val_accuracy: 0.5597\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5543 - accuracy: 0.8338 - val_loss: 1.5652 - val_accuracy: 0.5575\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5422 - accuracy: 0.8266 - val_loss: 1.6123 - val_accuracy: 0.5509\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5451 - accuracy: 0.8338 - val_loss: 1.5665 - val_accuracy: 0.5619\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5184 - accuracy: 0.8332 - val_loss: 1.5357 - val_accuracy: 0.5686\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4985 - accuracy: 0.8399 - val_loss: 1.5649 - val_accuracy: 0.5708\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4940 - accuracy: 0.8460 - val_loss: 1.5608 - val_accuracy: 0.5686\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4978 - accuracy: 0.8432 - val_loss: 1.5770 - val_accuracy: 0.5531\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4559 - accuracy: 0.8615 - val_loss: 1.5722 - val_accuracy: 0.5509\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4213 - accuracy: 0.8787 - val_loss: 1.5698 - val_accuracy: 0.5664\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4193 - accuracy: 0.8693 - val_loss: 1.5752 - val_accuracy: 0.5575\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4064 - accuracy: 0.8737 - val_loss: 1.6093 - val_accuracy: 0.5420\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4128 - accuracy: 0.8643 - val_loss: 1.5954 - val_accuracy: 0.5531\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3917 - accuracy: 0.8776 - val_loss: 1.6372 - val_accuracy: 0.5465\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3573 - accuracy: 0.8864 - val_loss: 1.5986 - val_accuracy: 0.5531\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3705 - accuracy: 0.8809 - val_loss: 1.6014 - val_accuracy: 0.5487\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3372 - accuracy: 0.8920 - val_loss: 1.6329 - val_accuracy: 0.5487\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3405 - accuracy: 0.8964 - val_loss: 1.6344 - val_accuracy: 0.5509\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3245 - accuracy: 0.9003 - val_loss: 1.6116 - val_accuracy: 0.5553\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3266 - accuracy: 0.8920 - val_loss: 1.5995 - val_accuracy: 0.5531\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3091 - accuracy: 0.9053 - val_loss: 1.6786 - val_accuracy: 0.5487\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3057 - accuracy: 0.9008 - val_loss: 1.6034 - val_accuracy: 0.5487\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2840 - accuracy: 0.9180 - val_loss: 1.6363 - val_accuracy: 0.5553\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3021 - accuracy: 0.8997 - val_loss: 1.6232 - val_accuracy: 0.5597\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2926 - accuracy: 0.9119 - val_loss: 1.6300 - val_accuracy: 0.5376\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2700 - accuracy: 0.9141 - val_loss: 1.6793 - val_accuracy: 0.5465\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2847 - accuracy: 0.9114 - val_loss: 1.6093 - val_accuracy: 0.5619\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2899 - accuracy: 0.9064 - val_loss: 1.6020 - val_accuracy: 0.5664\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2299 - accuracy: 0.9296 - val_loss: 1.5925 - val_accuracy: 0.5730\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 28s 15ms/step - loss: 0.2440 - accuracy: 0.9274 - val_loss: 1.6443 - val_accuracy: 0.5730\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 31s 17ms/step - loss: 0.2315 - accuracy: 0.9258 - val_loss: 1.7072 - val_accuracy: 0.5531\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 31s 17ms/step - loss: 0.2420 - accuracy: 0.9302 - val_loss: 1.6249 - val_accuracy: 0.5686\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 31s 17ms/step - loss: 0.2086 - accuracy: 0.9368 - val_loss: 1.6770 - val_accuracy: 0.5509\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 31s 17ms/step - loss: 0.2122 - accuracy: 0.9335 - val_loss: 1.6644 - val_accuracy: 0.5597\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2221 - accuracy: 0.9319 - val_loss: 1.7061 - val_accuracy: 0.5531\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2270 - accuracy: 0.9335 - val_loss: 1.6743 - val_accuracy: 0.5686\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 58.897\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(2048, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)        \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2570b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 6.1925 - accuracy: 0.2172 - val_loss: 2.2873 - val_accuracy: 0.3739\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 2.4739 - accuracy: 0.3873 - val_loss: 2.0661 - val_accuracy: 0.4314\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.9088 - accuracy: 0.4615 - val_loss: 1.9328 - val_accuracy: 0.4580\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.6624 - accuracy: 0.5263 - val_loss: 1.8849 - val_accuracy: 0.4535\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.4295 - accuracy: 0.5795 - val_loss: 1.8193 - val_accuracy: 0.4867\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.2320 - accuracy: 0.6305 - val_loss: 1.7959 - val_accuracy: 0.4867\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 1.0762 - accuracy: 0.6748 - val_loss: 1.7754 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.9318 - accuracy: 0.7235 - val_loss: 1.8129 - val_accuracy: 0.4867\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.8448 - accuracy: 0.7396 - val_loss: 1.7530 - val_accuracy: 0.5066\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.7125 - accuracy: 0.7834 - val_loss: 1.7731 - val_accuracy: 0.5088\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.6337 - accuracy: 0.8033 - val_loss: 1.7829 - val_accuracy: 0.5199\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.5573 - accuracy: 0.8294 - val_loss: 1.7399 - val_accuracy: 0.5177\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4847 - accuracy: 0.8565 - val_loss: 1.7477 - val_accuracy: 0.5221\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.4123 - accuracy: 0.8809 - val_loss: 1.8011 - val_accuracy: 0.5265\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3518 - accuracy: 0.8936 - val_loss: 1.7971 - val_accuracy: 0.5221\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.3238 - accuracy: 0.9075 - val_loss: 1.8041 - val_accuracy: 0.5221\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2845 - accuracy: 0.9230 - val_loss: 1.7856 - val_accuracy: 0.5376\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2664 - accuracy: 0.9291 - val_loss: 1.8310 - val_accuracy: 0.5177\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.2172 - accuracy: 0.9391 - val_loss: 1.9099 - val_accuracy: 0.5177\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1990 - accuracy: 0.9413 - val_loss: 1.8603 - val_accuracy: 0.5310\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1935 - accuracy: 0.9457 - val_loss: 1.8513 - val_accuracy: 0.5288\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1754 - accuracy: 0.9496 - val_loss: 1.8641 - val_accuracy: 0.5332\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1658 - accuracy: 0.9529 - val_loss: 1.8210 - val_accuracy: 0.5487\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1477 - accuracy: 0.9612 - val_loss: 1.8811 - val_accuracy: 0.5376\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1327 - accuracy: 0.9618 - val_loss: 1.8766 - val_accuracy: 0.5354\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1260 - accuracy: 0.9629 - val_loss: 1.8449 - val_accuracy: 0.5420\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1272 - accuracy: 0.9634 - val_loss: 1.9113 - val_accuracy: 0.5354\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0884 - accuracy: 0.9784 - val_loss: 1.9412 - val_accuracy: 0.5133\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.1031 - accuracy: 0.9751 - val_loss: 1.9312 - val_accuracy: 0.5354\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0886 - accuracy: 0.9784 - val_loss: 2.0091 - val_accuracy: 0.5221\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0830 - accuracy: 0.9839 - val_loss: 1.9103 - val_accuracy: 0.5310\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0848 - accuracy: 0.9751 - val_loss: 2.0205 - val_accuracy: 0.5199\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0763 - accuracy: 0.9812 - val_loss: 1.9791 - val_accuracy: 0.5155\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0716 - accuracy: 0.9834 - val_loss: 1.9406 - val_accuracy: 0.5288\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0712 - accuracy: 0.9856 - val_loss: 1.9353 - val_accuracy: 0.5376\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0681 - accuracy: 0.9812 - val_loss: 1.9676 - val_accuracy: 0.5288\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0623 - accuracy: 0.9823 - val_loss: 2.0012 - val_accuracy: 0.5310\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0705 - accuracy: 0.9784 - val_loss: 2.0143 - val_accuracy: 0.5398\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0570 - accuracy: 0.9884 - val_loss: 2.0506 - val_accuracy: 0.5288\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0613 - accuracy: 0.9806 - val_loss: 1.9842 - val_accuracy: 0.5310\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0650 - accuracy: 0.9828 - val_loss: 1.9790 - val_accuracy: 0.5465\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0505 - accuracy: 0.9873 - val_loss: 2.0079 - val_accuracy: 0.5420\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0469 - accuracy: 0.9922 - val_loss: 2.0656 - val_accuracy: 0.5288\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0537 - accuracy: 0.9867 - val_loss: 2.0657 - val_accuracy: 0.5265\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 0.0494 - accuracy: 0.9884 - val_loss: 2.0046 - val_accuracy: 0.5398\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0433 - accuracy: 0.9884 - val_loss: 1.9924 - val_accuracy: 0.5376\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0388 - accuracy: 0.9922 - val_loss: 2.0434 - val_accuracy: 0.5442\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 0.0496 - accuracy: 0.9884 - val_loss: 1.9951 - val_accuracy: 0.5354\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 0.0403 - accuracy: 0.9917 - val_loss: 2.0417 - val_accuracy: 0.5310\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0340 - accuracy: 0.9917 - val_loss: 2.0683 - val_accuracy: 0.5332\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0394 - accuracy: 0.9922 - val_loss: 2.0873 - val_accuracy: 0.5265\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0405 - accuracy: 0.9917 - val_loss: 2.0372 - val_accuracy: 0.5420\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 0.0360 - accuracy: 0.9922 - val_loss: 2.0393 - val_accuracy: 0.5376\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0344 - accuracy: 0.9917 - val_loss: 1.9898 - val_accuracy: 0.5398\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 0.0326 - accuracy: 0.9900 - val_loss: 1.9974 - val_accuracy: 0.5332\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 0.0249 - accuracy: 0.9950 - val_loss: 2.0355 - val_accuracy: 0.5376\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0324 - accuracy: 0.9934 - val_loss: 2.0131 - val_accuracy: 0.5354\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0269 - accuracy: 0.9939 - val_loss: 2.0404 - val_accuracy: 0.5288\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0329 - accuracy: 0.9911 - val_loss: 2.1316 - val_accuracy: 0.5265\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 24s 14ms/step - loss: 0.0295 - accuracy: 0.9934 - val_loss: 2.0957 - val_accuracy: 0.5265\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0314 - accuracy: 0.9917 - val_loss: 2.0638 - val_accuracy: 0.5221\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0232 - accuracy: 0.9956 - val_loss: 2.0493 - val_accuracy: 0.5199\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0328 - accuracy: 0.9917 - val_loss: 2.0153 - val_accuracy: 0.5332\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0224 - accuracy: 0.9950 - val_loss: 2.0788 - val_accuracy: 0.5420\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0254 - accuracy: 0.9945 - val_loss: 2.0289 - val_accuracy: 0.5398\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0274 - accuracy: 0.9939 - val_loss: 1.9660 - val_accuracy: 0.5465\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0265 - accuracy: 0.9945 - val_loss: 2.0863 - val_accuracy: 0.5398\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0200 - accuracy: 0.9961 - val_loss: 2.0710 - val_accuracy: 0.5354\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 2.1255 - val_accuracy: 0.5288\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0333 - accuracy: 0.9906 - val_loss: 2.0581 - val_accuracy: 0.5398\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0249 - accuracy: 0.9961 - val_loss: 2.0823 - val_accuracy: 0.5332\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0241 - accuracy: 0.9950 - val_loss: 2.0999 - val_accuracy: 0.5332\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0198 - accuracy: 0.9956 - val_loss: 2.0813 - val_accuracy: 0.5376\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0192 - accuracy: 0.9967 - val_loss: 2.0987 - val_accuracy: 0.5465\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0222 - accuracy: 0.9945 - val_loss: 2.1266 - val_accuracy: 0.5332\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0196 - accuracy: 0.9956 - val_loss: 2.0910 - val_accuracy: 0.5442\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0179 - accuracy: 0.9972 - val_loss: 2.1232 - val_accuracy: 0.5376\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0197 - accuracy: 0.9961 - val_loss: 2.1332 - val_accuracy: 0.5332\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0175 - accuracy: 0.9961 - val_loss: 2.1185 - val_accuracy: 0.5420\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0160 - accuracy: 0.9983 - val_loss: 2.1798 - val_accuracy: 0.5310\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0156 - accuracy: 0.9967 - val_loss: 2.1409 - val_accuracy: 0.5332\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0174 - accuracy: 0.9961 - val_loss: 2.1046 - val_accuracy: 0.5465\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0204 - accuracy: 0.9945 - val_loss: 2.1721 - val_accuracy: 0.5354\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0193 - accuracy: 0.9961 - val_loss: 2.1191 - val_accuracy: 0.5465\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 0.0153 - accuracy: 0.9961 - val_loss: 2.1101 - val_accuracy: 0.5509\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0148 - accuracy: 0.9967 - val_loss: 2.1473 - val_accuracy: 0.5442\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0243 - accuracy: 0.9956 - val_loss: 2.1922 - val_accuracy: 0.5442\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0189 - accuracy: 0.9967 - val_loss: 2.1189 - val_accuracy: 0.5531\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0143 - accuracy: 0.9978 - val_loss: 2.0452 - val_accuracy: 0.5420\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0168 - accuracy: 0.9956 - val_loss: 2.1491 - val_accuracy: 0.5376\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0199 - accuracy: 0.9934 - val_loss: 2.0853 - val_accuracy: 0.5376\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0217 - accuracy: 0.9939 - val_loss: 2.2454 - val_accuracy: 0.5310\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0213 - accuracy: 0.9950 - val_loss: 2.1299 - val_accuracy: 0.5376\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0216 - accuracy: 0.9934 - val_loss: 2.1008 - val_accuracy: 0.5398\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0188 - accuracy: 0.9956 - val_loss: 2.0915 - val_accuracy: 0.5420\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0148 - accuracy: 0.9961 - val_loss: 2.1461 - val_accuracy: 0.5420\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0163 - accuracy: 0.9972 - val_loss: 2.1272 - val_accuracy: 0.5442\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0154 - accuracy: 0.9967 - val_loss: 2.2027 - val_accuracy: 0.5332\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 2.1969 - val_accuracy: 0.5376\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 24s 13ms/step - loss: 0.0107 - accuracy: 0.9983 - val_loss: 2.2567 - val_accuracy: 0.5332\n",
      "399/399 [==============================] - 4s 9ms/step\n",
      "> 54.386\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(2048, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)        \n",
    "\tclass1 = Dense(512, activation='relu')(class1)   \n",
    "\tclass1 = Dense(256, activation='relu')(class1)   \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e21440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 30s 17ms/step - loss: 7.8197 - accuracy: 0.1895 - val_loss: 2.2745 - val_accuracy: 0.3673\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 3.0071 - accuracy: 0.3219 - val_loss: 2.1334 - val_accuracy: 0.4336\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.4713 - accuracy: 0.3634 - val_loss: 2.0513 - val_accuracy: 0.4358\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.1715 - accuracy: 0.4083 - val_loss: 1.9742 - val_accuracy: 0.4425\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.9824 - accuracy: 0.4460 - val_loss: 1.9017 - val_accuracy: 0.4735\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.8629 - accuracy: 0.4731 - val_loss: 1.8661 - val_accuracy: 0.4823\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7272 - accuracy: 0.5025 - val_loss: 1.8273 - val_accuracy: 0.4712\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6268 - accuracy: 0.5258 - val_loss: 1.8055 - val_accuracy: 0.4712\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5863 - accuracy: 0.5313 - val_loss: 1.7611 - val_accuracy: 0.4912\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4757 - accuracy: 0.5618 - val_loss: 1.7527 - val_accuracy: 0.5044\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4298 - accuracy: 0.5823 - val_loss: 1.7338 - val_accuracy: 0.4934\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3664 - accuracy: 0.5906 - val_loss: 1.6916 - val_accuracy: 0.5243\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2781 - accuracy: 0.6183 - val_loss: 1.7198 - val_accuracy: 0.5265\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2239 - accuracy: 0.6294 - val_loss: 1.6810 - val_accuracy: 0.5177\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2101 - accuracy: 0.6399 - val_loss: 1.6709 - val_accuracy: 0.5177\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0834 - accuracy: 0.6809 - val_loss: 1.6283 - val_accuracy: 0.5398\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.0624 - accuracy: 0.6770 - val_loss: 1.6417 - val_accuracy: 0.5310\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 1.0177 - accuracy: 0.6842 - val_loss: 1.6359 - val_accuracy: 0.5310\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9423 - accuracy: 0.7197 - val_loss: 1.6579 - val_accuracy: 0.5442\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9565 - accuracy: 0.7058 - val_loss: 1.6205 - val_accuracy: 0.5442\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8784 - accuracy: 0.7330 - val_loss: 1.6471 - val_accuracy: 0.5531\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8285 - accuracy: 0.7435 - val_loss: 1.6648 - val_accuracy: 0.5243\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7451 - accuracy: 0.7767 - val_loss: 1.6638 - val_accuracy: 0.5487\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7674 - accuracy: 0.7535 - val_loss: 1.6169 - val_accuracy: 0.5442\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6826 - accuracy: 0.7978 - val_loss: 1.6300 - val_accuracy: 0.5553\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6638 - accuracy: 0.7945 - val_loss: 1.6421 - val_accuracy: 0.5487\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 0.6288 - accuracy: 0.8033 - val_loss: 1.6684 - val_accuracy: 0.5575\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 0.5750 - accuracy: 0.8216 - val_loss: 1.6925 - val_accuracy: 0.5553\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5902 - accuracy: 0.8066 - val_loss: 1.6816 - val_accuracy: 0.5509\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4998 - accuracy: 0.8532 - val_loss: 1.6298 - val_accuracy: 0.5531\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 0.4823 - accuracy: 0.8388 - val_loss: 1.7176 - val_accuracy: 0.5487\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 0.4789 - accuracy: 0.8465 - val_loss: 1.7282 - val_accuracy: 0.5398\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 0.4317 - accuracy: 0.8687 - val_loss: 1.6630 - val_accuracy: 0.5553\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 0.4092 - accuracy: 0.8720 - val_loss: 1.7171 - val_accuracy: 0.5664\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 0.3957 - accuracy: 0.8759 - val_loss: 1.6473 - val_accuracy: 0.5796\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3512 - accuracy: 0.9008 - val_loss: 1.7094 - val_accuracy: 0.5752\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3294 - accuracy: 0.8975 - val_loss: 1.7337 - val_accuracy: 0.5597\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3507 - accuracy: 0.8920 - val_loss: 1.7132 - val_accuracy: 0.5796\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3049 - accuracy: 0.9108 - val_loss: 1.6925 - val_accuracy: 0.5708\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3198 - accuracy: 0.8975 - val_loss: 1.6836 - val_accuracy: 0.5642\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2838 - accuracy: 0.9197 - val_loss: 1.7289 - val_accuracy: 0.5774\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2780 - accuracy: 0.9147 - val_loss: 1.6676 - val_accuracy: 0.5796\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2704 - accuracy: 0.9186 - val_loss: 1.6978 - val_accuracy: 0.5885\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2396 - accuracy: 0.9296 - val_loss: 1.7180 - val_accuracy: 0.5885\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2509 - accuracy: 0.9235 - val_loss: 1.6940 - val_accuracy: 0.5819\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2157 - accuracy: 0.9352 - val_loss: 1.7302 - val_accuracy: 0.5841\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 0.2232 - accuracy: 0.9307 - val_loss: 1.7764 - val_accuracy: 0.5796\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 0.2472 - accuracy: 0.9307 - val_loss: 1.7178 - val_accuracy: 0.5708\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 0.1981 - accuracy: 0.9319 - val_loss: 1.7301 - val_accuracy: 0.5863\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1822 - accuracy: 0.9435 - val_loss: 1.7670 - val_accuracy: 0.5664\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1908 - accuracy: 0.9402 - val_loss: 1.7164 - val_accuracy: 0.5796\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1820 - accuracy: 0.9413 - val_loss: 1.7217 - val_accuracy: 0.5819\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1741 - accuracy: 0.9535 - val_loss: 1.7554 - val_accuracy: 0.5708\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1690 - accuracy: 0.9518 - val_loss: 1.7125 - val_accuracy: 0.5863\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1461 - accuracy: 0.9584 - val_loss: 1.7849 - val_accuracy: 0.5730\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1513 - accuracy: 0.9535 - val_loss: 1.7612 - val_accuracy: 0.5885\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1287 - accuracy: 0.9657 - val_loss: 1.7894 - val_accuracy: 0.5951\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1281 - accuracy: 0.9645 - val_loss: 1.7977 - val_accuracy: 0.5841\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1358 - accuracy: 0.9618 - val_loss: 1.8000 - val_accuracy: 0.5819\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1278 - accuracy: 0.9601 - val_loss: 1.7534 - val_accuracy: 0.5819\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 0.1325 - accuracy: 0.9601 - val_loss: 1.8477 - val_accuracy: 0.5730\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1318 - accuracy: 0.9596 - val_loss: 1.7837 - val_accuracy: 0.5907\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1250 - accuracy: 0.9640 - val_loss: 1.8231 - val_accuracy: 0.5819\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1158 - accuracy: 0.9695 - val_loss: 1.8782 - val_accuracy: 0.5796\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1203 - accuracy: 0.9640 - val_loss: 1.8883 - val_accuracy: 0.5996\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1194 - accuracy: 0.9640 - val_loss: 1.8634 - val_accuracy: 0.5863\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 0.1030 - accuracy: 0.9684 - val_loss: 1.8420 - val_accuracy: 0.5863\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0793 - accuracy: 0.9806 - val_loss: 1.8412 - val_accuracy: 0.5841\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1111 - accuracy: 0.9701 - val_loss: 1.8550 - val_accuracy: 0.5863\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0847 - accuracy: 0.9756 - val_loss: 1.9064 - val_accuracy: 0.5819\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1064 - accuracy: 0.9640 - val_loss: 1.8562 - val_accuracy: 0.5819\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1049 - accuracy: 0.9684 - val_loss: 1.9734 - val_accuracy: 0.5730\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0878 - accuracy: 0.9745 - val_loss: 1.8313 - val_accuracy: 0.5996\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0769 - accuracy: 0.9773 - val_loss: 1.9029 - val_accuracy: 0.5885\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0863 - accuracy: 0.9745 - val_loss: 1.9154 - val_accuracy: 0.5929\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0876 - accuracy: 0.9734 - val_loss: 1.9595 - val_accuracy: 0.5752\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0808 - accuracy: 0.9778 - val_loss: 1.8885 - val_accuracy: 0.5841\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0817 - accuracy: 0.9767 - val_loss: 1.9223 - val_accuracy: 0.5929\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0881 - accuracy: 0.9729 - val_loss: 1.8274 - val_accuracy: 0.5796\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0821 - accuracy: 0.9751 - val_loss: 1.9345 - val_accuracy: 0.5863\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0937 - accuracy: 0.9778 - val_loss: 1.9206 - val_accuracy: 0.5885\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0694 - accuracy: 0.9806 - val_loss: 1.9450 - val_accuracy: 0.5796\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0681 - accuracy: 0.9789 - val_loss: 1.9424 - val_accuracy: 0.5885\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0624 - accuracy: 0.9812 - val_loss: 1.8491 - val_accuracy: 0.5929\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0661 - accuracy: 0.9789 - val_loss: 1.9319 - val_accuracy: 0.5863\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0634 - accuracy: 0.9828 - val_loss: 1.8718 - val_accuracy: 0.6018\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0652 - accuracy: 0.9789 - val_loss: 1.9587 - val_accuracy: 0.5885\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0452 - accuracy: 0.9873 - val_loss: 2.0295 - val_accuracy: 0.5929\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0581 - accuracy: 0.9812 - val_loss: 1.8347 - val_accuracy: 0.6018\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0510 - accuracy: 0.9856 - val_loss: 1.9334 - val_accuracy: 0.6084\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0555 - accuracy: 0.9839 - val_loss: 1.9110 - val_accuracy: 0.5951\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0546 - accuracy: 0.9828 - val_loss: 1.9376 - val_accuracy: 0.5973\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0582 - accuracy: 0.9856 - val_loss: 1.8521 - val_accuracy: 0.5841\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0533 - accuracy: 0.9828 - val_loss: 1.9468 - val_accuracy: 0.5885\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0594 - accuracy: 0.9839 - val_loss: 1.9483 - val_accuracy: 0.5885\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0597 - accuracy: 0.9823 - val_loss: 1.9730 - val_accuracy: 0.5841\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0506 - accuracy: 0.9856 - val_loss: 1.9825 - val_accuracy: 0.5951\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0562 - accuracy: 0.9839 - val_loss: 1.9193 - val_accuracy: 0.5929\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0483 - accuracy: 0.9867 - val_loss: 1.9352 - val_accuracy: 0.5929\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.0482 - accuracy: 0.9867 - val_loss: 1.9116 - val_accuracy: 0.5929\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 57.644\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(2048, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.5)(class1)        \n",
    "\tclass1 = Dense(512, activation='relu')(class1)   \n",
    "\tclass1 = Dense(256, activation='relu')(class1)   \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ebff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 29s 16ms/step - loss: 8.9684 - accuracy: 0.1413 - val_loss: 2.4853 - val_accuracy: 0.2854\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 3.9361 - accuracy: 0.1906 - val_loss: 2.6068 - val_accuracy: 0.3119\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 3.2799 - accuracy: 0.2044 - val_loss: 2.5169 - val_accuracy: 0.3739\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.9853 - accuracy: 0.2238 - val_loss: 2.5110 - val_accuracy: 0.3739\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.8223 - accuracy: 0.2560 - val_loss: 2.5052 - val_accuracy: 0.3761\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.6255 - accuracy: 0.2670 - val_loss: 2.4671 - val_accuracy: 0.3960\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.5220 - accuracy: 0.2947 - val_loss: 2.3824 - val_accuracy: 0.3650\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.4806 - accuracy: 0.2953 - val_loss: 2.3562 - val_accuracy: 0.4137\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.4363 - accuracy: 0.3102 - val_loss: 2.3137 - val_accuracy: 0.4181\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.3053 - accuracy: 0.3374 - val_loss: 2.2291 - val_accuracy: 0.4336\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.2604 - accuracy: 0.3501 - val_loss: 2.1515 - val_accuracy: 0.4336\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.2275 - accuracy: 0.3690 - val_loss: 2.0560 - val_accuracy: 0.4425\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.1524 - accuracy: 0.3900 - val_loss: 2.0011 - val_accuracy: 0.4513\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.0727 - accuracy: 0.3922 - val_loss: 1.9724 - val_accuracy: 0.4447\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.0543 - accuracy: 0.4100 - val_loss: 1.9286 - val_accuracy: 0.4469\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.0190 - accuracy: 0.4216 - val_loss: 1.9428 - val_accuracy: 0.4535\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.9652 - accuracy: 0.4388 - val_loss: 1.8920 - val_accuracy: 0.4624\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.9487 - accuracy: 0.4393 - val_loss: 1.8794 - val_accuracy: 0.4712\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.9133 - accuracy: 0.4532 - val_loss: 1.8639 - val_accuracy: 0.4668\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.8250 - accuracy: 0.4753 - val_loss: 1.8459 - val_accuracy: 0.4602\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.8079 - accuracy: 0.4787 - val_loss: 1.8090 - val_accuracy: 0.4646\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.8141 - accuracy: 0.4670 - val_loss: 1.7979 - val_accuracy: 0.4646\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.7738 - accuracy: 0.4753 - val_loss: 1.8152 - val_accuracy: 0.4580\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7613 - accuracy: 0.4781 - val_loss: 1.8153 - val_accuracy: 0.4513\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7313 - accuracy: 0.4886 - val_loss: 1.8294 - val_accuracy: 0.4602\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7168 - accuracy: 0.4859 - val_loss: 1.7907 - val_accuracy: 0.4602\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6858 - accuracy: 0.4964 - val_loss: 1.7834 - val_accuracy: 0.4602\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6799 - accuracy: 0.5042 - val_loss: 1.7977 - val_accuracy: 0.4580\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6637 - accuracy: 0.5086 - val_loss: 1.7773 - val_accuracy: 0.4889\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6751 - accuracy: 0.5091 - val_loss: 1.7730 - val_accuracy: 0.4823\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6417 - accuracy: 0.5102 - val_loss: 1.7778 - val_accuracy: 0.4801\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5989 - accuracy: 0.5130 - val_loss: 1.7701 - val_accuracy: 0.5022\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6147 - accuracy: 0.5208 - val_loss: 1.7816 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5824 - accuracy: 0.5175 - val_loss: 1.7628 - val_accuracy: 0.5022\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5890 - accuracy: 0.5274 - val_loss: 1.7627 - val_accuracy: 0.4934\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5617 - accuracy: 0.5285 - val_loss: 1.7568 - val_accuracy: 0.4889\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5505 - accuracy: 0.5330 - val_loss: 1.7613 - val_accuracy: 0.4823\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5535 - accuracy: 0.5230 - val_loss: 1.7613 - val_accuracy: 0.4867\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5422 - accuracy: 0.5224 - val_loss: 1.7713 - val_accuracy: 0.4956\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 1.5062 - accuracy: 0.5435 - val_loss: 1.7723 - val_accuracy: 0.4912\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5164 - accuracy: 0.5307 - val_loss: 1.7463 - val_accuracy: 0.4779\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5350 - accuracy: 0.5368 - val_loss: 1.7498 - val_accuracy: 0.4757\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5188 - accuracy: 0.5429 - val_loss: 1.7291 - val_accuracy: 0.4978\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5031 - accuracy: 0.5435 - val_loss: 1.7504 - val_accuracy: 0.4845\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4734 - accuracy: 0.5512 - val_loss: 1.7591 - val_accuracy: 0.4845\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5039 - accuracy: 0.5346 - val_loss: 1.7554 - val_accuracy: 0.5022\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4944 - accuracy: 0.5391 - val_loss: 1.7432 - val_accuracy: 0.4889\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4837 - accuracy: 0.5463 - val_loss: 1.7413 - val_accuracy: 0.4912\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 23s 12ms/step - loss: 1.4704 - accuracy: 0.5463 - val_loss: 1.7262 - val_accuracy: 0.5066\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4626 - accuracy: 0.5540 - val_loss: 1.7174 - val_accuracy: 0.5022\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4424 - accuracy: 0.5562 - val_loss: 1.7304 - val_accuracy: 0.4912\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4324 - accuracy: 0.5584 - val_loss: 1.7494 - val_accuracy: 0.4956\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4198 - accuracy: 0.5596 - val_loss: 1.7400 - val_accuracy: 0.4978\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4365 - accuracy: 0.5546 - val_loss: 1.7753 - val_accuracy: 0.4956\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4231 - accuracy: 0.5623 - val_loss: 1.7357 - val_accuracy: 0.5022\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4344 - accuracy: 0.5640 - val_loss: 1.7448 - val_accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4304 - accuracy: 0.5557 - val_loss: 1.7595 - val_accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3989 - accuracy: 0.5673 - val_loss: 1.7322 - val_accuracy: 0.5155\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4024 - accuracy: 0.5706 - val_loss: 1.7308 - val_accuracy: 0.5066\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4084 - accuracy: 0.5612 - val_loss: 1.7030 - val_accuracy: 0.5199\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3841 - accuracy: 0.5662 - val_loss: 1.7398 - val_accuracy: 0.5155\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3923 - accuracy: 0.5684 - val_loss: 1.7134 - val_accuracy: 0.5044\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3583 - accuracy: 0.5806 - val_loss: 1.7512 - val_accuracy: 0.5133\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3638 - accuracy: 0.5801 - val_loss: 1.7342 - val_accuracy: 0.5088\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3530 - accuracy: 0.5806 - val_loss: 1.7684 - val_accuracy: 0.5111\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3466 - accuracy: 0.5806 - val_loss: 1.7438 - val_accuracy: 0.5265\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3484 - accuracy: 0.5856 - val_loss: 1.6934 - val_accuracy: 0.5133\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3364 - accuracy: 0.5900 - val_loss: 1.7270 - val_accuracy: 0.5088\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3182 - accuracy: 0.5850 - val_loss: 1.7147 - val_accuracy: 0.5088\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2889 - accuracy: 0.6039 - val_loss: 1.7231 - val_accuracy: 0.5199\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3186 - accuracy: 0.5961 - val_loss: 1.7147 - val_accuracy: 0.5044\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2988 - accuracy: 0.6017 - val_loss: 1.6995 - val_accuracy: 0.5199\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2999 - accuracy: 0.5972 - val_loss: 1.7744 - val_accuracy: 0.5199\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3050 - accuracy: 0.5950 - val_loss: 1.6930 - val_accuracy: 0.5088\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2710 - accuracy: 0.6161 - val_loss: 1.6700 - val_accuracy: 0.5177\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2783 - accuracy: 0.6033 - val_loss: 1.6636 - val_accuracy: 0.5221\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2812 - accuracy: 0.6050 - val_loss: 1.6856 - val_accuracy: 0.5243\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2710 - accuracy: 0.6078 - val_loss: 1.7023 - val_accuracy: 0.5221\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2528 - accuracy: 0.6133 - val_loss: 1.7113 - val_accuracy: 0.5199\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2471 - accuracy: 0.6144 - val_loss: 1.6967 - val_accuracy: 0.5243\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2382 - accuracy: 0.6133 - val_loss: 1.7088 - val_accuracy: 0.5199\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2441 - accuracy: 0.6161 - val_loss: 1.6931 - val_accuracy: 0.5310\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2120 - accuracy: 0.6277 - val_loss: 1.7097 - val_accuracy: 0.5221\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2226 - accuracy: 0.6294 - val_loss: 1.6780 - val_accuracy: 0.5265\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2007 - accuracy: 0.6260 - val_loss: 1.6995 - val_accuracy: 0.5066\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2005 - accuracy: 0.6321 - val_loss: 1.7049 - val_accuracy: 0.5354\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2225 - accuracy: 0.6233 - val_loss: 1.7173 - val_accuracy: 0.5199\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1877 - accuracy: 0.6310 - val_loss: 1.6970 - val_accuracy: 0.5221\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1781 - accuracy: 0.6294 - val_loss: 1.7111 - val_accuracy: 0.5376\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1658 - accuracy: 0.6366 - val_loss: 1.7149 - val_accuracy: 0.5199\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1549 - accuracy: 0.6421 - val_loss: 1.7061 - val_accuracy: 0.5310\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1375 - accuracy: 0.6515 - val_loss: 1.7200 - val_accuracy: 0.5310\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.1427 - accuracy: 0.6355 - val_loss: 1.6699 - val_accuracy: 0.5465\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 23s 13ms/step - loss: 1.1297 - accuracy: 0.6399 - val_loss: 1.6761 - val_accuracy: 0.5310\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1324 - accuracy: 0.6438 - val_loss: 1.7278 - val_accuracy: 0.5265\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1188 - accuracy: 0.6488 - val_loss: 1.6677 - val_accuracy: 0.5398\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0903 - accuracy: 0.6548 - val_loss: 1.6878 - val_accuracy: 0.5310\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1001 - accuracy: 0.6587 - val_loss: 1.6718 - val_accuracy: 0.5288\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0901 - accuracy: 0.6637 - val_loss: 1.7079 - val_accuracy: 0.5420\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0969 - accuracy: 0.6571 - val_loss: 1.6593 - val_accuracy: 0.5354\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 55.890\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(2048, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.3)(class1)\n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.3)(class1)        \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.3)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.3)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b622fa69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32768,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul] name: dense_8/random_uniform/mul/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-37198fc3e443>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m# entry point, run the test harness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-37198fc3e443>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# define model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# create data generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mdatagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturewise_center\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-37198fc3e443>\u001b[0m in \u001b[0;36mdefine_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# add new classifier layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mflat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'he_uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 463\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    893\u001b[0m                                       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'kernel'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m             self.bias = self.add_weight(shape=(self.units,),\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         weight = K.variable(initializer(shape, dtype=dtype),\n\u001b[0m\u001b[0;32m    280\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             x = K.random_uniform(shape, -limit, limit,\n\u001b[1;32m--> 227\u001b[1;33m                                  dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[0;32m   4355\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         return tf_keras_backend.random_uniform(\n\u001b[1;32m-> 4357\u001b[1;33m             shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[0;32m   5596\u001b[0m     \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5597\u001b[0m   return random_ops.random_uniform(\n\u001b[1;32m-> 5598\u001b[1;33m       shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[0;32m   5599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m       \u001b[0mrnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_random_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m     \u001b[1;31m# TODO(b/132092188): C++ shape inference inside functional ops does not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;31m# cross FuncGraph boundaries since that information is only available in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1204\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6696\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6697\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6698\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6699\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6700\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32768,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul] name: dense_8/random_uniform/mul/"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(2048, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)      \n",
    "\tclass1 = Dense(1072, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=8, epochs=50, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bdbee58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/50\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 7.6154 - accuracy: 0.1762 - val_loss: 2.3848 - val_accuracy: 0.3805\n",
      "Epoch 2/50\n",
      "1805/1805 [==============================] - 25s 14ms/step - loss: 3.1425 - accuracy: 0.3152 - val_loss: 2.1237 - val_accuracy: 0.4270\n",
      "Epoch 3/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 2.4928 - accuracy: 0.3668 - val_loss: 2.0146 - val_accuracy: 0.4358\n",
      "Epoch 4/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 2.2040 - accuracy: 0.4216 - val_loss: 1.9546 - val_accuracy: 0.4336\n",
      "Epoch 5/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 2.0961 - accuracy: 0.4393 - val_loss: 1.9328 - val_accuracy: 0.4425\n",
      "Epoch 6/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.8214 - accuracy: 0.4892 - val_loss: 1.8956 - val_accuracy: 0.4381\n",
      "Epoch 7/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.7428 - accuracy: 0.4986 - val_loss: 1.8347 - val_accuracy: 0.4646\n",
      "Epoch 8/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.6748 - accuracy: 0.5036 - val_loss: 1.8322 - val_accuracy: 0.4580\n",
      "Epoch 9/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.5958 - accuracy: 0.5263 - val_loss: 1.8132 - val_accuracy: 0.4668\n",
      "Epoch 10/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.5505 - accuracy: 0.5424 - val_loss: 1.8061 - val_accuracy: 0.4889\n",
      "Epoch 11/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.5041 - accuracy: 0.5496 - val_loss: 1.8040 - val_accuracy: 0.4757\n",
      "Epoch 12/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.4434 - accuracy: 0.5612 - val_loss: 1.7727 - val_accuracy: 0.5022\n",
      "Epoch 13/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.3749 - accuracy: 0.5834 - val_loss: 1.7541 - val_accuracy: 0.5177\n",
      "Epoch 14/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.3625 - accuracy: 0.5845 - val_loss: 1.7496 - val_accuracy: 0.5155\n",
      "Epoch 15/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.3497 - accuracy: 0.6017 - val_loss: 1.7540 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.2463 - accuracy: 0.6155 - val_loss: 1.7936 - val_accuracy: 0.4867\n",
      "Epoch 17/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.2211 - accuracy: 0.6294 - val_loss: 1.7495 - val_accuracy: 0.5111\n",
      "Epoch 18/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.1997 - accuracy: 0.6266 - val_loss: 1.7863 - val_accuracy: 0.4823\n",
      "Epoch 19/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.1720 - accuracy: 0.6443 - val_loss: 1.7263 - val_accuracy: 0.5022\n",
      "Epoch 20/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.1328 - accuracy: 0.6537 - val_loss: 1.7640 - val_accuracy: 0.5111\n",
      "Epoch 21/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.1001 - accuracy: 0.6526 - val_loss: 1.7093 - val_accuracy: 0.5177\n",
      "Epoch 22/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 1.0537 - accuracy: 0.6886 - val_loss: 1.7203 - val_accuracy: 0.5177\n",
      "Epoch 23/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9977 - accuracy: 0.6875 - val_loss: 1.7272 - val_accuracy: 0.5199\n",
      "Epoch 24/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9794 - accuracy: 0.6859 - val_loss: 1.7049 - val_accuracy: 0.5177\n",
      "Epoch 25/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9775 - accuracy: 0.6981 - val_loss: 1.7038 - val_accuracy: 0.5177\n",
      "Epoch 26/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9554 - accuracy: 0.7141 - val_loss: 1.7130 - val_accuracy: 0.5310\n",
      "Epoch 27/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.8902 - accuracy: 0.7357 - val_loss: 1.7192 - val_accuracy: 0.5221\n",
      "Epoch 28/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.9435 - accuracy: 0.7058 - val_loss: 1.6955 - val_accuracy: 0.5265\n",
      "Epoch 29/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.8810 - accuracy: 0.7186 - val_loss: 1.6908 - val_accuracy: 0.5199\n",
      "Epoch 30/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.8604 - accuracy: 0.7363 - val_loss: 1.7323 - val_accuracy: 0.5243\n",
      "Epoch 31/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.7829 - accuracy: 0.7518 - val_loss: 1.6778 - val_accuracy: 0.5310\n",
      "Epoch 32/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.8042 - accuracy: 0.7507 - val_loss: 1.7400 - val_accuracy: 0.5265\n",
      "Epoch 33/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.7767 - accuracy: 0.7596 - val_loss: 1.6476 - val_accuracy: 0.5376\n",
      "Epoch 34/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.7700 - accuracy: 0.7596 - val_loss: 1.6901 - val_accuracy: 0.5310\n",
      "Epoch 35/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.7470 - accuracy: 0.7562 - val_loss: 1.6761 - val_accuracy: 0.5332\n",
      "Epoch 36/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6934 - accuracy: 0.7812 - val_loss: 1.6816 - val_accuracy: 0.5398\n",
      "Epoch 37/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6699 - accuracy: 0.7922 - val_loss: 1.7178 - val_accuracy: 0.5575\n",
      "Epoch 38/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6787 - accuracy: 0.7867 - val_loss: 1.6747 - val_accuracy: 0.5531\n",
      "Epoch 39/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6576 - accuracy: 0.7856 - val_loss: 1.6883 - val_accuracy: 0.5597\n",
      "Epoch 40/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6669 - accuracy: 0.7934 - val_loss: 1.6854 - val_accuracy: 0.5531\n",
      "Epoch 41/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6834 - accuracy: 0.7839 - val_loss: 1.6645 - val_accuracy: 0.5597\n",
      "Epoch 42/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6210 - accuracy: 0.8116 - val_loss: 1.6919 - val_accuracy: 0.5487\n",
      "Epoch 43/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.6069 - accuracy: 0.8039 - val_loss: 1.7299 - val_accuracy: 0.5509\n",
      "Epoch 44/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5777 - accuracy: 0.8227 - val_loss: 1.7108 - val_accuracy: 0.5465\n",
      "Epoch 45/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5708 - accuracy: 0.8166 - val_loss: 1.7571 - val_accuracy: 0.5398\n",
      "Epoch 46/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5558 - accuracy: 0.8299 - val_loss: 1.6665 - val_accuracy: 0.5619\n",
      "Epoch 47/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5540 - accuracy: 0.8199 - val_loss: 1.6676 - val_accuracy: 0.5575\n",
      "Epoch 48/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5196 - accuracy: 0.8366 - val_loss: 1.6898 - val_accuracy: 0.5420\n",
      "Epoch 49/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.5340 - accuracy: 0.8327 - val_loss: 1.7097 - val_accuracy: 0.5398\n",
      "Epoch 50/50\n",
      "1805/1805 [==============================] - 26s 14ms/step - loss: 0.4858 - accuracy: 0.8493 - val_loss: 1.7318 - val_accuracy: 0.5442\n",
      "399/399 [==============================] - 5s 11ms/step\n",
      "> 56.140\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1072, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=50, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c10887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1072, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb20198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1072, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b90378",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/100\n",
      "1805/1805 [==============================] - 29s 16ms/step - loss: 7.3409 - accuracy: 0.2150 - val_loss: 2.2158 - val_accuracy: 0.4093\n",
      "Epoch 2/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 3.0761 - accuracy: 0.3197 - val_loss: 2.0474 - val_accuracy: 0.4248\n",
      "Epoch 3/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.5521 - accuracy: 0.3717 - val_loss: 1.9911 - val_accuracy: 0.4469\n",
      "Epoch 4/100\n",
      "1805/1805 [==============================] - 21s 12ms/step - loss: 2.1569 - accuracy: 0.4294 - val_loss: 1.9169 - val_accuracy: 0.4491\n",
      "Epoch 5/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.9505 - accuracy: 0.4609 - val_loss: 1.9207 - val_accuracy: 0.4491\n",
      "Epoch 6/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.8212 - accuracy: 0.4781 - val_loss: 1.8538 - val_accuracy: 0.4624\n",
      "Epoch 7/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7217 - accuracy: 0.4970 - val_loss: 1.8478 - val_accuracy: 0.4779\n",
      "Epoch 8/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5723 - accuracy: 0.5346 - val_loss: 1.8354 - val_accuracy: 0.4558\n",
      "Epoch 9/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5664 - accuracy: 0.5429 - val_loss: 1.8087 - val_accuracy: 0.4668\n",
      "Epoch 10/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4803 - accuracy: 0.5557 - val_loss: 1.7955 - val_accuracy: 0.4801\n",
      "Epoch 11/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4326 - accuracy: 0.5634 - val_loss: 1.7757 - val_accuracy: 0.4801\n",
      "Epoch 12/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3824 - accuracy: 0.5812 - val_loss: 1.7334 - val_accuracy: 0.5088\n",
      "Epoch 13/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3222 - accuracy: 0.5989 - val_loss: 1.7605 - val_accuracy: 0.4912\n",
      "Epoch 14/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2705 - accuracy: 0.6105 - val_loss: 1.7555 - val_accuracy: 0.4912\n",
      "Epoch 15/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2503 - accuracy: 0.6155 - val_loss: 1.7472 - val_accuracy: 0.4801\n",
      "Epoch 16/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2136 - accuracy: 0.6421 - val_loss: 1.7369 - val_accuracy: 0.5066\n",
      "Epoch 17/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2036 - accuracy: 0.6316 - val_loss: 1.7642 - val_accuracy: 0.4956\n",
      "Epoch 18/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1487 - accuracy: 0.6438 - val_loss: 1.6970 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1438 - accuracy: 0.6571 - val_loss: 1.7330 - val_accuracy: 0.5044\n",
      "Epoch 20/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0997 - accuracy: 0.6615 - val_loss: 1.7186 - val_accuracy: 0.5088\n",
      "Epoch 21/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0434 - accuracy: 0.6958 - val_loss: 1.7158 - val_accuracy: 0.5155\n",
      "Epoch 22/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0335 - accuracy: 0.6726 - val_loss: 1.7217 - val_accuracy: 0.5111\n",
      "Epoch 23/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9987 - accuracy: 0.6986 - val_loss: 1.7174 - val_accuracy: 0.5022\n",
      "Epoch 24/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9450 - accuracy: 0.7130 - val_loss: 1.7268 - val_accuracy: 0.4978\n",
      "Epoch 25/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9310 - accuracy: 0.7152 - val_loss: 1.6892 - val_accuracy: 0.5066\n",
      "Epoch 26/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8989 - accuracy: 0.7235 - val_loss: 1.7076 - val_accuracy: 0.5155\n",
      "Epoch 27/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8684 - accuracy: 0.7230 - val_loss: 1.7195 - val_accuracy: 0.5133\n",
      "Epoch 28/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8668 - accuracy: 0.7341 - val_loss: 1.7231 - val_accuracy: 0.5243\n",
      "Epoch 29/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.8349 - accuracy: 0.7352 - val_loss: 1.7290 - val_accuracy: 0.5243\n",
      "Epoch 30/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7957 - accuracy: 0.7584 - val_loss: 1.6689 - val_accuracy: 0.5354\n",
      "Epoch 31/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7440 - accuracy: 0.7562 - val_loss: 1.6855 - val_accuracy: 0.5265\n",
      "Epoch 32/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7760 - accuracy: 0.7562 - val_loss: 1.7597 - val_accuracy: 0.5199\n",
      "Epoch 33/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7175 - accuracy: 0.7812 - val_loss: 1.8235 - val_accuracy: 0.5177\n",
      "Epoch 34/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7087 - accuracy: 0.7751 - val_loss: 1.7149 - val_accuracy: 0.5265\n",
      "Epoch 35/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6705 - accuracy: 0.7934 - val_loss: 1.7197 - val_accuracy: 0.5376\n",
      "Epoch 36/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.7035 - accuracy: 0.7856 - val_loss: 1.6484 - val_accuracy: 0.5465\n",
      "Epoch 37/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6142 - accuracy: 0.8094 - val_loss: 1.7469 - val_accuracy: 0.5442\n",
      "Epoch 38/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5788 - accuracy: 0.8266 - val_loss: 1.8257 - val_accuracy: 0.5265\n",
      "Epoch 39/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6114 - accuracy: 0.8089 - val_loss: 1.7939 - val_accuracy: 0.5487\n",
      "Epoch 40/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6239 - accuracy: 0.8150 - val_loss: 1.7572 - val_accuracy: 0.5310\n",
      "Epoch 41/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.6077 - accuracy: 0.8100 - val_loss: 1.7799 - val_accuracy: 0.5354\n",
      "Epoch 42/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5624 - accuracy: 0.8216 - val_loss: 1.7216 - val_accuracy: 0.5354\n",
      "Epoch 43/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5362 - accuracy: 0.8399 - val_loss: 1.7529 - val_accuracy: 0.5442\n",
      "Epoch 44/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5131 - accuracy: 0.8454 - val_loss: 1.8475 - val_accuracy: 0.5354\n",
      "Epoch 45/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4901 - accuracy: 0.8460 - val_loss: 1.7346 - val_accuracy: 0.5509\n",
      "Epoch 46/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4645 - accuracy: 0.8460 - val_loss: 1.7811 - val_accuracy: 0.5265\n",
      "Epoch 47/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.5216 - accuracy: 0.8393 - val_loss: 1.7951 - val_accuracy: 0.5354\n",
      "Epoch 48/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4890 - accuracy: 0.8488 - val_loss: 1.7349 - val_accuracy: 0.5376\n",
      "Epoch 49/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4995 - accuracy: 0.8521 - val_loss: 1.7865 - val_accuracy: 0.5243\n",
      "Epoch 50/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4713 - accuracy: 0.8471 - val_loss: 1.8242 - val_accuracy: 0.5420\n",
      "Epoch 51/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4332 - accuracy: 0.8654 - val_loss: 1.8418 - val_accuracy: 0.5354\n",
      "Epoch 52/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3968 - accuracy: 0.8765 - val_loss: 1.7925 - val_accuracy: 0.5487\n",
      "Epoch 53/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4251 - accuracy: 0.8759 - val_loss: 1.8450 - val_accuracy: 0.5332\n",
      "Epoch 54/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4407 - accuracy: 0.8609 - val_loss: 1.8180 - val_accuracy: 0.5376\n",
      "Epoch 55/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4429 - accuracy: 0.8659 - val_loss: 1.7350 - val_accuracy: 0.5442\n",
      "Epoch 56/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3724 - accuracy: 0.8859 - val_loss: 1.8299 - val_accuracy: 0.5487\n",
      "Epoch 57/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.4111 - accuracy: 0.8676 - val_loss: 1.7645 - val_accuracy: 0.5487\n",
      "Epoch 58/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3240 - accuracy: 0.8970 - val_loss: 1.8340 - val_accuracy: 0.5575\n",
      "Epoch 59/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3518 - accuracy: 0.8942 - val_loss: 1.8312 - val_accuracy: 0.5553\n",
      "Epoch 60/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3935 - accuracy: 0.8792 - val_loss: 1.8422 - val_accuracy: 0.5376\n",
      "Epoch 61/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3285 - accuracy: 0.8992 - val_loss: 1.8274 - val_accuracy: 0.5619\n",
      "Epoch 62/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3459 - accuracy: 0.8920 - val_loss: 1.8283 - val_accuracy: 0.5509\n",
      "Epoch 63/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3818 - accuracy: 0.8803 - val_loss: 1.8608 - val_accuracy: 0.5509\n",
      "Epoch 64/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3066 - accuracy: 0.8997 - val_loss: 1.9113 - val_accuracy: 0.5531\n",
      "Epoch 65/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3073 - accuracy: 0.9036 - val_loss: 1.8774 - val_accuracy: 0.5531\n",
      "Epoch 66/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3292 - accuracy: 0.8981 - val_loss: 1.8657 - val_accuracy: 0.5420\n",
      "Epoch 67/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3447 - accuracy: 0.8892 - val_loss: 1.8541 - val_accuracy: 0.5442\n",
      "Epoch 68/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2959 - accuracy: 0.9108 - val_loss: 1.8921 - val_accuracy: 0.5398\n",
      "Epoch 69/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.3049 - accuracy: 0.9025 - val_loss: 1.9177 - val_accuracy: 0.5310\n",
      "Epoch 70/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2839 - accuracy: 0.9064 - val_loss: 1.8766 - val_accuracy: 0.5531\n",
      "Epoch 71/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2675 - accuracy: 0.9141 - val_loss: 1.8921 - val_accuracy: 0.5442\n",
      "Epoch 72/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2688 - accuracy: 0.9180 - val_loss: 1.8861 - val_accuracy: 0.5531\n",
      "Epoch 73/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2962 - accuracy: 0.9119 - val_loss: 1.8473 - val_accuracy: 0.5619\n",
      "Epoch 74/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2714 - accuracy: 0.9147 - val_loss: 1.8463 - val_accuracy: 0.5509\n",
      "Epoch 75/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2571 - accuracy: 0.9213 - val_loss: 1.8496 - val_accuracy: 0.5465\n",
      "Epoch 76/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2742 - accuracy: 0.9097 - val_loss: 1.8899 - val_accuracy: 0.5442\n",
      "Epoch 77/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2506 - accuracy: 0.9269 - val_loss: 1.8723 - val_accuracy: 0.5487\n",
      "Epoch 78/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2587 - accuracy: 0.9191 - val_loss: 1.9303 - val_accuracy: 0.5376\n",
      "Epoch 79/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2494 - accuracy: 0.9224 - val_loss: 1.8817 - val_accuracy: 0.5619\n",
      "Epoch 80/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2429 - accuracy: 0.9313 - val_loss: 1.9470 - val_accuracy: 0.5442\n",
      "Epoch 81/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2258 - accuracy: 0.9269 - val_loss: 1.8739 - val_accuracy: 0.5597\n",
      "Epoch 82/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2663 - accuracy: 0.9147 - val_loss: 1.8906 - val_accuracy: 0.5487\n",
      "Epoch 83/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2250 - accuracy: 0.9307 - val_loss: 1.9740 - val_accuracy: 0.5531\n",
      "Epoch 84/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2481 - accuracy: 0.9230 - val_loss: 1.9349 - val_accuracy: 0.5553\n",
      "Epoch 85/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2089 - accuracy: 0.9330 - val_loss: 1.9038 - val_accuracy: 0.5420\n",
      "Epoch 86/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2017 - accuracy: 0.9330 - val_loss: 1.8552 - val_accuracy: 0.5619\n",
      "Epoch 87/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2107 - accuracy: 0.9324 - val_loss: 1.9851 - val_accuracy: 0.5619\n",
      "Epoch 88/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1970 - accuracy: 0.9391 - val_loss: 1.9581 - val_accuracy: 0.5575\n",
      "Epoch 89/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1930 - accuracy: 0.9463 - val_loss: 1.8601 - val_accuracy: 0.5553\n",
      "Epoch 90/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2124 - accuracy: 0.9324 - val_loss: 1.9296 - val_accuracy: 0.5752\n",
      "Epoch 91/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1995 - accuracy: 0.9402 - val_loss: 2.0166 - val_accuracy: 0.5575\n",
      "Epoch 92/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1974 - accuracy: 0.9335 - val_loss: 2.0260 - val_accuracy: 0.5575\n",
      "Epoch 93/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1882 - accuracy: 0.9413 - val_loss: 1.9909 - val_accuracy: 0.5642\n",
      "Epoch 94/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.2112 - accuracy: 0.9330 - val_loss: 2.0147 - val_accuracy: 0.5509\n",
      "Epoch 95/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1879 - accuracy: 0.9380 - val_loss: 1.8669 - val_accuracy: 0.5664\n",
      "Epoch 96/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1899 - accuracy: 0.9380 - val_loss: 1.9997 - val_accuracy: 0.5686\n",
      "Epoch 97/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1936 - accuracy: 0.9396 - val_loss: 1.9920 - val_accuracy: 0.5531\n",
      "Epoch 98/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1874 - accuracy: 0.9435 - val_loss: 1.9971 - val_accuracy: 0.5597\n",
      "Epoch 99/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1555 - accuracy: 0.9485 - val_loss: 1.9028 - val_accuracy: 0.5575\n",
      "Epoch 100/100\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.1936 - accuracy: 0.9407 - val_loss: 1.9260 - val_accuracy: 0.5531\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 57.644\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1072, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1072, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45469393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bcf5c9b75ed5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m# entry point, run the test harness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-bcf5c9b75ed5>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# define model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;31m# create data generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mdatagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturewise_center\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bcf5c9b75ed5>\u001b[0m in \u001b[0;36mdefine_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mflat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'he_uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1072\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dropout' is not defined"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1072, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6893743d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8c983f812cff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m# entry point, run the test harness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-8c983f812cff>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# define model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# create data generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mdatagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturewise_center\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8c983f812cff>\u001b[0m in \u001b[0;36mdefine_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mflat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'he_uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mclass1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dropout' is not defined"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(2048, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1) \n",
    "\tclass1 = Dense(1024, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=50, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23959752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.4)(class1)\n",
    "\tclass1 = Dense(2048, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.4)(class1) \n",
    "\tclass1 = Dense(1024, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.4)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.4)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.4)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.6)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=50, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10277a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1805 samples, validate on 452 samples\n",
      "Epoch 1/50\n",
      "1805/1805 [==============================] - 29s 16ms/step - loss: 8.9570 - accuracy: 0.1579 - val_loss: 2.3055 - val_accuracy: 0.3540\n",
      "Epoch 2/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 3.4083 - accuracy: 0.2681 - val_loss: 2.1071 - val_accuracy: 0.4204\n",
      "Epoch 3/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.6460 - accuracy: 0.3053 - val_loss: 2.0661 - val_accuracy: 0.4159\n",
      "Epoch 4/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.3651 - accuracy: 0.3518 - val_loss: 2.0081 - val_accuracy: 0.4292\n",
      "Epoch 5/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.1442 - accuracy: 0.4044 - val_loss: 1.9943 - val_accuracy: 0.4469\n",
      "Epoch 6/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 2.0126 - accuracy: 0.4255 - val_loss: 1.9169 - val_accuracy: 0.4403\n",
      "Epoch 7/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.9158 - accuracy: 0.4443 - val_loss: 1.9304 - val_accuracy: 0.4314\n",
      "Epoch 8/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.8542 - accuracy: 0.4560 - val_loss: 1.9102 - val_accuracy: 0.4469\n",
      "Epoch 9/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7672 - accuracy: 0.4737 - val_loss: 1.8701 - val_accuracy: 0.4712\n",
      "Epoch 10/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.7167 - accuracy: 0.4864 - val_loss: 1.9040 - val_accuracy: 0.4469\n",
      "Epoch 11/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6819 - accuracy: 0.4886 - val_loss: 1.8941 - val_accuracy: 0.4513\n",
      "Epoch 12/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6287 - accuracy: 0.5075 - val_loss: 1.8591 - val_accuracy: 0.4602\n",
      "Epoch 13/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6090 - accuracy: 0.5125 - val_loss: 1.8444 - val_accuracy: 0.4712\n",
      "Epoch 14/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.6200 - accuracy: 0.5186 - val_loss: 1.8449 - val_accuracy: 0.4735\n",
      "Epoch 15/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5529 - accuracy: 0.5230 - val_loss: 1.8420 - val_accuracy: 0.4823\n",
      "Epoch 16/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.5053 - accuracy: 0.5324 - val_loss: 1.8476 - val_accuracy: 0.4735\n",
      "Epoch 17/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4983 - accuracy: 0.5368 - val_loss: 1.8500 - val_accuracy: 0.4624\n",
      "Epoch 18/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4833 - accuracy: 0.5413 - val_loss: 1.8383 - val_accuracy: 0.4912\n",
      "Epoch 19/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4575 - accuracy: 0.5540 - val_loss: 1.8418 - val_accuracy: 0.4801\n",
      "Epoch 20/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4674 - accuracy: 0.5584 - val_loss: 1.8090 - val_accuracy: 0.4823\n",
      "Epoch 21/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.4060 - accuracy: 0.5668 - val_loss: 1.8544 - val_accuracy: 0.4801\n",
      "Epoch 22/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3944 - accuracy: 0.5684 - val_loss: 1.8199 - val_accuracy: 0.4735\n",
      "Epoch 23/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3548 - accuracy: 0.5867 - val_loss: 1.8021 - val_accuracy: 0.4978\n",
      "Epoch 24/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3319 - accuracy: 0.5812 - val_loss: 1.8008 - val_accuracy: 0.4978\n",
      "Epoch 25/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3473 - accuracy: 0.5812 - val_loss: 1.8201 - val_accuracy: 0.5088\n",
      "Epoch 26/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2875 - accuracy: 0.5994 - val_loss: 1.7802 - val_accuracy: 0.5022\n",
      "Epoch 27/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.3037 - accuracy: 0.5922 - val_loss: 1.7687 - val_accuracy: 0.4912\n",
      "Epoch 28/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2729 - accuracy: 0.5889 - val_loss: 1.8225 - val_accuracy: 0.5044\n",
      "Epoch 29/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2884 - accuracy: 0.5989 - val_loss: 1.7740 - val_accuracy: 0.4934\n",
      "Epoch 30/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2267 - accuracy: 0.6166 - val_loss: 1.7767 - val_accuracy: 0.5177\n",
      "Epoch 31/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2273 - accuracy: 0.6089 - val_loss: 1.8328 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1712 - accuracy: 0.6399 - val_loss: 1.7446 - val_accuracy: 0.5022\n",
      "Epoch 33/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2196 - accuracy: 0.6216 - val_loss: 1.7901 - val_accuracy: 0.5044\n",
      "Epoch 34/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1770 - accuracy: 0.6283 - val_loss: 1.7739 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.2015 - accuracy: 0.6222 - val_loss: 1.7867 - val_accuracy: 0.4956\n",
      "Epoch 36/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1559 - accuracy: 0.6310 - val_loss: 1.7640 - val_accuracy: 0.5022\n",
      "Epoch 37/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1670 - accuracy: 0.6299 - val_loss: 1.8355 - val_accuracy: 0.5088\n",
      "Epoch 38/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0923 - accuracy: 0.6582 - val_loss: 1.7612 - val_accuracy: 0.5066\n",
      "Epoch 39/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.1438 - accuracy: 0.6355 - val_loss: 1.8051 - val_accuracy: 0.5044\n",
      "Epoch 40/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0720 - accuracy: 0.6626 - val_loss: 1.7924 - val_accuracy: 0.5044\n",
      "Epoch 41/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0989 - accuracy: 0.6488 - val_loss: 1.7797 - val_accuracy: 0.4956\n",
      "Epoch 42/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0693 - accuracy: 0.6620 - val_loss: 1.8149 - val_accuracy: 0.4934\n",
      "Epoch 43/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0772 - accuracy: 0.6526 - val_loss: 1.7939 - val_accuracy: 0.5111\n",
      "Epoch 44/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0624 - accuracy: 0.6626 - val_loss: 1.8330 - val_accuracy: 0.5044\n",
      "Epoch 45/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0281 - accuracy: 0.6770 - val_loss: 1.8022 - val_accuracy: 0.5155\n",
      "Epoch 46/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0184 - accuracy: 0.6742 - val_loss: 1.8472 - val_accuracy: 0.4912\n",
      "Epoch 47/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0309 - accuracy: 0.6698 - val_loss: 1.7838 - val_accuracy: 0.5111\n",
      "Epoch 48/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0381 - accuracy: 0.6720 - val_loss: 1.7587 - val_accuracy: 0.5177\n",
      "Epoch 49/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 0.9954 - accuracy: 0.6870 - val_loss: 1.7791 - val_accuracy: 0.5088\n",
      "Epoch 50/50\n",
      "1805/1805 [==============================] - 22s 12ms/step - loss: 1.0392 - accuracy: 0.6637 - val_loss: 1.7736 - val_accuracy: 0.5243\n",
      "399/399 [==============================] - 6s 15ms/step\n",
      "> 53.133\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.3)(class1)\n",
    "\tclass1 = Dense(2048, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.3)(class1) \n",
    "\tclass1 = Dense(1024, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.3)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(flat1)\n",
    "\tclass1 = Dropout(0.3)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.3)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=50, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(128, activation='relu')(class1)    \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8fe5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(4096, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\tclass1 = Dropout(0.2)(class1)\n",
    "\tclass1 = Dense(2048, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(1024, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(512, activation='relu')(class1)\n",
    "\tclass1 = Dropout(0.2)(class1)    \n",
    "\tclass1 = Dense(256, activation='relu')(class1)   \n",
    "\tclass1 = Dropout(0.5)(class1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('new vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe322a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796261e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a767f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ad25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532c696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649bce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826788b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284a416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c15892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dccb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84fb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a68c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350818c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fabcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252c2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820de9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4ec5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e6c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f5e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd9b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0d2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd802ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15880ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc62b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbddc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5dd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421ecaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3bf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71d234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d8758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3ede3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69def7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07729a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91ffa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb5d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d82c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb726ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29ec78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb1aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e315ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca05b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231971b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20891dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6abb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23cab95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2656/2656 [==============================] - 41s 16ms/step - loss: 3.7155 - accuracy: 0.3366\n",
      "Epoch 2/10\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.6419 - accuracy: 0.4959\n",
      "Epoch 3/10\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.3569 - accuracy: 0.5550\n",
      "Epoch 4/10\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 1.1396 - accuracy: 0.6088\n",
      "Epoch 5/10\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9827 - accuracy: 0.6608\n",
      "Epoch 6/10\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8530 - accuracy: 0.7052\n",
      "Epoch 7/10\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7198 - accuracy: 0.7489\n",
      "Epoch 8/10\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6124 - accuracy: 0.7929\n",
      "Epoch 9/10\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5135 - accuracy: 0.8249\n",
      "Epoch 10/10\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.4361 - accuracy: 0.8573\n",
      "2656/2656 [==============================] - 26s 10ms/step\n",
      "> 88.893\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=10, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('vgg16 fd1 10 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d532079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 3.7578 - accuracy: 0.3396\n",
      "Epoch 2/15\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 1.6280 - accuracy: 0.4992\n",
      "Epoch 3/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.3502 - accuracy: 0.5651\n",
      "Epoch 4/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.1575 - accuracy: 0.6227\n",
      "Epoch 5/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9791 - accuracy: 0.6724\n",
      "Epoch 6/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8363 - accuracy: 0.7169\n",
      "Epoch 7/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7327 - accuracy: 0.7568\n",
      "Epoch 8/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6121 - accuracy: 0.7986\n",
      "Epoch 9/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5224 - accuracy: 0.8302\n",
      "Epoch 10/15\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.4433 - accuracy: 0.8596\n",
      "Epoch 11/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3735 - accuracy: 0.8867\n",
      "Epoch 12/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3202 - accuracy: 0.9055\n",
      "Epoch 13/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2717 - accuracy: 0.9175\n",
      "Epoch 14/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2285 - accuracy: 0.9334\n",
      "Epoch 15/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1889 - accuracy: 0.9465\n",
      "2656/2656 [==============================] - 26s 10ms/step\n",
      "> 95.595\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=15, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd1 15 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a92aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 3.3801 - accuracy: 0.3573\n",
      "Epoch 2/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.6089 - accuracy: 0.5019\n",
      "Epoch 3/20\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3637 - accuracy: 0.5587\n",
      "Epoch 4/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.1732 - accuracy: 0.6111\n",
      "Epoch 5/20\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.0117 - accuracy: 0.6596\n",
      "Epoch 6/20\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8602 - accuracy: 0.7142\n",
      "Epoch 7/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7262 - accuracy: 0.7500\n",
      "Epoch 8/20\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.6045 - accuracy: 0.7929\n",
      "Epoch 9/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5103 - accuracy: 0.8264\n",
      "Epoch 10/20\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.4225 - accuracy: 0.8581\n",
      "Epoch 11/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3430 - accuracy: 0.8870\n",
      "Epoch 12/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2845 - accuracy: 0.9093\n",
      "Epoch 13/20\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.2271 - accuracy: 0.9330\n",
      "Epoch 14/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1836 - accuracy: 0.9511\n",
      "Epoch 15/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1479 - accuracy: 0.9642\n",
      "Epoch 16/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1119 - accuracy: 0.9733\n",
      "Epoch 17/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0900 - accuracy: 0.9812\n",
      "Epoch 18/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0715 - accuracy: 0.9895\n",
      "Epoch 19/20\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.0577 - accuracy: 0.9913\n",
      "Epoch 20/20\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0487 - accuracy: 0.9925\n",
      "2656/2656 [==============================] - 27s 10ms/step\n",
      "> 99.398\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=20, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8b0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 9.2221 - accuracy: 0.2590\n",
      "Epoch 2/20\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 2.0833 - accuracy: 0.3603\n",
      "Epoch 3/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.7440 - accuracy: 0.4612\n",
      "Epoch 4/20\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.6310 - accuracy: 0.4797\n",
      "Epoch 5/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.5747 - accuracy: 0.4895\n",
      "Epoch 6/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.5489 - accuracy: 0.5008\n",
      "Epoch 7/20\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.5230 - accuracy: 0.5309\n",
      "Epoch 8/20\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.5024 - accuracy: 0.5410\n",
      "Epoch 9/20\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.4719 - accuracy: 0.5463\n",
      "Epoch 10/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.4506 - accuracy: 0.5489\n",
      "Epoch 11/20\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.4253 - accuracy: 0.5565\n",
      "Epoch 12/20\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.4362 - accuracy: 0.5535\n",
      "Epoch 13/20\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.4465 - accuracy: 0.5527\n",
      "Epoch 14/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.4034 - accuracy: 0.5610\n",
      "Epoch 15/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.3900 - accuracy: 0.5621\n",
      "Epoch 16/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.3743 - accuracy: 0.5651\n",
      "Epoch 17/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.3727 - accuracy: 0.5678\n",
      "Epoch 18/20\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.3726 - accuracy: 0.5663\n",
      "Epoch 19/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.3565 - accuracy: 0.5666\n",
      "Epoch 20/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.3617 - accuracy: 0.5685\n",
      "2656/2656 [==============================] - 29s 11ms/step\n",
      "> 56.702\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0009, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=20, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d635924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 6.3151 - accuracy: 0.2824\n",
      "Epoch 2/30\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.9309 - accuracy: 0.4213\n",
      "Epoch 3/30\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.7196 - accuracy: 0.4699\n",
      "Epoch 4/30\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.6308 - accuracy: 0.4785\n",
      "Epoch 5/30\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.5807 - accuracy: 0.4868\n",
      "Epoch 6/30\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.5615 - accuracy: 0.5041\n",
      "Epoch 7/30\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.5337 - accuracy: 0.5192\n",
      "Epoch 8/30\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.5113 - accuracy: 0.5286\n",
      "Epoch 9/30\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.4965 - accuracy: 0.5320\n",
      "Epoch 10/30\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.4681 - accuracy: 0.5346\n",
      "Epoch 11/30\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.4555 - accuracy: 0.5392\n",
      "Epoch 12/30\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.4441 - accuracy: 0.5399\n",
      "Epoch 13/30\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.4322 - accuracy: 0.5478\n",
      "Epoch 14/30\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 1.4232 - accuracy: 0.5459\n",
      "Epoch 15/30\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.4086 - accuracy: 0.5429\n",
      "Epoch 16/30\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 1.4255 - accuracy: 0.5467\n",
      "Epoch 17/30\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.4178 - accuracy: 0.5493\n",
      "Epoch 18/30\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 1.4019 - accuracy: 0.5512\n",
      "Epoch 19/30\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.3958 - accuracy: 0.5505\n",
      "Epoch 20/30\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.3885 - accuracy: 0.5505\n",
      "Epoch 21/30\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.3786 - accuracy: 0.5489\n",
      "Epoch 22/30\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.3786 - accuracy: 0.5497\n",
      "Epoch 23/30\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.3656 - accuracy: 0.5527\n",
      "Epoch 24/30\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.3780 - accuracy: 0.5505\n",
      "Epoch 25/30\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3643 - accuracy: 0.5557\n",
      "Epoch 26/30\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.3497 - accuracy: 0.5587\n",
      "Epoch 27/30\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.3513 - accuracy: 0.5572\n",
      "Epoch 28/30\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3413 - accuracy: 0.5599\n",
      "Epoch 29/30\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3536 - accuracy: 0.5576\n",
      "Epoch 30/30\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.3494 - accuracy: 0.5629\n",
      "2656/2656 [==============================] - 28s 10ms/step\n",
      "> 55.986\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0008, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=30, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3760a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 5.5218 - accuracy: 0.2730\n",
      "Epoch 2/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 2.0058 - accuracy: 0.2959\n",
      "Epoch 3/20\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.8865 - accuracy: 0.3694\n",
      "Epoch 4/20\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.7489 - accuracy: 0.4639\n",
      "Epoch 5/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.6137 - accuracy: 0.5222\n",
      "Epoch 6/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.5194 - accuracy: 0.5422\n",
      "Epoch 7/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.4477 - accuracy: 0.5700\n",
      "Epoch 8/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.3639 - accuracy: 0.5881\n",
      "Epoch 9/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.2852 - accuracy: 0.6009\n",
      "Epoch 10/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.2288 - accuracy: 0.6261\n",
      "Epoch 11/20\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 1.1698 - accuracy: 0.6401\n",
      "Epoch 12/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.1261 - accuracy: 0.6498\n",
      "Epoch 13/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.0881 - accuracy: 0.6562\n",
      "Epoch 14/20\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.0608 - accuracy: 0.6657\n",
      "Epoch 15/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.0029 - accuracy: 0.6826\n",
      "Epoch 16/20\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.9962 - accuracy: 0.6770\n",
      "Epoch 17/20\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.9540 - accuracy: 0.6939\n",
      "Epoch 18/20\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.9244 - accuracy: 0.6995\n",
      "Epoch 19/20\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.9041 - accuracy: 0.7063\n",
      "Epoch 20/20\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.8862 - accuracy: 0.7093\n",
      "2656/2656 [==============================] - 37s 14ms/step\n",
      "> 72.440\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=20, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9262b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 4.9003 - accuracy: 0.3065\n",
      "Epoch 2/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.8126 - accuracy: 0.4428\n",
      "Epoch 3/50\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 1.6622 - accuracy: 0.4861\n",
      "Epoch 4/50\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.5897 - accuracy: 0.5192\n",
      "Epoch 5/50\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.5375 - accuracy: 0.5264\n",
      "Epoch 6/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.5048 - accuracy: 0.5335\n",
      "Epoch 7/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.4466 - accuracy: 0.5497\n",
      "Epoch 8/50\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.4183 - accuracy: 0.5580\n",
      "Epoch 9/50\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.3740 - accuracy: 0.5678\n",
      "Epoch 10/50\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.3387 - accuracy: 0.5753\n",
      "Epoch 11/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.2954 - accuracy: 0.5877\n",
      "Epoch 12/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.2658 - accuracy: 0.5953\n",
      "Epoch 13/50\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.2313 - accuracy: 0.6028\n",
      "Epoch 14/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.2032 - accuracy: 0.6039\n",
      "Epoch 15/50\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.1727 - accuracy: 0.6224\n",
      "Epoch 16/50\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 1.1195 - accuracy: 0.6348\n",
      "Epoch 17/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.0713 - accuracy: 0.6529\n",
      "Epoch 18/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.0365 - accuracy: 0.6589\n",
      "Epoch 19/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9998 - accuracy: 0.6766\n",
      "Epoch 20/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9630 - accuracy: 0.6860\n",
      "Epoch 21/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9413 - accuracy: 0.6962\n",
      "Epoch 22/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9085 - accuracy: 0.6995\n",
      "Epoch 23/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8715 - accuracy: 0.7120\n",
      "Epoch 24/50\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8488 - accuracy: 0.7191\n",
      "Epoch 25/50\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8384 - accuracy: 0.7221\n",
      "Epoch 26/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.8318 - accuracy: 0.7240\n",
      "Epoch 27/50\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8341 - accuracy: 0.7255\n",
      "Epoch 28/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8003 - accuracy: 0.7319\n",
      "Epoch 29/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7839 - accuracy: 0.7440\n",
      "Epoch 30/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7588 - accuracy: 0.7496\n",
      "Epoch 31/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7494 - accuracy: 0.7541\n",
      "Epoch 32/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7178 - accuracy: 0.7605\n",
      "Epoch 33/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6878 - accuracy: 0.7711\n",
      "Epoch 34/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6681 - accuracy: 0.7775\n",
      "Epoch 35/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6434 - accuracy: 0.7865\n",
      "Epoch 36/50\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6448 - accuracy: 0.7899\n",
      "Epoch 37/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6183 - accuracy: 0.7948\n",
      "Epoch 38/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.5938 - accuracy: 0.7989\n",
      "Epoch 39/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5718 - accuracy: 0.8106\n",
      "Epoch 40/50\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.5704 - accuracy: 0.8099\n",
      "Epoch 41/50\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5497 - accuracy: 0.8159\n",
      "Epoch 42/50\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5319 - accuracy: 0.8178\n",
      "Epoch 43/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.5133 - accuracy: 0.8197\n",
      "Epoch 44/50\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.5237 - accuracy: 0.8230\n",
      "Epoch 45/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5023 - accuracy: 0.8261\n",
      "Epoch 46/50\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.4925 - accuracy: 0.8317\n",
      "Epoch 47/50\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.4837 - accuracy: 0.8317\n",
      "Epoch 48/50\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.4654 - accuracy: 0.8355\n",
      "Epoch 49/50\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.4595 - accuracy: 0.8358\n",
      "Epoch 50/50\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.4470 - accuracy: 0.8419\n",
      "2656/2656 [==============================] - 28s 11ms/step\n",
      "> 84.639\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=50, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('vgg16 fd1 20 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e57d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 3.0696 - accuracy: 0.3705\n",
      "Epoch 2/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 1.8724 - accuracy: 0.4469\n",
      "Epoch 3/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.5933 - accuracy: 0.5015\n",
      "Epoch 4/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 1.5131 - accuracy: 0.5282\n",
      "Epoch 5/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 1.4655 - accuracy: 0.5459\n",
      "Epoch 6/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 1.4150 - accuracy: 0.5584\n",
      "Epoch 7/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 1.3787 - accuracy: 0.5651\n",
      "Epoch 8/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.3227 - accuracy: 0.5791\n",
      "Epoch 9/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.2647 - accuracy: 0.5945\n",
      "Epoch 10/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.2166 - accuracy: 0.6013\n",
      "Epoch 11/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.1640 - accuracy: 0.6062\n",
      "Epoch 12/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.1275 - accuracy: 0.6254\n",
      "Epoch 13/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.0883 - accuracy: 0.6352\n",
      "Epoch 14/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.0401 - accuracy: 0.6491\n",
      "Epoch 15/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.0128 - accuracy: 0.6589\n",
      "Epoch 16/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9874 - accuracy: 0.6687\n",
      "Epoch 17/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9355 - accuracy: 0.6811\n",
      "Epoch 18/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8942 - accuracy: 0.6962\n",
      "Epoch 19/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8537 - accuracy: 0.6995\n",
      "Epoch 20/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8074 - accuracy: 0.7176\n",
      "Epoch 21/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7986 - accuracy: 0.7244\n",
      "Epoch 22/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7593 - accuracy: 0.7372\n",
      "Epoch 23/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6941 - accuracy: 0.7579\n",
      "Epoch 24/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6516 - accuracy: 0.7643\n",
      "Epoch 25/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6145 - accuracy: 0.7756\n",
      "Epoch 26/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5887 - accuracy: 0.7861\n",
      "Epoch 27/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5686 - accuracy: 0.7907\n",
      "Epoch 28/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5403 - accuracy: 0.7993\n",
      "Epoch 29/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5065 - accuracy: 0.8133\n",
      "Epoch 30/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.4713 - accuracy: 0.8261\n",
      "Epoch 31/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.4321 - accuracy: 0.8430\n",
      "Epoch 32/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.4175 - accuracy: 0.8509\n",
      "Epoch 33/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3738 - accuracy: 0.8645\n",
      "Epoch 34/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3462 - accuracy: 0.8754\n",
      "Epoch 35/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3351 - accuracy: 0.8765\n",
      "Epoch 36/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3236 - accuracy: 0.8840\n",
      "Epoch 37/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3052 - accuracy: 0.8912\n",
      "Epoch 38/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3029 - accuracy: 0.8934\n",
      "Epoch 39/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2781 - accuracy: 0.9006\n",
      "Epoch 40/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2583 - accuracy: 0.9047\n",
      "Epoch 41/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2287 - accuracy: 0.9138\n",
      "Epoch 42/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2143 - accuracy: 0.9198\n",
      "Epoch 43/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2040 - accuracy: 0.9232\n",
      "Epoch 44/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1969 - accuracy: 0.9281\n",
      "Epoch 45/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1980 - accuracy: 0.9281\n",
      "Epoch 46/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1861 - accuracy: 0.9315\n",
      "Epoch 47/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1773 - accuracy: 0.9341\n",
      "Epoch 48/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1749 - accuracy: 0.9375\n",
      "Epoch 49/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1931 - accuracy: 0.9356\n",
      "Epoch 50/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2260 - accuracy: 0.9209\n",
      "Epoch 51/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1804 - accuracy: 0.9334\n",
      "Epoch 52/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1525 - accuracy: 0.9435\n",
      "Epoch 53/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1451 - accuracy: 0.9428\n",
      "Epoch 54/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1295 - accuracy: 0.9495\n",
      "Epoch 55/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1191 - accuracy: 0.9533\n",
      "Epoch 56/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1142 - accuracy: 0.9571\n",
      "Epoch 57/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1114 - accuracy: 0.9571\n",
      "Epoch 58/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1086 - accuracy: 0.9612\n",
      "Epoch 59/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1010 - accuracy: 0.9612\n",
      "Epoch 60/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0998 - accuracy: 0.9627\n",
      "Epoch 61/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1024 - accuracy: 0.9620\n",
      "Epoch 62/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0949 - accuracy: 0.9639\n",
      "Epoch 63/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0924 - accuracy: 0.9654\n",
      "Epoch 64/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0909 - accuracy: 0.9654\n",
      "Epoch 65/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0871 - accuracy: 0.9661\n",
      "Epoch 66/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0855 - accuracy: 0.9676\n",
      "Epoch 67/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0806 - accuracy: 0.9691\n",
      "Epoch 68/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0828 - accuracy: 0.9688\n",
      "Epoch 69/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1024 - accuracy: 0.9669\n",
      "Epoch 70/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0966 - accuracy: 0.9695\n",
      "Epoch 71/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0761 - accuracy: 0.9718\n",
      "Epoch 72/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0716 - accuracy: 0.9725\n",
      "Epoch 73/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0662 - accuracy: 0.9744\n",
      "Epoch 74/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0638 - accuracy: 0.9755\n",
      "Epoch 75/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0616 - accuracy: 0.9767\n",
      "Epoch 76/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0615 - accuracy: 0.9763\n",
      "Epoch 77/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0592 - accuracy: 0.9770\n",
      "Epoch 78/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0581 - accuracy: 0.9774\n",
      "Epoch 79/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0577 - accuracy: 0.9774\n",
      "Epoch 80/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0577 - accuracy: 0.9770\n",
      "Epoch 81/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0566 - accuracy: 0.9770\n",
      "Epoch 82/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0559 - accuracy: 0.9778\n",
      "Epoch 83/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0562 - accuracy: 0.9767\n",
      "Epoch 84/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0568 - accuracy: 0.9770\n",
      "Epoch 85/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0536 - accuracy: 0.9782\n",
      "Epoch 86/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0522 - accuracy: 0.9785\n",
      "Epoch 87/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0514 - accuracy: 0.9789\n",
      "Epoch 88/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.0490 - accuracy: 0.9812\n",
      "Epoch 89/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0502 - accuracy: 0.9793\n",
      "Epoch 90/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0491 - accuracy: 0.9827\n",
      "Epoch 91/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.0479 - accuracy: 0.9834\n",
      "Epoch 92/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0469 - accuracy: 0.9853\n",
      "Epoch 93/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0463 - accuracy: 0.9838\n",
      "Epoch 94/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0463 - accuracy: 0.9849\n",
      "Epoch 95/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0447 - accuracy: 0.9853\n",
      "Epoch 96/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0445 - accuracy: 0.9864\n",
      "Epoch 97/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0437 - accuracy: 0.9861\n",
      "Epoch 98/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0439 - accuracy: 0.9861\n",
      "Epoch 99/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0433 - accuracy: 0.9868\n",
      "Epoch 100/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0425 - accuracy: 0.9861\n",
      "Epoch 101/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0423 - accuracy: 0.9864\n",
      "Epoch 102/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0422 - accuracy: 0.9861\n",
      "Epoch 103/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0437 - accuracy: 0.9864\n",
      "Epoch 104/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0420 - accuracy: 0.9864\n",
      "Epoch 105/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0412 - accuracy: 0.9864\n",
      "Epoch 106/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0399 - accuracy: 0.9868\n",
      "Epoch 107/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0398 - accuracy: 0.9868\n",
      "Epoch 108/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0404 - accuracy: 0.9861\n",
      "Epoch 109/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0418 - accuracy: 0.9868\n",
      "Epoch 110/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0381 - accuracy: 0.9872\n",
      "Epoch 111/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0394 - accuracy: 0.9883\n",
      "Epoch 112/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0386 - accuracy: 0.9880\n",
      "Epoch 113/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0372 - accuracy: 0.9891\n",
      "Epoch 114/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0366 - accuracy: 0.9887\n",
      "Epoch 115/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0364 - accuracy: 0.9883\n",
      "Epoch 116/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0372 - accuracy: 0.9883\n",
      "Epoch 117/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0381 - accuracy: 0.9895\n",
      "Epoch 118/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0361 - accuracy: 0.9898\n",
      "Epoch 119/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0344 - accuracy: 0.9895\n",
      "Epoch 120/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0334 - accuracy: 0.9898\n",
      "Epoch 121/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0337 - accuracy: 0.9898\n",
      "Epoch 122/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0340 - accuracy: 0.9895\n",
      "Epoch 123/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0339 - accuracy: 0.9898\n",
      "Epoch 124/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0335 - accuracy: 0.9891\n",
      "Epoch 125/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0329 - accuracy: 0.9895\n",
      "Epoch 126/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0328 - accuracy: 0.9898\n",
      "Epoch 127/150\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.0325 - accuracy: 0.9898\n",
      "Epoch 128/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0317 - accuracy: 0.9898\n",
      "Epoch 129/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0319 - accuracy: 0.9902\n",
      "Epoch 130/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0314 - accuracy: 0.9891\n",
      "Epoch 131/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0314 - accuracy: 0.9898\n",
      "Epoch 132/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0316 - accuracy: 0.9906\n",
      "Epoch 133/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0309 - accuracy: 0.9910\n",
      "Epoch 134/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0308 - accuracy: 0.9910\n",
      "Epoch 135/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0302 - accuracy: 0.9910\n",
      "Epoch 136/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0306 - accuracy: 0.9913\n",
      "Epoch 137/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0301 - accuracy: 0.9917\n",
      "Epoch 138/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0298 - accuracy: 0.9913\n",
      "Epoch 139/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0298 - accuracy: 0.9921\n",
      "Epoch 140/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0299 - accuracy: 0.9921\n",
      "Epoch 141/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0296 - accuracy: 0.9921\n",
      "Epoch 142/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0300 - accuracy: 0.9917\n",
      "Epoch 143/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0304 - accuracy: 0.9913\n",
      "Epoch 144/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0296 - accuracy: 0.9917\n",
      "Epoch 145/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.0290 - accuracy: 0.9917\n",
      "Epoch 146/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0283 - accuracy: 0.9917\n",
      "Epoch 147/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0281 - accuracy: 0.9917\n",
      "Epoch 148/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0290 - accuracy: 0.9921\n",
      "Epoch 149/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0281 - accuracy: 0.9921\n",
      "Epoch 150/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0279 - accuracy: 0.9925\n",
      "2656/2656 [==============================] - 26s 10ms/step\n",
      "> 99.247\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd1 150 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c48422d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1716362e-09 4.0048058e-03 3.2865743e-09 9.9257058e-01 3.2242565e-04\n",
      " 3.4610061e-06 1.0153035e-06 5.4095917e-08 1.6381325e-07 3.3786517e-05\n",
      " 9.2687039e-04 2.8542274e-07 5.0231125e-07 6.2590000e-09 1.6551279e-11\n",
      " 1.6553949e-09 1.2841139e-12 2.1359960e-03 7.5284589e-08]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for a new image.\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "\t# load the image\n",
    "\timg = load_img(filename, target_size=(256, 256))\n",
    "\t# convert to array\n",
    "\timg = img_to_array(img)\n",
    "\t# reshape into a single sample with 3 channels\n",
    "\timg = img.reshape(1, 256, 256, 3)\n",
    "\t# center pixel data\n",
    "\timg = img.astype('float32')\n",
    "\timg = img - [123.68, 116.779, 103.939]\n",
    "\treturn img\n",
    "\n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "\t# load the image\n",
    "\timg = load_image('sample_image0.jpg')\n",
    "\t# load model\n",
    "\tmodel = load_model('vgg16 fd1 150 epoch.h5')\n",
    "\t# predict the class\n",
    "\tresult = model.predict(img)\n",
    "\tprint(result[0])\n",
    "\n",
    "# entry point, run the example\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2993d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 4.1032 - accuracy: 0.3230\n",
      "Epoch 2/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.7167 - accuracy: 0.4789\n",
      "Epoch 3/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.5836 - accuracy: 0.5034\n",
      "Epoch 4/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.5206 - accuracy: 0.5117\n",
      "Epoch 5/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.4826 - accuracy: 0.5237\n",
      "Epoch 6/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.4353 - accuracy: 0.5301\n",
      "Epoch 7/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.3949 - accuracy: 0.5497\n",
      "Epoch 8/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.3462 - accuracy: 0.5715\n",
      "Epoch 9/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.3139 - accuracy: 0.5798\n",
      "Epoch 10/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.2685 - accuracy: 0.5953\n",
      "Epoch 11/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 1.2403 - accuracy: 0.5986\n",
      "Epoch 12/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.1859 - accuracy: 0.6054\n",
      "Epoch 13/200\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 1.1548 - accuracy: 0.6160\n",
      "Epoch 14/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 1.1183 - accuracy: 0.6284\n",
      "Epoch 15/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.0867 - accuracy: 0.6419\n",
      "Epoch 16/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 1.0357 - accuracy: 0.6562\n",
      "Epoch 17/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.0202 - accuracy: 0.6615\n",
      "Epoch 18/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.9933 - accuracy: 0.6642\n",
      "Epoch 19/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.9480 - accuracy: 0.6747\n",
      "Epoch 20/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.9259 - accuracy: 0.6822\n",
      "Epoch 21/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.8913 - accuracy: 0.6901\n",
      "Epoch 22/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.8662 - accuracy: 0.6943\n",
      "Epoch 23/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.8524 - accuracy: 0.6988\n",
      "Epoch 24/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.8222 - accuracy: 0.7063\n",
      "Epoch 25/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.8167 - accuracy: 0.7071\n",
      "Epoch 26/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8060 - accuracy: 0.7101\n",
      "Epoch 27/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7928 - accuracy: 0.7127\n",
      "Epoch 28/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7637 - accuracy: 0.7157\n",
      "Epoch 29/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7508 - accuracy: 0.7240\n",
      "Epoch 30/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.7203 - accuracy: 0.7263\n",
      "Epoch 31/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.7210 - accuracy: 0.7323\n",
      "Epoch 32/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7086 - accuracy: 0.7398\n",
      "Epoch 33/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.6783 - accuracy: 0.7455\n",
      "Epoch 34/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6591 - accuracy: 0.7598\n",
      "Epoch 35/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6325 - accuracy: 0.7636\n",
      "Epoch 36/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6049 - accuracy: 0.7741\n",
      "Epoch 37/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5897 - accuracy: 0.7794\n",
      "Epoch 38/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5739 - accuracy: 0.7824\n",
      "Epoch 39/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5478 - accuracy: 0.7937\n",
      "Epoch 40/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5334 - accuracy: 0.7989\n",
      "Epoch 41/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5224 - accuracy: 0.8020\n",
      "Epoch 42/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5124 - accuracy: 0.8050\n",
      "Epoch 43/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5055 - accuracy: 0.8125\n",
      "Epoch 44/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.4858 - accuracy: 0.8163\n",
      "Epoch 45/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.4680 - accuracy: 0.8178\n",
      "Epoch 46/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.4610 - accuracy: 0.8227\n",
      "Epoch 47/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.4563 - accuracy: 0.8291\n",
      "Epoch 48/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.4269 - accuracy: 0.8351\n",
      "Epoch 49/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.4176 - accuracy: 0.8377\n",
      "Epoch 50/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.3978 - accuracy: 0.8475\n",
      "Epoch 51/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.3867 - accuracy: 0.8532\n",
      "Epoch 52/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3790 - accuracy: 0.8539\n",
      "Epoch 53/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.3817 - accuracy: 0.8584\n",
      "Epoch 54/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3676 - accuracy: 0.8633\n",
      "Epoch 55/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.3647 - accuracy: 0.8660\n",
      "Epoch 56/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3634 - accuracy: 0.8618\n",
      "Epoch 57/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3570 - accuracy: 0.8694\n",
      "Epoch 58/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3406 - accuracy: 0.8701\n",
      "Epoch 59/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3315 - accuracy: 0.8784\n",
      "Epoch 60/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3151 - accuracy: 0.8773\n",
      "Epoch 61/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3159 - accuracy: 0.8822\n",
      "Epoch 62/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3153 - accuracy: 0.8859\n",
      "Epoch 63/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.3016 - accuracy: 0.8837\n",
      "Epoch 64/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.2892 - accuracy: 0.8901\n",
      "Epoch 65/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.2920 - accuracy: 0.8897\n",
      "Epoch 66/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3024 - accuracy: 0.8859\n",
      "Epoch 67/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2937 - accuracy: 0.8889\n",
      "Epoch 68/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2822 - accuracy: 0.8938\n",
      "Epoch 69/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2707 - accuracy: 0.9029\n",
      "Epoch 70/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.2646 - accuracy: 0.9074\n",
      "Epoch 71/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.2539 - accuracy: 0.9096\n",
      "Epoch 72/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2478 - accuracy: 0.9134\n",
      "Epoch 73/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.2406 - accuracy: 0.9172\n",
      "Epoch 74/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.2386 - accuracy: 0.9160\n",
      "Epoch 75/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2622 - accuracy: 0.9074\n",
      "Epoch 76/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2568 - accuracy: 0.9138\n",
      "Epoch 77/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.2521 - accuracy: 0.9119\n",
      "Epoch 78/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.2451 - accuracy: 0.9149\n",
      "Epoch 79/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.2426 - accuracy: 0.9142\n",
      "Epoch 80/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2287 - accuracy: 0.9194\n",
      "Epoch 81/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2205 - accuracy: 0.9224\n",
      "Epoch 82/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.2213 - accuracy: 0.9221\n",
      "Epoch 83/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2159 - accuracy: 0.9202\n",
      "Epoch 84/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.2120 - accuracy: 0.9255\n",
      "Epoch 85/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2097 - accuracy: 0.9266\n",
      "Epoch 86/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2205 - accuracy: 0.9273\n",
      "Epoch 87/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2072 - accuracy: 0.9281\n",
      "Epoch 88/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.2047 - accuracy: 0.9349\n",
      "Epoch 89/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.1933 - accuracy: 0.9315\n",
      "Epoch 90/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1880 - accuracy: 0.9334\n",
      "Epoch 91/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1872 - accuracy: 0.9341\n",
      "Epoch 92/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1885 - accuracy: 0.9345\n",
      "Epoch 93/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1774 - accuracy: 0.9386\n",
      "Epoch 94/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1725 - accuracy: 0.9405\n",
      "Epoch 95/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1701 - accuracy: 0.9424\n",
      "Epoch 96/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1727 - accuracy: 0.9416\n",
      "Epoch 97/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1652 - accuracy: 0.9424\n",
      "Epoch 98/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1693 - accuracy: 0.9416\n",
      "Epoch 99/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1690 - accuracy: 0.9428\n",
      "Epoch 100/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1810 - accuracy: 0.9394\n",
      "Epoch 101/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1795 - accuracy: 0.9390\n",
      "Epoch 102/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1620 - accuracy: 0.9462\n",
      "Epoch 103/200\n",
      "1920/2656 [====================>.........] - ETA: 7s - loss: 0.1463 - accuracy: 0.9490"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=200, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd1 200 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46b7e5",
   "metadata": {},
   "source": [
    "# VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29a1e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 3.9782 - accuracy: 0.3309\n",
      "Epoch 2/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.5864 - accuracy: 0.5132\n",
      "Epoch 3/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.2283 - accuracy: 0.6017\n",
      "Epoch 4/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.9475 - accuracy: 0.6886\n",
      "Epoch 5/10\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.7162 - accuracy: 0.7553\n",
      "Epoch 6/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5388 - accuracy: 0.8257\n",
      "Epoch 7/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.4013 - accuracy: 0.8773\n",
      "Epoch 8/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2975 - accuracy: 0.9213\n",
      "Epoch 9/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2145 - accuracy: 0.9514\n",
      "Epoch 10/10\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1544 - accuracy: 0.9721\n",
      "2656/2656 [==============================] - 30s 11ms/step\n",
      "> 98.080\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=10, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd1 10 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=15, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd1 15 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010e73fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 3.9007 - accuracy: 0.3505\n",
      "Epoch 2/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.7424 - accuracy: 0.4586\n",
      "Epoch 3/100\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 1.5833 - accuracy: 0.5132\n",
      "Epoch 4/100\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.4881 - accuracy: 0.5279\n",
      "Epoch 5/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.4288 - accuracy: 0.5392\n",
      "Epoch 6/100\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 1.3594 - accuracy: 0.5595\n",
      "Epoch 7/100\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.2965 - accuracy: 0.5753\n",
      "Epoch 8/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.2385 - accuracy: 0.6073\n",
      "Epoch 9/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.1777 - accuracy: 0.6280\n",
      "Epoch 10/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.1217 - accuracy: 0.6412\n",
      "Epoch 11/100\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 1.0606 - accuracy: 0.6540\n",
      "Epoch 12/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.0087 - accuracy: 0.6717\n",
      "Epoch 13/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.9628 - accuracy: 0.6822\n",
      "Epoch 14/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.9164 - accuracy: 0.6939\n",
      "Epoch 15/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.8851 - accuracy: 0.7037\n",
      "Epoch 16/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.8397 - accuracy: 0.7195\n",
      "Epoch 17/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.8046 - accuracy: 0.7304\n",
      "Epoch 18/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.7800 - accuracy: 0.7327\n",
      "Epoch 19/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.7444 - accuracy: 0.7413\n",
      "Epoch 20/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.7116 - accuracy: 0.7496\n",
      "Epoch 21/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.7007 - accuracy: 0.7549\n",
      "Epoch 22/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.6797 - accuracy: 0.7575\n",
      "Epoch 23/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.6370 - accuracy: 0.7733\n",
      "Epoch 24/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.6245 - accuracy: 0.7767\n",
      "Epoch 25/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.6111 - accuracy: 0.7828\n",
      "Epoch 26/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5858 - accuracy: 0.7910\n",
      "Epoch 27/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.5503 - accuracy: 0.7959\n",
      "Epoch 28/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5237 - accuracy: 0.8061\n",
      "Epoch 29/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.5013 - accuracy: 0.8129\n",
      "Epoch 30/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4801 - accuracy: 0.8223\n",
      "Epoch 31/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4757 - accuracy: 0.8268\n",
      "Epoch 32/100\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.4576 - accuracy: 0.8321\n",
      "Epoch 33/100\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.4588 - accuracy: 0.8340\n",
      "Epoch 34/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4366 - accuracy: 0.8396\n",
      "Epoch 35/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4016 - accuracy: 0.8517\n",
      "Epoch 36/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3965 - accuracy: 0.8520\n",
      "Epoch 37/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.3844 - accuracy: 0.8577\n",
      "Epoch 38/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.3785 - accuracy: 0.8566\n",
      "Epoch 39/100\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.3654 - accuracy: 0.8660\n",
      "Epoch 40/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3503 - accuracy: 0.8686\n",
      "Epoch 41/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3320 - accuracy: 0.8750\n",
      "Epoch 42/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3161 - accuracy: 0.8825\n",
      "Epoch 43/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.3128 - accuracy: 0.8837\n",
      "Epoch 44/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3079 - accuracy: 0.8882\n",
      "Epoch 45/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.2976 - accuracy: 0.8893\n",
      "Epoch 46/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2968 - accuracy: 0.8953\n",
      "Epoch 47/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2817 - accuracy: 0.9014\n",
      "Epoch 48/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2693 - accuracy: 0.9089\n",
      "Epoch 49/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2648 - accuracy: 0.9078\n",
      "Epoch 50/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.2590 - accuracy: 0.9119\n",
      "Epoch 51/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2579 - accuracy: 0.9142\n",
      "Epoch 52/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.2525 - accuracy: 0.9175\n",
      "Epoch 53/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2387 - accuracy: 0.9187\n",
      "Epoch 54/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2333 - accuracy: 0.9209\n",
      "Epoch 55/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2345 - accuracy: 0.9228\n",
      "Epoch 56/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.2353 - accuracy: 0.9187\n",
      "Epoch 57/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.2263 - accuracy: 0.9221\n",
      "Epoch 58/100\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2181 - accuracy: 0.9224\n",
      "Epoch 59/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2146 - accuracy: 0.9258\n",
      "Epoch 60/100\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2106 - accuracy: 0.9288\n",
      "Epoch 61/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2066 - accuracy: 0.9300\n",
      "Epoch 62/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1998 - accuracy: 0.9319\n",
      "Epoch 63/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1996 - accuracy: 0.9300\n",
      "Epoch 64/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1925 - accuracy: 0.9322\n",
      "Epoch 65/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1863 - accuracy: 0.9356\n",
      "Epoch 66/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1815 - accuracy: 0.9371\n",
      "Epoch 67/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1763 - accuracy: 0.9398\n",
      "Epoch 68/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1768 - accuracy: 0.9398\n",
      "Epoch 69/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1724 - accuracy: 0.9409\n",
      "Epoch 70/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1755 - accuracy: 0.9409\n",
      "Epoch 71/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1696 - accuracy: 0.9420\n",
      "Epoch 72/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1647 - accuracy: 0.9431\n",
      "Epoch 73/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1630 - accuracy: 0.9447\n",
      "Epoch 74/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1674 - accuracy: 0.9431\n",
      "Epoch 75/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1538 - accuracy: 0.9473\n",
      "Epoch 76/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1504 - accuracy: 0.9473\n",
      "Epoch 77/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1483 - accuracy: 0.9477\n",
      "Epoch 78/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1477 - accuracy: 0.9480\n",
      "Epoch 79/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1443 - accuracy: 0.9503\n",
      "Epoch 80/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1424 - accuracy: 0.9514\n",
      "Epoch 81/100\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.1422 - accuracy: 0.9518\n",
      "Epoch 82/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.1453 - accuracy: 0.9522\n",
      "Epoch 83/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1394 - accuracy: 0.9529\n",
      "Epoch 84/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1384 - accuracy: 0.9529\n",
      "Epoch 85/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1321 - accuracy: 0.9559\n",
      "Epoch 86/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1330 - accuracy: 0.9552\n",
      "Epoch 87/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1370 - accuracy: 0.9559\n",
      "Epoch 88/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.1315 - accuracy: 0.9578\n",
      "Epoch 89/100\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.1306 - accuracy: 0.9593\n",
      "Epoch 90/100\n",
      "2656/2656 [==============================] - 41s 16ms/step - loss: 0.1241 - accuracy: 0.9593\n",
      "Epoch 91/100\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 0.1251 - accuracy: 0.9605\n",
      "Epoch 92/100\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.1171 - accuracy: 0.9620\n",
      "Epoch 93/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1122 - accuracy: 0.9627\n",
      "Epoch 94/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1171 - accuracy: 0.9627\n",
      "Epoch 95/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1120 - accuracy: 0.9627\n",
      "Epoch 96/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1138 - accuracy: 0.9635\n",
      "Epoch 97/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.1075 - accuracy: 0.9665\n",
      "Epoch 98/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1040 - accuracy: 0.9665\n",
      "Epoch 99/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1009 - accuracy: 0.9669\n",
      "Epoch 100/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0948 - accuracy: 0.9676\n",
      "2656/2656 [==============================] - 30s 11ms/step\n",
      "> 96.837\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=100, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd1 100 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0161892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 3.4433 - accuracy: 0.3419\n",
      "Epoch 2/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 1.6533 - accuracy: 0.4925\n",
      "Epoch 3/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.5052 - accuracy: 0.5425\n",
      "Epoch 4/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.3832 - accuracy: 0.5636\n",
      "Epoch 5/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 1.2868 - accuracy: 0.5840\n",
      "Epoch 6/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.1742 - accuracy: 0.6084\n",
      "Epoch 7/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.0881 - accuracy: 0.6393\n",
      "Epoch 8/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 1.0170 - accuracy: 0.6596\n",
      "Epoch 9/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.9553 - accuracy: 0.6785\n",
      "Epoch 10/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.8640 - accuracy: 0.6954\n",
      "Epoch 11/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.7760 - accuracy: 0.7229\n",
      "Epoch 12/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.7203 - accuracy: 0.7425\n",
      "Epoch 13/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.6497 - accuracy: 0.7703\n",
      "Epoch 14/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5916 - accuracy: 0.7922\n",
      "Epoch 15/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5386 - accuracy: 0.8087\n",
      "Epoch 16/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.4750 - accuracy: 0.8309\n",
      "Epoch 17/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.4338 - accuracy: 0.8400\n",
      "Epoch 18/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.3930 - accuracy: 0.8633\n",
      "Epoch 19/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.3733 - accuracy: 0.8694\n",
      "Epoch 20/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.3417 - accuracy: 0.8765\n",
      "Epoch 21/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2859 - accuracy: 0.8980\n",
      "Epoch 22/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2668 - accuracy: 0.9066\n",
      "Epoch 23/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2542 - accuracy: 0.9134\n",
      "Epoch 24/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2215 - accuracy: 0.9255\n",
      "Epoch 25/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1914 - accuracy: 0.9337\n",
      "Epoch 26/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1764 - accuracy: 0.9428\n",
      "Epoch 27/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1530 - accuracy: 0.9484\n",
      "Epoch 28/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1420 - accuracy: 0.9503\n",
      "Epoch 29/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1412 - accuracy: 0.9511\n",
      "Epoch 30/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1228 - accuracy: 0.9582\n",
      "Epoch 31/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1149 - accuracy: 0.9593\n",
      "Epoch 32/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0995 - accuracy: 0.9646\n",
      "Epoch 33/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0839 - accuracy: 0.9699\n",
      "Epoch 34/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0759 - accuracy: 0.9736\n",
      "Epoch 35/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0667 - accuracy: 0.9759\n",
      "Epoch 36/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0629 - accuracy: 0.9778\n",
      "Epoch 37/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0617 - accuracy: 0.9785\n",
      "Epoch 38/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0576 - accuracy: 0.9804\n",
      "Epoch 39/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0572 - accuracy: 0.9804\n",
      "Epoch 40/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0481 - accuracy: 0.9816\n",
      "Epoch 41/150\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 0.0424 - accuracy: 0.9842\n",
      "Epoch 42/150\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 0.0397 - accuracy: 0.9876\n",
      "Epoch 43/150\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.0380 - accuracy: 0.9880\n",
      "Epoch 44/150\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 0.0344 - accuracy: 0.9887\n",
      "Epoch 45/150\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.0305 - accuracy: 0.9891\n",
      "Epoch 46/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0292 - accuracy: 0.9898\n",
      "Epoch 47/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.0278 - accuracy: 0.9902\n",
      "Epoch 48/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0278 - accuracy: 0.9902\n",
      "Epoch 49/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0275 - accuracy: 0.9902\n",
      "Epoch 50/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0271 - accuracy: 0.9906\n",
      "Epoch 51/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0235 - accuracy: 0.9913\n",
      "Epoch 52/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0227 - accuracy: 0.9921\n",
      "Epoch 53/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0228 - accuracy: 0.9913\n",
      "Epoch 54/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0208 - accuracy: 0.9925\n",
      "Epoch 55/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0192 - accuracy: 0.9932\n",
      "Epoch 56/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0188 - accuracy: 0.9932\n",
      "Epoch 57/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0177 - accuracy: 0.9928\n",
      "Epoch 58/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0165 - accuracy: 0.9944\n",
      "Epoch 59/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0163 - accuracy: 0.9947\n",
      "Epoch 60/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0143 - accuracy: 0.9951\n",
      "Epoch 61/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0140 - accuracy: 0.9955\n",
      "Epoch 62/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0150 - accuracy: 0.9955\n",
      "Epoch 63/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0139 - accuracy: 0.9959\n",
      "Epoch 64/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0187 - accuracy: 0.9951\n",
      "Epoch 65/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0154 - accuracy: 0.9962\n",
      "Epoch 66/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0101 - accuracy: 0.9970\n",
      "Epoch 67/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0082 - accuracy: 0.9981\n",
      "Epoch 68/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0074 - accuracy: 0.9985\n",
      "Epoch 69/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0076 - accuracy: 0.9985\n",
      "Epoch 70/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0067 - accuracy: 0.9985\n",
      "Epoch 71/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0064 - accuracy: 0.9985\n",
      "Epoch 72/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 73/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 74/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0050 - accuracy: 0.9989\n",
      "Epoch 75/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0049 - accuracy: 0.9989\n",
      "Epoch 76/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0048 - accuracy: 0.9989\n",
      "Epoch 77/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0047 - accuracy: 0.9989\n",
      "Epoch 78/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0046 - accuracy: 0.9989\n",
      "Epoch 79/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0045 - accuracy: 0.9989\n",
      "Epoch 80/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0045 - accuracy: 0.9989\n",
      "Epoch 81/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0044 - accuracy: 0.9989\n",
      "Epoch 82/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0044 - accuracy: 0.9989\n",
      "Epoch 83/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0043 - accuracy: 0.9989\n",
      "Epoch 84/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0043 - accuracy: 0.9989\n",
      "Epoch 85/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 86/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 87/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 88/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 89/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 90/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 91/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 92/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 93/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 94/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 95/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 96/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 97/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 98/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 99/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 100/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 101/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0034 - accuracy: 0.9989\n",
      "Epoch 102/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 103/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 104/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0031 - accuracy: 0.9989\n",
      "Epoch 105/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0031 - accuracy: 0.9989\n",
      "Epoch 106/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0031 - accuracy: 0.9989\n",
      "Epoch 107/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 108/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 109/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 110/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 111/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 112/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 113/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 114/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 115/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 116/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 117/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 118/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 119/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 120/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 121/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 122/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 123/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 124/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 125/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 126/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0020 - accuracy: 0.9992\n",
      "Epoch 127/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0019 - accuracy: 0.9992\n",
      "Epoch 128/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 129/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 130/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0018 - accuracy: 0.9992\n",
      "Epoch 131/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 132/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 133/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 134/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 135/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 136/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 137/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 138/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 139/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 140/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 141/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 142/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 143/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 144/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 145/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 146/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 147/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 148/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 149/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 150/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "2656/2656 [==============================] - 30s 11ms/step\n",
      "> 99.962\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd1 150 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea82702",
   "metadata": {},
   "source": [
    "### fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a2a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = np.load('pannuke\\\\Fold_2\\\\images\\\\images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9000e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = np.load('pannuke\\\\Fold_2\\\\images\\\\types.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8e2bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 5 5 5]\n",
      "['Breast' 'Breast' 'Breast' ... 'Colon' 'Colon' 'Colon']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded2 = label_encoder.fit_transform(y_train_2)\n",
    "print(integer_encoded2)\n",
    "\n",
    "print(y_train_2)\n",
    "# binary encode\n",
    "onehot_encoder2 = OneHotEncoder(sparse=False)\n",
    "integer_encoded2 = integer_encoded2.reshape(len(integer_encoded2), 1)\n",
    "onehot_encoded2 = onehot_encoder.fit_transform(integer_encoded2)\n",
    "print(onehot_encoded2)\n",
    "\n",
    "y_train_2 = onehot_encoded2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb602af",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e3946c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 3.6820 - accuracy: 0.3314\n",
      "Epoch 2/10\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.6633 - accuracy: 0.4962\n",
      "Epoch 3/10\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 1.3367 - accuracy: 0.5826\n",
      "Epoch 4/10\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 1.1042 - accuracy: 0.6322\n",
      "Epoch 5/10\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.9413 - accuracy: 0.6857\n",
      "Epoch 6/10\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.7972 - accuracy: 0.7352\n",
      "Epoch 7/10\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.6680 - accuracy: 0.7780\n",
      "Epoch 8/10\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.5597 - accuracy: 0.8161\n",
      "Epoch 9/10\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.4723 - accuracy: 0.8462\n",
      "Epoch 10/10\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.3881 - accuracy: 0.8803\n",
      "2523/2523 [==============================] - 31s 12ms/step\n",
      "> 90.052\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_2, y_train_2, epochs=10, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\t#model.save('vgg16 fd2 10 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67abc195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 3.5589 - accuracy: 0.3500\n",
      "Epoch 2/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.5997 - accuracy: 0.4946\n",
      "Epoch 3/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.3473 - accuracy: 0.5640\n",
      "Epoch 4/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.1311 - accuracy: 0.6274\n",
      "Epoch 5/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.9390 - accuracy: 0.6952\n",
      "Epoch 6/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.7921 - accuracy: 0.7368\n",
      "Epoch 7/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.6547 - accuracy: 0.7852\n",
      "Epoch 8/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.5436 - accuracy: 0.8224\n",
      "Epoch 9/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.4396 - accuracy: 0.8569\n",
      "Epoch 10/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.3596 - accuracy: 0.8890\n",
      "Epoch 11/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2930 - accuracy: 0.9156\n",
      "Epoch 12/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2326 - accuracy: 0.9326\n",
      "Epoch 13/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1838 - accuracy: 0.9509\n",
      "Epoch 14/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1501 - accuracy: 0.9616\n",
      "Epoch 15/15\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1197 - accuracy: 0.9715\n",
      "2523/2523 [==============================] - 25s 10ms/step\n",
      "> 97.939\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_2, y_train_2, epochs=15, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd2 15 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2bd399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 3.5538 - accuracy: 0.3645\n",
      "Epoch 2/15\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.5942 - accuracy: 0.5056\n",
      "Epoch 3/15\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.3479 - accuracy: 0.5614\n",
      "Epoch 4/15\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.1578 - accuracy: 0.6171\n",
      "Epoch 5/15\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.0076 - accuracy: 0.6532\n",
      "Epoch 6/15\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.8900 - accuracy: 0.6928\n",
      "Epoch 7/15\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.7704 - accuracy: 0.7334\n",
      "Epoch 8/15\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.6702 - accuracy: 0.7696\n",
      "Epoch 9/15\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.5710 - accuracy: 0.8087\n",
      "Epoch 10/15\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.4784 - accuracy: 0.8370\n",
      "Epoch 11/15\n",
      "2656/2656 [==============================] - 44s 16ms/step - loss: 0.4119 - accuracy: 0.8607\n",
      "Epoch 12/15\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.3516 - accuracy: 0.8904\n",
      "Epoch 13/15\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.3059 - accuracy: 0.9036\n",
      "Epoch 14/15\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.2580 - accuracy: 0.9183\n",
      "Epoch 15/15\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.2199 - accuracy: 0.9326\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=15, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd1 15 epoch for testing.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8959d1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2523/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 27s 11ms/sample - loss: 2.0178 - accuracy: 0.5057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 50.575\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('vgg16 fd1 15 epoch for testing.h5')\n",
    "\n",
    "# evaluate model\n",
    "_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50d4d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2523/2523 [==============================] - 31s 12ms/step - loss: 4.8436 - accuracy: 0.3004\n",
      "Epoch 2/150\n",
      "2523/2523 [==============================] - 27s 11ms/step - loss: 1.7148 - accuracy: 0.4689\n",
      "Epoch 3/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.5579 - accuracy: 0.5113\n",
      "Epoch 4/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.4761 - accuracy: 0.5529\n",
      "Epoch 5/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.4328 - accuracy: 0.5660\n",
      "Epoch 6/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.3769 - accuracy: 0.5779\n",
      "Epoch 7/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.3232 - accuracy: 0.5870\n",
      "Epoch 8/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.2656 - accuracy: 0.5953\n",
      "Epoch 9/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.2450 - accuracy: 0.5977\n",
      "Epoch 10/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.1864 - accuracy: 0.6064\n",
      "Epoch 11/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.1307 - accuracy: 0.6254\n",
      "Epoch 12/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.0831 - accuracy: 0.6361\n",
      "Epoch 13/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 1.0398 - accuracy: 0.6449\n",
      "Epoch 14/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.9856 - accuracy: 0.6603\n",
      "Epoch 15/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.9465 - accuracy: 0.6667\n",
      "Epoch 16/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.9159 - accuracy: 0.6706\n",
      "Epoch 17/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.8969 - accuracy: 0.6766\n",
      "Epoch 18/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.8491 - accuracy: 0.6857\n",
      "Epoch 19/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.8507 - accuracy: 0.6901\n",
      "Epoch 20/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.8097 - accuracy: 0.7186\n",
      "Epoch 21/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.7646 - accuracy: 0.7245\n",
      "Epoch 22/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.7343 - accuracy: 0.7388\n",
      "Epoch 23/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.6941 - accuracy: 0.7499\n",
      "Epoch 24/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.6595 - accuracy: 0.7665\n",
      "Epoch 25/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.6480 - accuracy: 0.7650\n",
      "Epoch 26/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.6156 - accuracy: 0.7761\n",
      "Epoch 27/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.5704 - accuracy: 0.7887\n",
      "Epoch 28/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.5581 - accuracy: 0.7994\n",
      "Epoch 29/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.5268 - accuracy: 0.8038\n",
      "Epoch 30/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.5047 - accuracy: 0.8169\n",
      "Epoch 31/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.4639 - accuracy: 0.8264\n",
      "Epoch 32/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.4428 - accuracy: 0.8347\n",
      "Epoch 33/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.4221 - accuracy: 0.8470\n",
      "Epoch 34/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.3849 - accuracy: 0.8589\n",
      "Epoch 35/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.3434 - accuracy: 0.8724\n",
      "Epoch 36/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.3208 - accuracy: 0.8755\n",
      "Epoch 37/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.3224 - accuracy: 0.8819\n",
      "Epoch 38/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2891 - accuracy: 0.8926\n",
      "Epoch 39/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2558 - accuracy: 0.9057\n",
      "Epoch 40/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2435 - accuracy: 0.9092\n",
      "Epoch 41/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2362 - accuracy: 0.9112\n",
      "Epoch 42/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2391 - accuracy: 0.9092\n",
      "Epoch 43/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2248 - accuracy: 0.9168\n",
      "Epoch 44/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2172 - accuracy: 0.9180\n",
      "Epoch 45/150\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.1980 - accuracy: 0.9298\n",
      "Epoch 46/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1842 - accuracy: 0.9330\n",
      "Epoch 47/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1797 - accuracy: 0.9346\n",
      "Epoch 48/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1851 - accuracy: 0.9362\n",
      "Epoch 49/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1806 - accuracy: 0.9362\n",
      "Epoch 50/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1615 - accuracy: 0.9390\n",
      "Epoch 51/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1469 - accuracy: 0.9441\n",
      "Epoch 52/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1502 - accuracy: 0.9425\n",
      "Epoch 53/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1439 - accuracy: 0.9441\n",
      "Epoch 54/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1363 - accuracy: 0.9469\n",
      "Epoch 55/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1317 - accuracy: 0.9493\n",
      "Epoch 56/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1283 - accuracy: 0.9509\n",
      "Epoch 57/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1245 - accuracy: 0.9512\n",
      "Epoch 58/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1207 - accuracy: 0.9536\n",
      "Epoch 59/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1155 - accuracy: 0.9540\n",
      "Epoch 60/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1239 - accuracy: 0.9544\n",
      "Epoch 61/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1130 - accuracy: 0.9580\n",
      "Epoch 62/150\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.1063 - accuracy: 0.9600\n",
      "Epoch 63/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1093 - accuracy: 0.9572\n",
      "Epoch 64/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1085 - accuracy: 0.9584\n",
      "Epoch 65/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0984 - accuracy: 0.9608\n",
      "Epoch 66/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0935 - accuracy: 0.9639\n",
      "Epoch 67/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0931 - accuracy: 0.9643\n",
      "Epoch 68/150\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0853 - accuracy: 0.9671\n",
      "Epoch 69/150\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0826 - accuracy: 0.9679\n",
      "Epoch 70/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0802 - accuracy: 0.9675\n",
      "Epoch 71/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0843 - accuracy: 0.9675\n",
      "Epoch 72/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0857 - accuracy: 0.9695\n",
      "Epoch 73/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0810 - accuracy: 0.9679\n",
      "Epoch 74/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0760 - accuracy: 0.9695\n",
      "Epoch 75/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0792 - accuracy: 0.9719\n",
      "Epoch 76/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0750 - accuracy: 0.9711\n",
      "Epoch 77/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0747 - accuracy: 0.9730\n",
      "Epoch 78/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0909 - accuracy: 0.9675\n",
      "Epoch 79/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0714 - accuracy: 0.9727\n",
      "Epoch 80/150\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0666 - accuracy: 0.9742\n",
      "Epoch 81/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0634 - accuracy: 0.9758\n",
      "Epoch 82/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0727 - accuracy: 0.9727\n",
      "Epoch 83/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0726 - accuracy: 0.9746\n",
      "Epoch 84/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0633 - accuracy: 0.9746\n",
      "Epoch 85/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0613 - accuracy: 0.9746\n",
      "Epoch 86/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0571 - accuracy: 0.9774\n",
      "Epoch 87/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0572 - accuracy: 0.9758\n",
      "Epoch 88/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0567 - accuracy: 0.9766\n",
      "Epoch 89/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0525 - accuracy: 0.9798\n",
      "Epoch 90/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0570 - accuracy: 0.9790\n",
      "Epoch 91/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0532 - accuracy: 0.9790\n",
      "Epoch 92/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0492 - accuracy: 0.9798\n",
      "Epoch 93/150\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0470 - accuracy: 0.9810\n",
      "Epoch 94/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0433 - accuracy: 0.9830\n",
      "Epoch 95/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0425 - accuracy: 0.9841\n",
      "Epoch 96/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0433 - accuracy: 0.9826\n",
      "Epoch 97/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0424 - accuracy: 0.9849\n",
      "Epoch 98/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0391 - accuracy: 0.9857\n",
      "Epoch 99/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0404 - accuracy: 0.9861\n",
      "Epoch 100/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0379 - accuracy: 0.9873\n",
      "Epoch 101/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0379 - accuracy: 0.9877\n",
      "Epoch 102/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0401 - accuracy: 0.9865\n",
      "Epoch 103/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0395 - accuracy: 0.9885\n",
      "Epoch 104/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0414 - accuracy: 0.9861\n",
      "Epoch 105/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0349 - accuracy: 0.9885\n",
      "Epoch 106/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0306 - accuracy: 0.9901\n",
      "Epoch 107/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0289 - accuracy: 0.9897\n",
      "Epoch 108/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0277 - accuracy: 0.9897\n",
      "Epoch 109/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0271 - accuracy: 0.9897\n",
      "Epoch 110/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0272 - accuracy: 0.9897\n",
      "Epoch 111/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0276 - accuracy: 0.9905\n",
      "Epoch 112/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0257 - accuracy: 0.9905\n",
      "Epoch 113/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0255 - accuracy: 0.9913\n",
      "Epoch 114/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0232 - accuracy: 0.9929\n",
      "Epoch 115/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0225 - accuracy: 0.9929\n",
      "Epoch 116/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0225 - accuracy: 0.9921\n",
      "Epoch 117/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0218 - accuracy: 0.9929\n",
      "Epoch 118/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0217 - accuracy: 0.9929\n",
      "Epoch 119/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0213 - accuracy: 0.9933\n",
      "Epoch 120/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0211 - accuracy: 0.9933\n",
      "Epoch 121/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0208 - accuracy: 0.9933\n",
      "Epoch 122/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0202 - accuracy: 0.9933\n",
      "Epoch 123/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0204 - accuracy: 0.9933\n",
      "Epoch 124/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0207 - accuracy: 0.9933\n",
      "Epoch 125/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0211 - accuracy: 0.9937\n",
      "Epoch 126/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0184 - accuracy: 0.9937\n",
      "Epoch 127/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0182 - accuracy: 0.9941\n",
      "Epoch 128/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0178 - accuracy: 0.9945\n",
      "Epoch 129/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0166 - accuracy: 0.9945\n",
      "Epoch 130/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0177 - accuracy: 0.9945\n",
      "Epoch 131/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0171 - accuracy: 0.9945\n",
      "Epoch 132/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0166 - accuracy: 0.9945\n",
      "Epoch 133/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0168 - accuracy: 0.9945\n",
      "Epoch 134/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0157 - accuracy: 0.9945\n",
      "Epoch 135/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0153 - accuracy: 0.9945\n",
      "Epoch 136/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0160 - accuracy: 0.9948\n",
      "Epoch 137/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0152 - accuracy: 0.9945\n",
      "Epoch 138/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0149 - accuracy: 0.9945\n",
      "Epoch 139/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0149 - accuracy: 0.9952\n",
      "Epoch 140/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0148 - accuracy: 0.9968\n",
      "Epoch 141/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0139 - accuracy: 0.9968\n",
      "Epoch 142/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0133 - accuracy: 0.9968\n",
      "Epoch 143/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0130 - accuracy: 0.9972\n",
      "Epoch 144/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0118 - accuracy: 0.9976\n",
      "Epoch 145/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0115 - accuracy: 0.9976\n",
      "Epoch 146/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0113 - accuracy: 0.9976\n",
      "Epoch 147/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0107 - accuracy: 0.9976\n",
      "Epoch 148/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0105 - accuracy: 0.9976\n",
      "Epoch 149/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 150/150\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0099 - accuracy: 0.9976\n",
      "2523/2523 [==============================] - 25s 10ms/step\n",
      "> 99.762\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_2, y_train_2, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd2 150 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e79fd",
   "metadata": {},
   "source": [
    "# VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "645c9a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2523/2523 [==============================] - 37s 15ms/step - loss: 3.5838 - accuracy: 0.3286\n",
      "Epoch 2/10\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 1.5732 - accuracy: 0.5069\n",
      "Epoch 3/10\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 1.2789 - accuracy: 0.5858\n",
      "Epoch 4/10\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 1.0579 - accuracy: 0.6417\n",
      "Epoch 5/10\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.8868 - accuracy: 0.6924\n",
      "Epoch 6/10\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.7386 - accuracy: 0.7515\n",
      "Epoch 7/10\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.6126 - accuracy: 0.8002\n",
      "Epoch 8/10\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.4937 - accuracy: 0.8383\n",
      "Epoch 9/10\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.3854 - accuracy: 0.8870\n",
      "Epoch 10/10\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.2954 - accuracy: 0.9191\n",
      "2523/2523 [==============================] - 29s 11ms/step\n",
      "> 95.125\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_2, y_train_2, epochs=10, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd2 10 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef54492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2523/2523 [==============================] - 35s 14ms/step - loss: 3.9151 - accuracy: 0.3290\n",
      "Epoch 2/150\n",
      "2523/2523 [==============================] - 32s 13ms/step - loss: 1.7352 - accuracy: 0.4816\n",
      "Epoch 3/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 1.5840 - accuracy: 0.5137\n",
      "Epoch 4/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 1.5172 - accuracy: 0.5311\n",
      "Epoch 5/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 1.4359 - accuracy: 0.5315\n",
      "Epoch 6/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 1.3823 - accuracy: 0.5458\n",
      "Epoch 7/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 1.3019 - accuracy: 0.5672\n",
      "Epoch 8/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 1.2161 - accuracy: 0.5929\n",
      "Epoch 9/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 1.1601 - accuracy: 0.6080\n",
      "Epoch 10/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 1.1108 - accuracy: 0.6219\n",
      "Epoch 11/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 1.0517 - accuracy: 0.6413\n",
      "Epoch 12/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.9943 - accuracy: 0.6591\n",
      "Epoch 13/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.9219 - accuracy: 0.6790\n",
      "Epoch 14/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.8776 - accuracy: 0.6873\n",
      "Epoch 15/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.7817 - accuracy: 0.7214\n",
      "Epoch 16/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.7471 - accuracy: 0.7372\n",
      "Epoch 17/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.6403 - accuracy: 0.7685\n",
      "Epoch 18/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.5860 - accuracy: 0.7868\n",
      "Epoch 19/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.5456 - accuracy: 0.7963\n",
      "Epoch 20/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.5010 - accuracy: 0.8185\n",
      "Epoch 21/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.4636 - accuracy: 0.8315\n",
      "Epoch 22/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.4267 - accuracy: 0.8419\n",
      "Epoch 23/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.4063 - accuracy: 0.8462\n",
      "Epoch 24/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.3919 - accuracy: 0.8561\n",
      "Epoch 25/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.3526 - accuracy: 0.8696\n",
      "Epoch 26/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.3250 - accuracy: 0.8791\n",
      "Epoch 27/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.3087 - accuracy: 0.8874\n",
      "Epoch 28/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.2914 - accuracy: 0.8942\n",
      "Epoch 29/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.2697 - accuracy: 0.9041\n",
      "Epoch 30/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.2431 - accuracy: 0.9152\n",
      "Epoch 31/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.2431 - accuracy: 0.9164\n",
      "Epoch 32/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.2026 - accuracy: 0.9291\n",
      "Epoch 33/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1777 - accuracy: 0.9374\n",
      "Epoch 34/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1614 - accuracy: 0.9445\n",
      "Epoch 35/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1542 - accuracy: 0.9473\n",
      "Epoch 36/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1414 - accuracy: 0.9524\n",
      "Epoch 37/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1334 - accuracy: 0.9584\n",
      "Epoch 38/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1191 - accuracy: 0.9600\n",
      "Epoch 39/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1089 - accuracy: 0.9631\n",
      "Epoch 40/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.1003 - accuracy: 0.9655\n",
      "Epoch 41/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0915 - accuracy: 0.9663\n",
      "Epoch 42/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0877 - accuracy: 0.9691\n",
      "Epoch 43/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0860 - accuracy: 0.9675\n",
      "Epoch 44/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0798 - accuracy: 0.9707\n",
      "Epoch 45/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0758 - accuracy: 0.9730\n",
      "Epoch 46/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0723 - accuracy: 0.9742\n",
      "Epoch 47/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0692 - accuracy: 0.9762\n",
      "Epoch 48/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0649 - accuracy: 0.9774\n",
      "Epoch 49/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0664 - accuracy: 0.9786\n",
      "Epoch 50/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0584 - accuracy: 0.9802\n",
      "Epoch 51/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0526 - accuracy: 0.9818\n",
      "Epoch 52/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0482 - accuracy: 0.9837\n",
      "Epoch 53/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0450 - accuracy: 0.9834\n",
      "Epoch 54/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0414 - accuracy: 0.9849\n",
      "Epoch 55/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0384 - accuracy: 0.9877\n",
      "Epoch 56/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0329 - accuracy: 0.9889\n",
      "Epoch 57/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0320 - accuracy: 0.9889\n",
      "Epoch 58/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0302 - accuracy: 0.9893\n",
      "Epoch 59/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0276 - accuracy: 0.9893\n",
      "Epoch 60/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0268 - accuracy: 0.9905\n",
      "Epoch 61/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0249 - accuracy: 0.9913\n",
      "Epoch 62/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0239 - accuracy: 0.9921\n",
      "Epoch 63/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0223 - accuracy: 0.9925\n",
      "Epoch 64/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0229 - accuracy: 0.9925\n",
      "Epoch 65/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0223 - accuracy: 0.9925\n",
      "Epoch 66/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0197 - accuracy: 0.9933\n",
      "Epoch 67/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0199 - accuracy: 0.9933\n",
      "Epoch 68/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0240 - accuracy: 0.9925\n",
      "Epoch 69/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0222 - accuracy: 0.9925\n",
      "Epoch 70/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0192 - accuracy: 0.9941\n",
      "Epoch 71/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0183 - accuracy: 0.9933\n",
      "Epoch 72/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0181 - accuracy: 0.9945\n",
      "Epoch 73/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0170 - accuracy: 0.9945\n",
      "Epoch 74/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0170 - accuracy: 0.9945\n",
      "Epoch 75/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0159 - accuracy: 0.9941\n",
      "Epoch 76/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0156 - accuracy: 0.9945\n",
      "Epoch 77/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0160 - accuracy: 0.9945\n",
      "Epoch 78/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0152 - accuracy: 0.9945\n",
      "Epoch 79/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0150 - accuracy: 0.9945\n",
      "Epoch 80/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0150 - accuracy: 0.9945\n",
      "Epoch 81/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0157 - accuracy: 0.9945\n",
      "Epoch 82/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0145 - accuracy: 0.9952\n",
      "Epoch 83/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0126 - accuracy: 0.9948\n",
      "Epoch 84/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0122 - accuracy: 0.9952\n",
      "Epoch 85/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0122 - accuracy: 0.9952\n",
      "Epoch 86/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0125 - accuracy: 0.9952\n",
      "Epoch 87/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0118 - accuracy: 0.9952\n",
      "Epoch 88/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0118 - accuracy: 0.9952\n",
      "Epoch 89/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0119 - accuracy: 0.9952\n",
      "Epoch 90/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0118 - accuracy: 0.9952\n",
      "Epoch 91/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0122 - accuracy: 0.9952\n",
      "Epoch 92/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0120 - accuracy: 0.9952\n",
      "Epoch 93/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0136 - accuracy: 0.9952\n",
      "Epoch 94/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0120 - accuracy: 0.9952\n",
      "Epoch 95/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0111 - accuracy: 0.9952\n",
      "Epoch 96/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0109 - accuracy: 0.9952\n",
      "Epoch 97/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0108 - accuracy: 0.9952\n",
      "Epoch 98/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0111 - accuracy: 0.9952\n",
      "Epoch 99/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0111 - accuracy: 0.9952\n",
      "Epoch 100/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0118 - accuracy: 0.9952\n",
      "Epoch 101/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0111 - accuracy: 0.9952\n",
      "Epoch 102/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0126 - accuracy: 0.9952\n",
      "Epoch 103/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0116 - accuracy: 0.9952\n",
      "Epoch 104/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0110 - accuracy: 0.9952\n",
      "Epoch 105/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0105 - accuracy: 0.9952\n",
      "Epoch 106/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0104 - accuracy: 0.9952\n",
      "Epoch 107/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0103 - accuracy: 0.9952\n",
      "Epoch 108/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0103 - accuracy: 0.9952\n",
      "Epoch 109/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0103 - accuracy: 0.9952\n",
      "Epoch 110/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0109 - accuracy: 0.9952\n",
      "Epoch 111/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0096 - accuracy: 0.9956\n",
      "Epoch 112/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0095 - accuracy: 0.9956\n",
      "Epoch 113/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0094 - accuracy: 0.9956\n",
      "Epoch 114/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0096 - accuracy: 0.9956\n",
      "Epoch 115/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0093 - accuracy: 0.9956\n",
      "Epoch 116/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0097 - accuracy: 0.9956\n",
      "Epoch 117/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0093 - accuracy: 0.9956\n",
      "Epoch 118/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0093 - accuracy: 0.9960\n",
      "Epoch 119/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0093 - accuracy: 0.9956\n",
      "Epoch 120/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0091 - accuracy: 0.9956\n",
      "Epoch 121/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0091 - accuracy: 0.9956\n",
      "Epoch 122/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0098 - accuracy: 0.9956\n",
      "Epoch 123/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0092 - accuracy: 0.9956\n",
      "Epoch 124/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0091 - accuracy: 0.9956\n",
      "Epoch 125/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0094 - accuracy: 0.9956\n",
      "Epoch 126/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0091 - accuracy: 0.9956\n",
      "Epoch 127/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0093 - accuracy: 0.9956\n",
      "Epoch 128/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0091 - accuracy: 0.9956\n",
      "Epoch 129/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0088 - accuracy: 0.9956\n",
      "Epoch 130/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0089 - accuracy: 0.9956\n",
      "Epoch 131/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0097 - accuracy: 0.9956\n",
      "Epoch 132/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0089 - accuracy: 0.9956\n",
      "Epoch 133/150\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0087 - accuracy: 0.9956\n",
      "Epoch 134/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0089 - accuracy: 0.9956\n",
      "Epoch 135/150\n",
      "2523/2523 [==============================] - 31s 12ms/step - loss: 0.0086 - accuracy: 0.9956\n",
      "Epoch 136/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0090 - accuracy: 0.9956\n",
      "Epoch 137/150\n",
      "2523/2523 [==============================] - 31s 12ms/step - loss: 0.0085 - accuracy: 0.9956\n",
      "Epoch 138/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0087 - accuracy: 0.9956\n",
      "Epoch 139/150\n",
      "2523/2523 [==============================] - 31s 12ms/step - loss: 0.0085 - accuracy: 0.9956\n",
      "Epoch 140/150\n",
      "2523/2523 [==============================] - 32s 13ms/step - loss: 0.0085 - accuracy: 0.9956\n",
      "Epoch 141/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0087 - accuracy: 0.9956\n",
      "Epoch 142/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0084 - accuracy: 0.9956\n",
      "Epoch 143/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0086 - accuracy: 0.9956\n",
      "Epoch 144/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0084 - accuracy: 0.9956\n",
      "Epoch 145/150\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.0087 - accuracy: 0.9956\n",
      "Epoch 146/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0082 - accuracy: 0.9956\n",
      "Epoch 147/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0086 - accuracy: 0.9956\n",
      "Epoch 148/150\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0084 - accuracy: 0.9956\n",
      "Epoch 149/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0083 - accuracy: 0.9960\n",
      "Epoch 150/150\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0079 - accuracy: 0.9960\n",
      "2523/2523 [==============================] - 30s 12ms/step\n",
      "> 99.604\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_2, y_train_2, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd2 150 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110602e",
   "metadata": {},
   "source": [
    "# fold - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7b530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_3 = np.load('pannuke\\\\Fold_3\\\\images\\\\images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a14dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3 = np.load('pannuke\\\\Fold_3\\\\images\\\\types.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7d2dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 5 5 5]\n",
      "['Breast' 'Breast' 'Breast' ... 'Colon' 'Colon' 'Colon']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded3 = label_encoder.fit_transform(y_train_3)\n",
    "print(integer_encoded3)\n",
    "\n",
    "print(y_train_3)\n",
    "# binary encode\n",
    "onehot_encoder3 = OneHotEncoder(sparse=False)\n",
    "integer_encoded3 = integer_encoded3.reshape(len(integer_encoded3), 1)\n",
    "onehot_encoded3 = onehot_encoder3.fit_transform(integer_encoded3)\n",
    "print(onehot_encoded3)\n",
    "\n",
    "y_train_3 = onehot_encoded3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd2952",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121b4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2722/2722 [==============================] - 34s 13ms/step - loss: 3.8692 - accuracy: 0.3306\n",
      "Epoch 2/10\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.6747 - accuracy: 0.4971\n",
      "Epoch 3/10\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.4063 - accuracy: 0.5580\n",
      "Epoch 4/10\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.2542 - accuracy: 0.5985\n",
      "Epoch 5/10\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0738 - accuracy: 0.6473\n",
      "Epoch 6/10\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9141 - accuracy: 0.7046\n",
      "Epoch 7/10\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7876 - accuracy: 0.7359\n",
      "Epoch 8/10\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7381 - accuracy: 0.7616\n",
      "Epoch 9/10\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.6040 - accuracy: 0.7994\n",
      "Epoch 10/10\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4896 - accuracy: 0.8420\n",
      "2722/2722 [==============================] - 27s 10ms/step\n",
      "> 86.517\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_3, y_train_3, epochs=10, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_3, y_train_3, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd3 10 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a68fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 3.6818 - accuracy: 0.3453\n",
      "Epoch 2/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.6523 - accuracy: 0.5000\n",
      "Epoch 3/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.4337 - accuracy: 0.5511\n",
      "Epoch 4/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.2689 - accuracy: 0.5882\n",
      "Epoch 5/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.1053 - accuracy: 0.6330\n",
      "Epoch 6/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9595 - accuracy: 0.6727\n",
      "Epoch 7/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.8008 - accuracy: 0.7215\n",
      "Epoch 8/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6866 - accuracy: 0.7660\n",
      "Epoch 9/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5687 - accuracy: 0.8075\n",
      "Epoch 10/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4696 - accuracy: 0.8472\n",
      "Epoch 11/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3944 - accuracy: 0.8773\n",
      "Epoch 12/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3209 - accuracy: 0.9034\n",
      "Epoch 13/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2631 - accuracy: 0.9195\n",
      "Epoch 14/15\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2066 - accuracy: 0.9420\n",
      "Epoch 15/15\n",
      "2722/2722 [==============================] - 34s 12ms/step - loss: 0.1624 - accuracy: 0.9574\n",
      "2722/2722 [==============================] - 40s 15ms/step\n",
      "> 96.583\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_3, y_train_3, epochs=15, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_3, y_train_3, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd3 15 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626d10f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 4.6744 - accuracy: 0.2641\n",
      "Epoch 2/150\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 1.9464 - accuracy: 0.3946\n",
      "Epoch 3/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 1.7356 - accuracy: 0.4622\n",
      "Epoch 4/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 1.6236 - accuracy: 0.5176\n",
      "Epoch 5/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.5429 - accuracy: 0.5312\n",
      "Epoch 6/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.5820 - accuracy: 0.5305\n",
      "Epoch 7/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.5117 - accuracy: 0.5342\n",
      "Epoch 8/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.4931 - accuracy: 0.5371\n",
      "Epoch 9/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.4684 - accuracy: 0.5448\n",
      "Epoch 10/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 1.4459 - accuracy: 0.5525\n",
      "Epoch 11/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.4279 - accuracy: 0.5580\n",
      "Epoch 12/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.4196 - accuracy: 0.5614\n",
      "Epoch 13/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.4021 - accuracy: 0.5658\n",
      "Epoch 14/150\n",
      "2722/2722 [==============================] - 33s 12ms/step - loss: 1.3821 - accuracy: 0.5716\n",
      "Epoch 15/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.3618 - accuracy: 0.5794\n",
      "Epoch 16/150\n",
      "2722/2722 [==============================] - 29s 10ms/step - loss: 1.3363 - accuracy: 0.5882\n",
      "Epoch 17/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 1.3108 - accuracy: 0.5952\n",
      "Epoch 18/150\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 1.2817 - accuracy: 0.6010\n",
      "Epoch 19/150\n",
      "2722/2722 [==============================] - 33s 12ms/step - loss: 1.2630 - accuracy: 0.6058\n",
      "Epoch 20/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 1.2364 - accuracy: 0.6128\n",
      "Epoch 21/150\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 1.2168 - accuracy: 0.6132\n",
      "Epoch 22/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 1.2156 - accuracy: 0.6128\n",
      "Epoch 23/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.1837 - accuracy: 0.6209\n",
      "Epoch 24/150\n",
      "2722/2722 [==============================] - 35s 13ms/step - loss: 1.1622 - accuracy: 0.6260\n",
      "Epoch 25/150\n",
      "2722/2722 [==============================] - 33s 12ms/step - loss: 1.1578 - accuracy: 0.6356\n",
      "Epoch 26/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.1367 - accuracy: 0.6367\n",
      "Epoch 27/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.1307 - accuracy: 0.6345\n",
      "Epoch 28/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 1.1134 - accuracy: 0.6389\n",
      "Epoch 29/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 1.1058 - accuracy: 0.6440\n",
      "Epoch 30/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 1.0983 - accuracy: 0.6414\n",
      "Epoch 31/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 1.0920 - accuracy: 0.6458\n",
      "Epoch 32/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0888 - accuracy: 0.6484\n",
      "Epoch 33/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 1.0829 - accuracy: 0.6532\n",
      "Epoch 34/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0681 - accuracy: 0.6583\n",
      "Epoch 35/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0518 - accuracy: 0.6583\n",
      "Epoch 36/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0386 - accuracy: 0.6661\n",
      "Epoch 37/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0412 - accuracy: 0.6664\n",
      "Epoch 38/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0288 - accuracy: 0.6679\n",
      "Epoch 39/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0330 - accuracy: 0.6639\n",
      "Epoch 40/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0151 - accuracy: 0.6760\n",
      "Epoch 41/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0070 - accuracy: 0.6767\n",
      "Epoch 42/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0086 - accuracy: 0.6767\n",
      "Epoch 43/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9897 - accuracy: 0.6804\n",
      "Epoch 44/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.9730 - accuracy: 0.6874\n",
      "Epoch 45/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.9640 - accuracy: 0.6877\n",
      "Epoch 46/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.9497 - accuracy: 0.6943\n",
      "Epoch 47/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.9470 - accuracy: 0.6870\n",
      "Epoch 48/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9562 - accuracy: 0.6907\n",
      "Epoch 49/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9365 - accuracy: 0.6969\n",
      "Epoch 50/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9272 - accuracy: 0.7035\n",
      "Epoch 51/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9200 - accuracy: 0.7039\n",
      "Epoch 52/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.9011 - accuracy: 0.7083\n",
      "Epoch 53/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9015 - accuracy: 0.7068\n",
      "Epoch 54/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9003 - accuracy: 0.7094\n",
      "Epoch 55/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.8852 - accuracy: 0.7109\n",
      "Epoch 56/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.8784 - accuracy: 0.7164\n",
      "Epoch 57/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.8597 - accuracy: 0.7193\n",
      "Epoch 58/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 0.8498 - accuracy: 0.7237\n",
      "Epoch 59/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 0.8722 - accuracy: 0.7190\n",
      "Epoch 60/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 0.8831 - accuracy: 0.7168\n",
      "Epoch 61/150\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.8404 - accuracy: 0.7292\n",
      "Epoch 62/150\n",
      "2722/2722 [==============================] - 37s 14ms/step - loss: 0.8607 - accuracy: 0.7241\n",
      "Epoch 63/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 0.8312 - accuracy: 0.7329\n",
      "Epoch 64/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.8079 - accuracy: 0.7329\n",
      "Epoch 65/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.8463 - accuracy: 0.7292\n",
      "Epoch 66/150\n",
      "2722/2722 [==============================] - 34s 13ms/step - loss: 0.8060 - accuracy: 0.7410\n",
      "Epoch 67/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.8030 - accuracy: 0.7395\n",
      "Epoch 68/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7741 - accuracy: 0.7476\n",
      "Epoch 69/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7570 - accuracy: 0.7542\n",
      "Epoch 70/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7397 - accuracy: 0.7619\n",
      "Epoch 71/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7325 - accuracy: 0.7645\n",
      "Epoch 72/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7214 - accuracy: 0.7704\n",
      "Epoch 73/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6969 - accuracy: 0.7715\n",
      "Epoch 74/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.6828 - accuracy: 0.7770\n",
      "Epoch 75/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 0.6685 - accuracy: 0.7785\n",
      "Epoch 76/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.6670 - accuracy: 0.7803\n",
      "Epoch 77/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.7277 - accuracy: 0.7726\n",
      "Epoch 78/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6629 - accuracy: 0.7818\n",
      "Epoch 79/150\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.6553 - accuracy: 0.7847\n",
      "Epoch 80/150\n",
      "2722/2722 [==============================] - 31s 12ms/step - loss: 0.6274 - accuracy: 0.7954\n",
      "Epoch 81/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.6060 - accuracy: 0.8020\n",
      "Epoch 82/150\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 0.6049 - accuracy: 0.8027\n",
      "Epoch 83/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.5967 - accuracy: 0.8031\n",
      "Epoch 84/150\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 0.6007 - accuracy: 0.8038\n",
      "Epoch 85/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.5873 - accuracy: 0.8071\n",
      "Epoch 86/150\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 0.5752 - accuracy: 0.8097\n",
      "Epoch 87/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5693 - accuracy: 0.8086\n",
      "Epoch 88/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7000 - accuracy: 0.7917\n",
      "Epoch 89/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6094 - accuracy: 0.8031\n",
      "Epoch 90/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5873 - accuracy: 0.8159\n",
      "Epoch 91/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.6396 - accuracy: 0.8024\n",
      "Epoch 92/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5884 - accuracy: 0.8123\n",
      "Epoch 93/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.5827 - accuracy: 0.8145\n",
      "Epoch 94/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.5641 - accuracy: 0.8189\n",
      "Epoch 95/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.5584 - accuracy: 0.8196\n",
      "Epoch 96/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.5424 - accuracy: 0.8178\n",
      "Epoch 97/150\n",
      "2722/2722 [==============================] - 31s 12ms/step - loss: 0.5494 - accuracy: 0.8218\n",
      "Epoch 98/150\n",
      "2722/2722 [==============================] - 33s 12ms/step - loss: 0.5403 - accuracy: 0.8248\n",
      "Epoch 99/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.5469 - accuracy: 0.8226\n",
      "Epoch 100/150\n",
      "2722/2722 [==============================] - 29s 10ms/step - loss: 0.5220 - accuracy: 0.8299\n",
      "Epoch 101/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.5134 - accuracy: 0.8288\n",
      "Epoch 102/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.5112 - accuracy: 0.8303\n",
      "Epoch 103/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.5053 - accuracy: 0.8354\n",
      "Epoch 104/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.5254 - accuracy: 0.8303\n",
      "Epoch 105/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.5124 - accuracy: 0.8373\n",
      "Epoch 106/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.5086 - accuracy: 0.8347\n",
      "Epoch 107/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.5082 - accuracy: 0.8369\n",
      "Epoch 108/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.7098 - accuracy: 0.8031\n",
      "Epoch 109/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.5901 - accuracy: 0.8152\n",
      "Epoch 110/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6249 - accuracy: 0.8141\n",
      "Epoch 111/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5421 - accuracy: 0.8358\n",
      "Epoch 112/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.5089 - accuracy: 0.8358\n",
      "Epoch 113/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.5063 - accuracy: 0.8409\n",
      "Epoch 114/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5061 - accuracy: 0.8450\n",
      "Epoch 115/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.4989 - accuracy: 0.8453\n",
      "Epoch 116/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5096 - accuracy: 0.8446\n",
      "Epoch 117/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.4922 - accuracy: 0.8468\n",
      "Epoch 118/150\n",
      "2722/2722 [==============================] - 29s 10ms/step - loss: 0.5093 - accuracy: 0.8497\n",
      "Epoch 119/150\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.4461 - accuracy: 0.8564\n",
      "Epoch 120/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4723 - accuracy: 0.8545\n",
      "Epoch 121/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4418 - accuracy: 0.8553\n",
      "Epoch 122/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4568 - accuracy: 0.8519\n",
      "Epoch 123/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4580 - accuracy: 0.8597\n",
      "Epoch 124/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4378 - accuracy: 0.8586\n",
      "Epoch 125/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4410 - accuracy: 0.8604\n",
      "Epoch 126/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.4602 - accuracy: 0.8567\n",
      "Epoch 127/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4260 - accuracy: 0.8622\n",
      "Epoch 128/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4149 - accuracy: 0.8655\n",
      "Epoch 129/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4186 - accuracy: 0.8637\n",
      "Epoch 130/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4088 - accuracy: 0.8641\n",
      "Epoch 131/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4218 - accuracy: 0.8641\n",
      "Epoch 132/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.4174 - accuracy: 0.8644\n",
      "Epoch 133/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.4047 - accuracy: 0.8696\n",
      "Epoch 134/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.3940 - accuracy: 0.8703\n",
      "Epoch 135/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.3934 - accuracy: 0.8696\n",
      "Epoch 136/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.3844 - accuracy: 0.8733\n",
      "Epoch 137/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.3791 - accuracy: 0.8736\n",
      "Epoch 138/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4120 - accuracy: 0.8711\n",
      "Epoch 139/150\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 0.3763 - accuracy: 0.8740\n",
      "Epoch 140/150\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 0.3772 - accuracy: 0.8725\n",
      "Epoch 141/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4224 - accuracy: 0.8674\n",
      "Epoch 142/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4859 - accuracy: 0.8608\n",
      "Epoch 143/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4450 - accuracy: 0.8707\n",
      "Epoch 144/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4015 - accuracy: 0.8755\n",
      "Epoch 145/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4759 - accuracy: 0.8549\n",
      "Epoch 146/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3957 - accuracy: 0.8740\n",
      "Epoch 147/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3887 - accuracy: 0.8854\n",
      "Epoch 148/150\n",
      "2722/2722 [==============================] - 33s 12ms/step - loss: 0.4272 - accuracy: 0.8641\n",
      "Epoch 149/150\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3975 - accuracy: 0.8729\n",
      "Epoch 150/150\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.3740 - accuracy: 0.8810\n",
      "2722/2722 [==============================] - 28s 10ms/step\n",
      "> 88.832\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_3, y_train_3, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_3, y_train_3, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd3 150 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147c77c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 4.2320 - accuracy: 0.3674\n",
      "Epoch 2/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.7411 - accuracy: 0.4853\n",
      "Epoch 3/200\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 1.6302 - accuracy: 0.5088\n",
      "Epoch 4/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.5699 - accuracy: 0.5228\n",
      "Epoch 5/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.5313 - accuracy: 0.5371\n",
      "Epoch 6/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.4997 - accuracy: 0.5481\n",
      "Epoch 7/200\n",
      "2722/2722 [==============================] - 30s 11ms/step - loss: 1.4674 - accuracy: 0.5536\n",
      "Epoch 8/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 1.4392 - accuracy: 0.5602\n",
      "Epoch 9/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.4202 - accuracy: 0.5691\n",
      "Epoch 10/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.3978 - accuracy: 0.5705\n",
      "Epoch 11/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.3804 - accuracy: 0.5716\n",
      "Epoch 12/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.3594 - accuracy: 0.5771\n",
      "Epoch 13/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.3785 - accuracy: 0.5775\n",
      "Epoch 14/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.3671 - accuracy: 0.5812\n",
      "Epoch 15/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.3518 - accuracy: 0.5860\n",
      "Epoch 16/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.3263 - accuracy: 0.5856\n",
      "Epoch 17/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.3290 - accuracy: 0.5889\n",
      "Epoch 18/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.2979 - accuracy: 0.5937\n",
      "Epoch 19/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.2730 - accuracy: 0.6014\n",
      "Epoch 20/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.2602 - accuracy: 0.6076\n",
      "Epoch 21/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.2324 - accuracy: 0.6157\n",
      "Epoch 22/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.2114 - accuracy: 0.6231\n",
      "Epoch 23/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.1779 - accuracy: 0.6330\n",
      "Epoch 24/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.1619 - accuracy: 0.6370\n",
      "Epoch 25/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.1358 - accuracy: 0.6411\n",
      "Epoch 26/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.1216 - accuracy: 0.6418\n",
      "Epoch 27/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.1182 - accuracy: 0.6481\n",
      "Epoch 28/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0964 - accuracy: 0.6495\n",
      "Epoch 29/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0921 - accuracy: 0.6543\n",
      "Epoch 30/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0657 - accuracy: 0.6661\n",
      "Epoch 31/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0788 - accuracy: 0.6635\n",
      "Epoch 32/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0505 - accuracy: 0.6613\n",
      "Epoch 33/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0454 - accuracy: 0.6635\n",
      "Epoch 34/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0204 - accuracy: 0.6679\n",
      "Epoch 35/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0226 - accuracy: 0.6767\n",
      "Epoch 36/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 1.0095 - accuracy: 0.6841\n",
      "Epoch 37/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0129 - accuracy: 0.6774\n",
      "Epoch 38/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9915 - accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9696 - accuracy: 0.6870\n",
      "Epoch 40/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.9569 - accuracy: 0.7046\n",
      "Epoch 41/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 1.0267 - accuracy: 0.6741\n",
      "Epoch 42/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9518 - accuracy: 0.6991\n",
      "Epoch 43/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.9283 - accuracy: 0.7087\n",
      "Epoch 44/200\n",
      "2722/2722 [==============================] - 32s 12ms/step - loss: 0.9053 - accuracy: 0.7134\n",
      "Epoch 45/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.8750 - accuracy: 0.7256\n",
      "Epoch 46/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.8555 - accuracy: 0.7344\n",
      "Epoch 47/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.8340 - accuracy: 0.7428\n",
      "Epoch 48/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7997 - accuracy: 0.7487\n",
      "Epoch 49/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7925 - accuracy: 0.7535\n",
      "Epoch 50/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7652 - accuracy: 0.7594\n",
      "Epoch 51/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.7398 - accuracy: 0.7682\n",
      "Epoch 52/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.7332 - accuracy: 0.7726\n",
      "Epoch 53/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.6943 - accuracy: 0.7799\n",
      "Epoch 54/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6902 - accuracy: 0.7796\n",
      "Epoch 55/200\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 0.6677 - accuracy: 0.7855\n",
      "Epoch 56/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6582 - accuracy: 0.7921\n",
      "Epoch 57/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6234 - accuracy: 0.7987\n",
      "Epoch 58/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.6177 - accuracy: 0.7979\n",
      "Epoch 59/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5914 - accuracy: 0.8101\n",
      "Epoch 60/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5722 - accuracy: 0.8112\n",
      "Epoch 61/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5782 - accuracy: 0.8119\n",
      "Epoch 62/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5639 - accuracy: 0.8167\n",
      "Epoch 63/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5489 - accuracy: 0.8178\n",
      "Epoch 64/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5148 - accuracy: 0.8281\n",
      "Epoch 65/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5051 - accuracy: 0.8314\n",
      "Epoch 66/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5620 - accuracy: 0.8240\n",
      "Epoch 67/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4986 - accuracy: 0.8339\n",
      "Epoch 68/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.5808 - accuracy: 0.8159\n",
      "Epoch 69/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4991 - accuracy: 0.8343\n",
      "Epoch 70/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4631 - accuracy: 0.8442\n",
      "Epoch 71/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4177 - accuracy: 0.8564\n",
      "Epoch 72/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.4032 - accuracy: 0.8637\n",
      "Epoch 73/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3855 - accuracy: 0.8666\n",
      "Epoch 74/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3756 - accuracy: 0.8674\n",
      "Epoch 75/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3671 - accuracy: 0.8725\n",
      "Epoch 76/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3617 - accuracy: 0.8744\n",
      "Epoch 77/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3528 - accuracy: 0.8766\n",
      "Epoch 78/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.3616 - accuracy: 0.8744\n",
      "Epoch 79/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3375 - accuracy: 0.8810\n",
      "Epoch 80/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.3379 - accuracy: 0.8817\n",
      "Epoch 81/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.3301 - accuracy: 0.8817\n",
      "Epoch 82/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3238 - accuracy: 0.8839\n",
      "Epoch 83/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.3352 - accuracy: 0.8813\n",
      "Epoch 84/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.3179 - accuracy: 0.8843\n",
      "Epoch 85/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.2966 - accuracy: 0.8894\n",
      "Epoch 86/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2941 - accuracy: 0.8920\n",
      "Epoch 87/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2798 - accuracy: 0.8960\n",
      "Epoch 88/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2759 - accuracy: 0.8971\n",
      "Epoch 89/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2766 - accuracy: 0.8964\n",
      "Epoch 90/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2598 - accuracy: 0.9052\n",
      "Epoch 91/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2511 - accuracy: 0.9048\n",
      "Epoch 92/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2463 - accuracy: 0.9056\n",
      "Epoch 93/200\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.2581 - accuracy: 0.9037\n",
      "Epoch 94/200\n",
      "2722/2722 [==============================] - 29s 11ms/step - loss: 0.2442 - accuracy: 0.9085\n",
      "Epoch 95/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2426 - accuracy: 0.9085\n",
      "Epoch 96/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2492 - accuracy: 0.9082\n",
      "Epoch 97/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.2446 - accuracy: 0.9100\n",
      "Epoch 98/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2274 - accuracy: 0.9122\n",
      "Epoch 99/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2235 - accuracy: 0.9155\n",
      "Epoch 100/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2262 - accuracy: 0.9151\n",
      "Epoch 101/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2198 - accuracy: 0.9188\n",
      "Epoch 102/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2200 - accuracy: 0.9192\n",
      "Epoch 103/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2534 - accuracy: 0.9115\n",
      "Epoch 104/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2166 - accuracy: 0.9170\n",
      "Epoch 105/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2077 - accuracy: 0.9229\n",
      "Epoch 106/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2057 - accuracy: 0.9217\n",
      "Epoch 107/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1996 - accuracy: 0.9243\n",
      "Epoch 108/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1974 - accuracy: 0.9262\n",
      "Epoch 109/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1961 - accuracy: 0.9247\n",
      "Epoch 110/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.2054 - accuracy: 0.9232\n",
      "Epoch 111/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1923 - accuracy: 0.9265\n",
      "Epoch 112/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1835 - accuracy: 0.9302\n",
      "Epoch 113/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1854 - accuracy: 0.9280\n",
      "Epoch 114/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1790 - accuracy: 0.9324\n",
      "Epoch 115/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1698 - accuracy: 0.9350\n",
      "Epoch 116/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1625 - accuracy: 0.9346\n",
      "Epoch 117/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1552 - accuracy: 0.9401\n",
      "Epoch 118/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1554 - accuracy: 0.9398\n",
      "Epoch 119/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1530 - accuracy: 0.9368\n",
      "Epoch 120/200\n",
      "2722/2722 [==============================] - 28s 10ms/step - loss: 0.1520 - accuracy: 0.9434\n",
      "Epoch 121/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1458 - accuracy: 0.9431\n",
      "Epoch 122/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1465 - accuracy: 0.9423\n",
      "Epoch 123/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1500 - accuracy: 0.9442\n",
      "Epoch 124/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1488 - accuracy: 0.9431\n",
      "Epoch 125/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1473 - accuracy: 0.9445\n",
      "Epoch 126/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1366 - accuracy: 0.9471\n",
      "Epoch 127/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1322 - accuracy: 0.9478\n",
      "Epoch 128/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1341 - accuracy: 0.9478\n",
      "Epoch 129/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1361 - accuracy: 0.9500\n",
      "Epoch 130/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1401 - accuracy: 0.9475\n",
      "Epoch 131/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1442 - accuracy: 0.9456\n",
      "Epoch 132/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1210 - accuracy: 0.9533\n",
      "Epoch 133/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1206 - accuracy: 0.9533\n",
      "Epoch 134/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1185 - accuracy: 0.9541\n",
      "Epoch 135/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1154 - accuracy: 0.9555\n",
      "Epoch 136/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1207 - accuracy: 0.9533\n",
      "Epoch 137/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1206 - accuracy: 0.9515\n",
      "Epoch 138/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1118 - accuracy: 0.9563\n",
      "Epoch 139/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1102 - accuracy: 0.9563\n",
      "Epoch 140/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1144 - accuracy: 0.9555\n",
      "Epoch 141/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1124 - accuracy: 0.9589\n",
      "Epoch 142/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1028 - accuracy: 0.9581\n",
      "Epoch 143/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1002 - accuracy: 0.9581\n",
      "Epoch 144/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0998 - accuracy: 0.9585\n",
      "Epoch 145/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1025 - accuracy: 0.9589\n",
      "Epoch 146/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0975 - accuracy: 0.9607\n",
      "Epoch 147/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0958 - accuracy: 0.9629\n",
      "Epoch 148/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0928 - accuracy: 0.9640\n",
      "Epoch 149/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0903 - accuracy: 0.9669\n",
      "Epoch 150/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0931 - accuracy: 0.9640\n",
      "Epoch 151/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0911 - accuracy: 0.9644\n",
      "Epoch 152/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0873 - accuracy: 0.9658\n",
      "Epoch 153/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0853 - accuracy: 0.9677\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0816 - accuracy: 0.9691\n",
      "Epoch 155/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0779 - accuracy: 0.9684\n",
      "Epoch 156/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0927 - accuracy: 0.9651\n",
      "Epoch 157/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0909 - accuracy: 0.9662\n",
      "Epoch 158/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0828 - accuracy: 0.9669\n",
      "Epoch 159/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0767 - accuracy: 0.9702\n",
      "Epoch 160/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0757 - accuracy: 0.9713\n",
      "Epoch 161/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0737 - accuracy: 0.9713\n",
      "Epoch 162/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0724 - accuracy: 0.9717\n",
      "Epoch 163/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0726 - accuracy: 0.9710\n",
      "Epoch 164/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0996 - accuracy: 0.9673\n",
      "Epoch 165/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.1139 - accuracy: 0.9578\n",
      "Epoch 166/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0869 - accuracy: 0.9680\n",
      "Epoch 167/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0762 - accuracy: 0.9706\n",
      "Epoch 168/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0781 - accuracy: 0.9706\n",
      "Epoch 169/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0779 - accuracy: 0.9702\n",
      "Epoch 170/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0788 - accuracy: 0.9713\n",
      "Epoch 171/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0848 - accuracy: 0.9699\n",
      "Epoch 172/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.1036 - accuracy: 0.9633\n",
      "Epoch 173/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0939 - accuracy: 0.9647\n",
      "Epoch 174/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0790 - accuracy: 0.9699\n",
      "Epoch 175/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0687 - accuracy: 0.9735\n",
      "Epoch 176/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0673 - accuracy: 0.9750\n",
      "Epoch 177/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0754 - accuracy: 0.9724\n",
      "Epoch 178/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0693 - accuracy: 0.9728\n",
      "Epoch 179/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0693 - accuracy: 0.9747\n",
      "Epoch 180/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0631 - accuracy: 0.9772\n",
      "Epoch 181/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0633 - accuracy: 0.9769\n",
      "Epoch 182/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0624 - accuracy: 0.9765\n",
      "Epoch 183/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0593 - accuracy: 0.9780\n",
      "Epoch 184/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0618 - accuracy: 0.9787\n",
      "Epoch 185/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0583 - accuracy: 0.9776\n",
      "Epoch 186/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0568 - accuracy: 0.9780\n",
      "Epoch 187/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0541 - accuracy: 0.9780\n",
      "Epoch 188/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0544 - accuracy: 0.9783\n",
      "Epoch 189/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0536 - accuracy: 0.9809\n",
      "Epoch 190/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0519 - accuracy: 0.9802\n",
      "Epoch 191/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0525 - accuracy: 0.9805\n",
      "Epoch 192/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0507 - accuracy: 0.9809\n",
      "Epoch 193/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0514 - accuracy: 0.9802\n",
      "Epoch 194/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0501 - accuracy: 0.9816\n",
      "Epoch 195/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0525 - accuracy: 0.9805\n",
      "Epoch 196/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0500 - accuracy: 0.9802\n",
      "Epoch 197/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0461 - accuracy: 0.9816\n",
      "Epoch 198/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0476 - accuracy: 0.9813\n",
      "Epoch 199/200\n",
      "2722/2722 [==============================] - 27s 10ms/step - loss: 0.0476 - accuracy: 0.9813\n",
      "Epoch 200/200\n",
      "2722/2722 [==============================] - 26s 10ms/step - loss: 0.0515 - accuracy: 0.9816\n",
      "2722/2722 [==============================] - 27s 10ms/step\n",
      "> 98.090\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_3, y_train_3, epochs=200, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_3, y_train_3, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd3 200 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bbd13c",
   "metadata": {},
   "source": [
    "# VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68db7788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 3.4009 - accuracy: 0.3262\n",
      "Epoch 2/10\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 1.7246 - accuracy: 0.4754\n",
      "Epoch 3/10\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 1.4683 - accuracy: 0.5426\n",
      "Epoch 4/10\n",
      "2722/2722 [==============================] - 35s 13ms/step - loss: 1.2171 - accuracy: 0.6154\n",
      "Epoch 5/10\n",
      "2722/2722 [==============================] - 36s 13ms/step - loss: 1.0172 - accuracy: 0.6719\n",
      "Epoch 6/10\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.8697 - accuracy: 0.7134\n",
      "Epoch 7/10\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.7122 - accuracy: 0.7623\n",
      "Epoch 8/10\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 0.5553 - accuracy: 0.8215\n",
      "Epoch 9/10\n",
      "2722/2722 [==============================] - 31s 11ms/step - loss: 0.4429 - accuracy: 0.8556\n",
      "Epoch 10/10\n",
      "2722/2722 [==============================] - 33s 12ms/step - loss: 0.3378 - accuracy: 0.8975\n",
      "2722/2722 [==============================] - 35s 13ms/step\n",
      "> 92.946\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_3, y_train_3, epochs=10, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_3, y_train_3, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd3 10 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2d0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2722/2722 [==============================] - 36s 13ms/step - loss: 3.6483 - accuracy: 0.3292\n",
      "Epoch 2/15\n",
      "2722/2722 [==============================] - 33s 12ms/step - loss: 1.6669 - accuracy: 0.4927\n",
      "Epoch 3/15\n",
      "2722/2722 [==============================] - 34s 13ms/step - loss: 1.3893 - accuracy: 0.5639\n",
      "Epoch 4/15\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 1.1585 - accuracy: 0.6253\n",
      "Epoch 5/15\n",
      "2722/2722 [==============================] - 34s 12ms/step - loss: 0.9881 - accuracy: 0.6712\n",
      "Epoch 6/15\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.8106 - accuracy: 0.7329\n",
      "Epoch 7/15\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.7307 - accuracy: 0.7601\n",
      "Epoch 8/15\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.5758 - accuracy: 0.8035\n",
      "Epoch 9/15\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.4517 - accuracy: 0.8571\n",
      "Epoch 10/15\n",
      "2722/2722 [==============================] - 34s 12ms/step - loss: 0.3532 - accuracy: 0.8931\n",
      "Epoch 11/15\n",
      "2722/2722 [==============================] - 37s 13ms/step - loss: 0.2751 - accuracy: 0.9221\n",
      "Epoch 12/15\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.2260 - accuracy: 0.9379\n",
      "Epoch 13/15\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.1649 - accuracy: 0.9600\n",
      "Epoch 14/15\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.1308 - accuracy: 0.9769\n",
      "Epoch 15/15\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.1449 - accuracy: 0.9702\n",
      "2722/2722 [==============================] - 43s 16ms/step\n",
      "> 98.604\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_3, y_train_3, epochs=15, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_3, y_train_3, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd3 15 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7b9e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 3.2972 - accuracy: 0.3240\n",
      "Epoch 2/150\n",
      "2722/2722 [==============================] - 53s 20ms/step - loss: 1.7627 - accuracy: 0.4563\n",
      "Epoch 3/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 1.6247 - accuracy: 0.5147\n",
      "Epoch 4/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 1.5328 - accuracy: 0.5386\n",
      "Epoch 5/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 1.4995 - accuracy: 0.5514\n",
      "Epoch 6/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 1.4040 - accuracy: 0.5768\n",
      "Epoch 7/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 1.3296 - accuracy: 0.6032\n",
      "Epoch 8/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 1.2593 - accuracy: 0.6172\n",
      "Epoch 9/150\n",
      "2722/2722 [==============================] - 48s 18ms/step - loss: 1.2163 - accuracy: 0.6194\n",
      "Epoch 10/150\n",
      "2722/2722 [==============================] - 44s 16ms/step - loss: 1.1692 - accuracy: 0.6385\n",
      "Epoch 11/150\n",
      "2722/2722 [==============================] - 50s 18ms/step - loss: 1.0989 - accuracy: 0.6444\n",
      "Epoch 12/150\n",
      "2722/2722 [==============================] - 48s 18ms/step - loss: 1.0882 - accuracy: 0.6539\n",
      "Epoch 13/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 1.0175 - accuracy: 0.6756\n",
      "Epoch 14/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.9862 - accuracy: 0.6763\n",
      "Epoch 15/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.9336 - accuracy: 0.6914\n",
      "Epoch 16/150\n",
      "2722/2722 [==============================] - 51s 19ms/step - loss: 0.8982 - accuracy: 0.7017\n",
      "Epoch 17/150\n",
      "2722/2722 [==============================] - 55s 20ms/step - loss: 0.8559 - accuracy: 0.7083\n",
      "Epoch 18/150\n",
      "2722/2722 [==============================] - 55s 20ms/step - loss: 0.8355 - accuracy: 0.7105\n",
      "Epoch 19/150\n",
      "2722/2722 [==============================] - 53s 20ms/step - loss: 0.8178 - accuracy: 0.7237\n",
      "Epoch 20/150\n",
      "2722/2722 [==============================] - 57s 21ms/step - loss: 0.7787 - accuracy: 0.7300\n",
      "Epoch 21/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 0.7373 - accuracy: 0.7425\n",
      "Epoch 22/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.7086 - accuracy: 0.7513\n",
      "Epoch 23/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 0.6838 - accuracy: 0.7575\n",
      "Epoch 24/150\n",
      "2722/2722 [==============================] - 45s 17ms/step - loss: 0.6668 - accuracy: 0.7660\n",
      "Epoch 25/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.6239 - accuracy: 0.7748\n",
      "Epoch 26/150\n",
      "2722/2722 [==============================] - 55s 20ms/step - loss: 0.5997 - accuracy: 0.7902\n",
      "Epoch 27/150\n",
      "2722/2722 [==============================] - 50s 19ms/step - loss: 0.5790 - accuracy: 0.7939\n",
      "Epoch 28/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.6611 - accuracy: 0.7752\n",
      "Epoch 29/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 0.5486 - accuracy: 0.8020\n",
      "Epoch 30/150\n",
      "2722/2722 [==============================] - 44s 16ms/step - loss: 0.4881 - accuracy: 0.8215\n",
      "Epoch 31/150\n",
      "2722/2722 [==============================] - 44s 16ms/step - loss: 0.5098 - accuracy: 0.8196\n",
      "Epoch 32/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 0.4385 - accuracy: 0.8446\n",
      "Epoch 33/150\n",
      "2722/2722 [==============================] - 42s 16ms/step - loss: 0.4117 - accuracy: 0.8512\n",
      "Epoch 34/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.3670 - accuracy: 0.8648\n",
      "Epoch 35/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.3682 - accuracy: 0.8711\n",
      "Epoch 36/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.3068 - accuracy: 0.8913\n",
      "Epoch 37/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 0.2853 - accuracy: 0.8993\n",
      "Epoch 38/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.2616 - accuracy: 0.9034\n",
      "Epoch 39/150\n",
      "2722/2722 [==============================] - 57s 21ms/step - loss: 0.2322 - accuracy: 0.9133\n",
      "Epoch 40/150\n",
      "2722/2722 [==============================] - 58s 21ms/step - loss: 0.2301 - accuracy: 0.9144\n",
      "Epoch 41/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.2056 - accuracy: 0.9258\n",
      "Epoch 42/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.1847 - accuracy: 0.9364\n",
      "Epoch 43/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.1475 - accuracy: 0.9500\n",
      "Epoch 44/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.1302 - accuracy: 0.9559\n",
      "Epoch 45/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.1194 - accuracy: 0.9596\n",
      "Epoch 46/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.1070 - accuracy: 0.9655\n",
      "Epoch 47/150\n",
      "2722/2722 [==============================] - 42s 16ms/step - loss: 0.0964 - accuracy: 0.9684\n",
      "Epoch 48/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.1176 - accuracy: 0.9607\n",
      "Epoch 49/150\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.0934 - accuracy: 0.9695\n",
      "Epoch 50/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0788 - accuracy: 0.9750\n",
      "Epoch 51/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0750 - accuracy: 0.9743\n",
      "Epoch 52/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0732 - accuracy: 0.9750\n",
      "Epoch 53/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0658 - accuracy: 0.9780\n",
      "Epoch 54/150\n",
      "2722/2722 [==============================] - 45s 17ms/step - loss: 0.0588 - accuracy: 0.9805\n",
      "Epoch 55/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0512 - accuracy: 0.9827\n",
      "Epoch 56/150\n",
      "2722/2722 [==============================] - 42s 16ms/step - loss: 0.0507 - accuracy: 0.9842\n",
      "Epoch 57/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0442 - accuracy: 0.9853\n",
      "Epoch 58/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0387 - accuracy: 0.9871\n",
      "Epoch 59/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.0371 - accuracy: 0.9886\n",
      "Epoch 60/150\n",
      "2722/2722 [==============================] - 54s 20ms/step - loss: 0.0344 - accuracy: 0.9886\n",
      "Epoch 61/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0320 - accuracy: 0.9879\n",
      "Epoch 62/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0313 - accuracy: 0.9886\n",
      "Epoch 63/150\n",
      "2722/2722 [==============================] - 54s 20ms/step - loss: 0.0302 - accuracy: 0.9882\n",
      "Epoch 64/150\n",
      "2722/2722 [==============================] - 44s 16ms/step - loss: 0.0293 - accuracy: 0.9890\n",
      "Epoch 65/150\n",
      "2722/2722 [==============================] - 48s 18ms/step - loss: 0.0277 - accuracy: 0.9904\n",
      "Epoch 66/150\n",
      "2722/2722 [==============================] - 53s 19ms/step - loss: 0.0294 - accuracy: 0.9904\n",
      "Epoch 67/150\n",
      "2722/2722 [==============================] - 44s 16ms/step - loss: 0.0256 - accuracy: 0.9919\n",
      "Epoch 68/150\n",
      "2722/2722 [==============================] - 47s 17ms/step - loss: 0.0231 - accuracy: 0.9923\n",
      "Epoch 69/150\n",
      "2722/2722 [==============================] - 45s 16ms/step - loss: 0.0220 - accuracy: 0.9930\n",
      "Epoch 70/150\n",
      "2722/2722 [==============================] - 48s 18ms/step - loss: 0.0218 - accuracy: 0.9930\n",
      "Epoch 71/150\n",
      "2722/2722 [==============================] - 50s 19ms/step - loss: 0.0192 - accuracy: 0.9941\n",
      "Epoch 72/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0183 - accuracy: 0.9941\n",
      "Epoch 73/150\n",
      "2722/2722 [==============================] - 44s 16ms/step - loss: 0.0178 - accuracy: 0.9945\n",
      "Epoch 74/150\n",
      "2722/2722 [==============================] - 42s 16ms/step - loss: 0.0163 - accuracy: 0.9949\n",
      "Epoch 75/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.0140 - accuracy: 0.9952\n",
      "Epoch 76/150\n",
      "2722/2722 [==============================] - 58s 21ms/step - loss: 0.0229 - accuracy: 0.9941\n",
      "Epoch 77/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 78/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0124 - accuracy: 0.9956\n",
      "Epoch 79/150\n",
      "2722/2722 [==============================] - 51s 19ms/step - loss: 0.0128 - accuracy: 0.9956\n",
      "Epoch 80/150\n",
      "2722/2722 [==============================] - 42s 16ms/step - loss: 0.0113 - accuracy: 0.9963\n",
      "Epoch 81/150\n",
      "2722/2722 [==============================] - 45s 16ms/step - loss: 0.0104 - accuracy: 0.9963\n",
      "Epoch 82/150\n",
      "2722/2722 [==============================] - 47s 17ms/step - loss: 0.0107 - accuracy: 0.9967\n",
      "Epoch 83/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0096 - accuracy: 0.9967\n",
      "Epoch 84/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 85/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 86/150\n",
      "2722/2722 [==============================] - 42s 16ms/step - loss: 0.0099 - accuracy: 0.9971\n",
      "Epoch 87/150\n",
      "2722/2722 [==============================] - 50s 18ms/step - loss: 0.0109 - accuracy: 0.9971\n",
      "Epoch 88/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.0083 - accuracy: 0.9974\n",
      "Epoch 89/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0079 - accuracy: 0.9974\n",
      "Epoch 90/150\n",
      "2722/2722 [==============================] - 48s 17ms/step - loss: 0.0072 - accuracy: 0.9974\n",
      "Epoch 91/150\n",
      "2722/2722 [==============================] - 51s 19ms/step - loss: 0.0070 - accuracy: 0.9974\n",
      "Epoch 92/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0070 - accuracy: 0.9974\n",
      "Epoch 93/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0064 - accuracy: 0.9974\n",
      "Epoch 94/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0063 - accuracy: 0.9974\n",
      "Epoch 95/150\n",
      "2722/2722 [==============================] - 47s 17ms/step - loss: 0.0063 - accuracy: 0.9974\n",
      "Epoch 96/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0063 - accuracy: 0.9974\n",
      "Epoch 97/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0061 - accuracy: 0.9974\n",
      "Epoch 98/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.0061 - accuracy: 0.9974\n",
      "Epoch 99/150\n",
      "2722/2722 [==============================] - 48s 18ms/step - loss: 0.0059 - accuracy: 0.9974\n",
      "Epoch 100/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0058 - accuracy: 0.9974\n",
      "Epoch 101/150\n",
      "2722/2722 [==============================] - 48s 18ms/step - loss: 0.0057 - accuracy: 0.9974\n",
      "Epoch 102/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0058 - accuracy: 0.9978\n",
      "Epoch 103/150\n",
      "2722/2722 [==============================] - 48s 18ms/step - loss: 0.0057 - accuracy: 0.9978\n",
      "Epoch 104/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0058 - accuracy: 0.9978\n",
      "Epoch 105/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0054 - accuracy: 0.9978\n",
      "Epoch 106/150\n",
      "2722/2722 [==============================] - 47s 17ms/step - loss: 0.0054 - accuracy: 0.9982\n",
      "Epoch 107/150\n",
      "2722/2722 [==============================] - 45s 17ms/step - loss: 0.0051 - accuracy: 0.9982\n",
      "Epoch 108/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0055 - accuracy: 0.9985\n",
      "Epoch 109/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.0047 - accuracy: 0.9985\n",
      "Epoch 110/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0046 - accuracy: 0.9985\n",
      "Epoch 111/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.0044 - accuracy: 0.9985\n",
      "Epoch 112/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.0044 - accuracy: 0.9985\n",
      "Epoch 113/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0043 - accuracy: 0.9985\n",
      "Epoch 114/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0042 - accuracy: 0.9985\n",
      "Epoch 115/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0041 - accuracy: 0.9985\n",
      "Epoch 116/150\n",
      "2722/2722 [==============================] - 45s 16ms/step - loss: 0.0040 - accuracy: 0.9985\n",
      "Epoch 117/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0040 - accuracy: 0.9985\n",
      "Epoch 118/150\n",
      "2722/2722 [==============================] - 45s 17ms/step - loss: 0.0039 - accuracy: 0.9985\n",
      "Epoch 119/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0040 - accuracy: 0.9985\n",
      "Epoch 120/150\n",
      "2722/2722 [==============================] - 52s 19ms/step - loss: 0.0038 - accuracy: 0.9985\n",
      "Epoch 121/150\n",
      "2722/2722 [==============================] - 55s 20ms/step - loss: 0.0037 - accuracy: 0.9985\n",
      "Epoch 122/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0037 - accuracy: 0.9985\n",
      "Epoch 123/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0039 - accuracy: 0.9985\n",
      "Epoch 124/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0041 - accuracy: 0.9985\n",
      "Epoch 125/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0039 - accuracy: 0.9985\n",
      "Epoch 126/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.0037 - accuracy: 0.9985\n",
      "Epoch 127/150\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.0033 - accuracy: 0.9985\n",
      "Epoch 128/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.0032 - accuracy: 0.9985\n",
      "Epoch 129/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 130/150\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 131/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 132/150\n",
      "2722/2722 [==============================] - 50s 18ms/step - loss: 0.0029 - accuracy: 0.9989\n",
      "Epoch 133/150\n",
      "2722/2722 [==============================] - 42s 15ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 134/150\n",
      "2722/2722 [==============================] - 45s 17ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 135/150\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 136/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0027 - accuracy: 0.9989\n",
      "Epoch 137/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0027 - accuracy: 0.9989\n",
      "Epoch 138/150\n",
      "2722/2722 [==============================] - 49s 18ms/step - loss: 0.0026 - accuracy: 0.9989\n",
      "Epoch 139/150\n",
      "2722/2722 [==============================] - 52s 19ms/step - loss: 0.0026 - accuracy: 0.9989\n",
      "Epoch 140/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.0026 - accuracy: 0.9989\n",
      "Epoch 141/150\n",
      "2722/2722 [==============================] - 46s 17ms/step - loss: 0.0025 - accuracy: 0.9989\n",
      "Epoch 142/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0025 - accuracy: 0.9989\n",
      "Epoch 143/150\n",
      "2722/2722 [==============================] - 45s 16ms/step - loss: 0.0025 - accuracy: 0.9989\n",
      "Epoch 144/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0025 - accuracy: 0.9989\n",
      "Epoch 145/150\n",
      "2722/2722 [==============================] - 40s 15ms/step - loss: 0.0025 - accuracy: 0.9989\n",
      "Epoch 146/150\n",
      "2722/2722 [==============================] - 43s 16ms/step - loss: 0.0024 - accuracy: 0.9989\n",
      "Epoch 147/150\n",
      "2722/2722 [==============================] - 39s 14ms/step - loss: 0.0024 - accuracy: 0.9989\n",
      "Epoch 148/150\n",
      "2722/2722 [==============================] - 41s 15ms/step - loss: 0.0023 - accuracy: 0.9989\n",
      "Epoch 149/150\n",
      "2722/2722 [==============================] - 38s 14ms/step - loss: 0.0023 - accuracy: 0.9989\n",
      "Epoch 150/150\n",
      "2722/2722 [==============================] - 45s 16ms/step - loss: 0.0023 - accuracy: 0.9989\n",
      "2722/2722 [==============================] - 43s 16ms/step\n",
      "> 99.890\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_3, y_train_3, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_3, y_train_3, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd3 150 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daf237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97132172",
   "metadata": {},
   "source": [
    "# with testing and training different set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6b6ca",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ea3851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 3.9563 - accuracy: 0.3532\n",
      "Epoch 2/15\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.5802 - accuracy: 0.5124\n",
      "Epoch 3/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.3195 - accuracy: 0.5787\n",
      "Epoch 4/15\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.1541 - accuracy: 0.6167\n",
      "Epoch 5/15\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.9970 - accuracy: 0.6672\n",
      "Epoch 6/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8337 - accuracy: 0.7154\n",
      "Epoch 7/15\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7002 - accuracy: 0.7692\n",
      "Epoch 8/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5718 - accuracy: 0.8121\n",
      "Epoch 9/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.4721 - accuracy: 0.8528\n",
      "Epoch 10/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.3761 - accuracy: 0.8859\n",
      "Epoch 11/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2990 - accuracy: 0.9115\n",
      "Epoch 12/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.2321 - accuracy: 0.9367\n",
      "Epoch 13/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1816 - accuracy: 0.9499\n",
      "Epoch 14/15\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.1440 - accuracy: 0.9635\n",
      "Epoch 15/15\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.1168 - accuracy: 0.9721\n",
      "2523/2523 [==============================] - 36s 14ms/step\n",
      "> 52.239\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=15, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd12 15 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "193ac050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2656/2656 [==============================] - 45s 17ms/step - loss: 3.7684 - accuracy: 0.3422\n",
      "Epoch 2/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.7645 - accuracy: 0.4642\n",
      "Epoch 3/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.6203 - accuracy: 0.5049\n",
      "Epoch 4/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.5300 - accuracy: 0.5410\n",
      "Epoch 5/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.4664 - accuracy: 0.5569\n",
      "Epoch 6/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.4245 - accuracy: 0.5670\n",
      "Epoch 7/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.3908 - accuracy: 0.5787\n",
      "Epoch 8/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.3334 - accuracy: 0.5862\n",
      "Epoch 9/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.2835 - accuracy: 0.6005\n",
      "Epoch 10/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.2282 - accuracy: 0.6133\n",
      "Epoch 11/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.2091 - accuracy: 0.6201\n",
      "Epoch 12/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.1714 - accuracy: 0.6306\n",
      "Epoch 13/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.1374 - accuracy: 0.6416\n",
      "Epoch 14/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.0987 - accuracy: 0.6529\n",
      "Epoch 15/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.0453 - accuracy: 0.6615\n",
      "Epoch 16/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.0151 - accuracy: 0.6706\n",
      "Epoch 17/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.9781 - accuracy: 0.6830\n",
      "Epoch 18/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.9521 - accuracy: 0.6852\n",
      "Epoch 19/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.9300 - accuracy: 0.6916\n",
      "Epoch 20/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.8963 - accuracy: 0.6995\n",
      "Epoch 21/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.8553 - accuracy: 0.7067\n",
      "Epoch 22/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8316 - accuracy: 0.7120\n",
      "Epoch 23/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8018 - accuracy: 0.7252\n",
      "Epoch 24/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.7740 - accuracy: 0.7334\n",
      "Epoch 25/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7412 - accuracy: 0.7459\n",
      "Epoch 26/150\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.7080 - accuracy: 0.7534\n",
      "Epoch 27/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.6848 - accuracy: 0.7568\n",
      "Epoch 28/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.6473 - accuracy: 0.7718\n",
      "Epoch 29/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.6187 - accuracy: 0.7824\n",
      "Epoch 30/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.6031 - accuracy: 0.7880\n",
      "Epoch 31/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.5996 - accuracy: 0.7865\n",
      "Epoch 32/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.5546 - accuracy: 0.7956\n",
      "Epoch 33/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.5254 - accuracy: 0.8057\n",
      "Epoch 34/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.5044 - accuracy: 0.8121\n",
      "Epoch 35/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.4733 - accuracy: 0.8238\n",
      "Epoch 36/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.4567 - accuracy: 0.8276\n",
      "Epoch 37/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.4415 - accuracy: 0.8343\n",
      "Epoch 38/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.4340 - accuracy: 0.8430\n",
      "Epoch 39/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.4054 - accuracy: 0.8513\n",
      "Epoch 40/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3957 - accuracy: 0.8505\n",
      "Epoch 41/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3791 - accuracy: 0.8554\n",
      "Epoch 42/150\n",
      "2656/2656 [==============================] - 45s 17ms/step - loss: 0.3651 - accuracy: 0.8596\n",
      "Epoch 43/150\n",
      "2656/2656 [==============================] - 45s 17ms/step - loss: 0.3603 - accuracy: 0.8648\n",
      "Epoch 44/150\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.3342 - accuracy: 0.8720\n",
      "Epoch 45/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3360 - accuracy: 0.8769\n",
      "Epoch 46/150\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.3306 - accuracy: 0.8791\n",
      "Epoch 47/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3145 - accuracy: 0.8795\n",
      "Epoch 48/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3061 - accuracy: 0.8844\n",
      "Epoch 49/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2940 - accuracy: 0.8938\n",
      "Epoch 50/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2811 - accuracy: 0.8965\n",
      "Epoch 51/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2841 - accuracy: 0.8950\n",
      "Epoch 52/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2755 - accuracy: 0.9002\n",
      "Epoch 53/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3095 - accuracy: 0.8934\n",
      "Epoch 54/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2478 - accuracy: 0.9078\n",
      "Epoch 55/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2373 - accuracy: 0.9127\n",
      "Epoch 56/150\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2392 - accuracy: 0.9108\n",
      "Epoch 57/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2295 - accuracy: 0.9119\n",
      "Epoch 58/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2135 - accuracy: 0.9191\n",
      "Epoch 59/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2080 - accuracy: 0.9175\n",
      "Epoch 60/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1970 - accuracy: 0.9236\n",
      "Epoch 61/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1984 - accuracy: 0.9217\n",
      "Epoch 62/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2007 - accuracy: 0.9243\n",
      "Epoch 63/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1917 - accuracy: 0.9281\n",
      "Epoch 64/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.1883 - accuracy: 0.9322\n",
      "Epoch 65/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.1830 - accuracy: 0.9337\n",
      "Epoch 66/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.1713 - accuracy: 0.9379\n",
      "Epoch 67/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.1659 - accuracy: 0.9398\n",
      "Epoch 68/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1666 - accuracy: 0.9398\n",
      "Epoch 69/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1750 - accuracy: 0.9398\n",
      "Epoch 70/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1979 - accuracy: 0.9319\n",
      "Epoch 71/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.1686 - accuracy: 0.9401\n",
      "Epoch 72/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1714 - accuracy: 0.9424\n",
      "Epoch 73/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.1532 - accuracy: 0.9454\n",
      "Epoch 74/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.1472 - accuracy: 0.9473\n",
      "Epoch 75/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1458 - accuracy: 0.9511\n",
      "Epoch 76/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.1303 - accuracy: 0.9522\n",
      "Epoch 77/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1267 - accuracy: 0.9544\n",
      "Epoch 78/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.1274 - accuracy: 0.9541\n",
      "Epoch 79/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1179 - accuracy: 0.9563\n",
      "Epoch 80/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1155 - accuracy: 0.9586\n",
      "Epoch 81/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1092 - accuracy: 0.9601\n",
      "Epoch 82/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.1094 - accuracy: 0.9582\n",
      "Epoch 83/150\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.1095 - accuracy: 0.9605\n",
      "Epoch 84/150\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.1094 - accuracy: 0.9601\n",
      "Epoch 85/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.1051 - accuracy: 0.9620\n",
      "Epoch 86/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.1118 - accuracy: 0.9605\n",
      "Epoch 87/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.1144 - accuracy: 0.9608\n",
      "Epoch 88/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.1059 - accuracy: 0.9635\n",
      "Epoch 89/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0982 - accuracy: 0.9635\n",
      "Epoch 90/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0948 - accuracy: 0.9661\n",
      "Epoch 91/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0937 - accuracy: 0.9650\n",
      "Epoch 92/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0915 - accuracy: 0.9684\n",
      "Epoch 93/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0895 - accuracy: 0.9695\n",
      "Epoch 94/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0894 - accuracy: 0.9676\n",
      "Epoch 95/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0889 - accuracy: 0.9688\n",
      "Epoch 96/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0842 - accuracy: 0.9703\n",
      "Epoch 97/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0860 - accuracy: 0.9695\n",
      "Epoch 98/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0835 - accuracy: 0.9721\n",
      "Epoch 99/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0806 - accuracy: 0.9733\n",
      "Epoch 100/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0785 - accuracy: 0.9748\n",
      "Epoch 101/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0747 - accuracy: 0.9748\n",
      "Epoch 102/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0745 - accuracy: 0.9748\n",
      "Epoch 103/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0655 - accuracy: 0.9770\n",
      "Epoch 104/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0660 - accuracy: 0.9767\n",
      "Epoch 105/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0651 - accuracy: 0.9778\n",
      "Epoch 106/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0635 - accuracy: 0.9774\n",
      "Epoch 107/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0631 - accuracy: 0.9797\n",
      "Epoch 108/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0600 - accuracy: 0.9789\n",
      "Epoch 109/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0578 - accuracy: 0.9804\n",
      "Epoch 110/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0558 - accuracy: 0.9816\n",
      "Epoch 111/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0548 - accuracy: 0.9808\n",
      "Epoch 112/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0540 - accuracy: 0.9812\n",
      "Epoch 113/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0533 - accuracy: 0.9816\n",
      "Epoch 114/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0540 - accuracy: 0.9816\n",
      "Epoch 115/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0521 - accuracy: 0.9819\n",
      "Epoch 116/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0544 - accuracy: 0.9823\n",
      "Epoch 117/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0510 - accuracy: 0.9831\n",
      "Epoch 118/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0505 - accuracy: 0.9846\n",
      "Epoch 119/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0513 - accuracy: 0.9846\n",
      "Epoch 120/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0491 - accuracy: 0.9838\n",
      "Epoch 121/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0514 - accuracy: 0.9834\n",
      "Epoch 122/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0451 - accuracy: 0.9849\n",
      "Epoch 123/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0423 - accuracy: 0.9857\n",
      "Epoch 124/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0451 - accuracy: 0.9857\n",
      "Epoch 125/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0448 - accuracy: 0.9846\n",
      "Epoch 126/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0443 - accuracy: 0.9846\n",
      "Epoch 127/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0414 - accuracy: 0.9857\n",
      "Epoch 128/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0422 - accuracy: 0.9868\n",
      "Epoch 129/150\n",
      "2656/2656 [==============================] - 25s 10ms/step - loss: 0.0395 - accuracy: 0.9880\n",
      "Epoch 130/150\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0397 - accuracy: 0.9868\n",
      "Epoch 131/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0397 - accuracy: 0.9876\n",
      "Epoch 132/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0396 - accuracy: 0.9883\n",
      "Epoch 133/150\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.0391 - accuracy: 0.9868\n",
      "Epoch 134/150\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0387 - accuracy: 0.9876\n",
      "Epoch 135/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0416 - accuracy: 0.9872\n",
      "Epoch 136/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0442 - accuracy: 0.9876\n",
      "Epoch 137/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0434 - accuracy: 0.9857\n",
      "Epoch 138/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0418 - accuracy: 0.9880\n",
      "Epoch 139/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0441 - accuracy: 0.9876\n",
      "Epoch 140/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0461 - accuracy: 0.9846\n",
      "Epoch 141/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0551 - accuracy: 0.9831\n",
      "Epoch 142/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0482 - accuracy: 0.9861\n",
      "Epoch 143/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0420 - accuracy: 0.9861\n",
      "Epoch 144/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0366 - accuracy: 0.9906\n",
      "Epoch 145/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0360 - accuracy: 0.9898\n",
      "Epoch 146/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0337 - accuracy: 0.9891\n",
      "Epoch 147/150\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.0361 - accuracy: 0.9898\n",
      "Epoch 148/150\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.0354 - accuracy: 0.9906\n",
      "Epoch 149/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0323 - accuracy: 0.9913\n",
      "Epoch 150/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0315 - accuracy: 0.9906\n",
      "2523/2523 [==============================] - 42s 17ms/step\n",
      "> 52.398\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg16 fd12 150 epoch .h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81c9eb",
   "metadata": {},
   "source": [
    "# VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=15, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd12 15 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958e9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2656/2656 [==============================] - 46s 17ms/step - loss: 3.5803 - accuracy: 0.3362\n",
      "Epoch 2/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.6775 - accuracy: 0.4857\n",
      "Epoch 3/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 1.5504 - accuracy: 0.5316\n",
      "Epoch 4/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 1.4618 - accuracy: 0.5486\n",
      "Epoch 5/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.3989 - accuracy: 0.5640\n",
      "Epoch 6/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.3194 - accuracy: 0.5794\n",
      "Epoch 7/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.2246 - accuracy: 0.6103\n",
      "Epoch 8/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.1294 - accuracy: 0.6299\n",
      "Epoch 9/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.0341 - accuracy: 0.6536\n",
      "Epoch 10/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.9290 - accuracy: 0.6796\n",
      "Epoch 11/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.8510 - accuracy: 0.7090\n",
      "Epoch 12/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.7845 - accuracy: 0.7259\n",
      "Epoch 13/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.7869 - accuracy: 0.7214\n",
      "Epoch 14/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.7052 - accuracy: 0.7489\n",
      "Epoch 15/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.6696 - accuracy: 0.7583\n",
      "Epoch 16/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.6240 - accuracy: 0.7756\n",
      "Epoch 17/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.5771 - accuracy: 0.7918\n",
      "Epoch 18/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.5453 - accuracy: 0.8076\n",
      "Epoch 19/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.5229 - accuracy: 0.8114\n",
      "Epoch 20/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4942 - accuracy: 0.8253\n",
      "Epoch 21/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4791 - accuracy: 0.8298\n",
      "Epoch 22/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4643 - accuracy: 0.8441\n",
      "Epoch 23/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4293 - accuracy: 0.8498\n",
      "Epoch 24/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4099 - accuracy: 0.8581\n",
      "Epoch 25/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3979 - accuracy: 0.8637\n",
      "Epoch 26/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3654 - accuracy: 0.8750\n",
      "Epoch 27/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3491 - accuracy: 0.8810\n",
      "Epoch 28/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.3116 - accuracy: 0.8855\n",
      "Epoch 29/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.2806 - accuracy: 0.8991\n",
      "Epoch 30/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2654 - accuracy: 0.9059\n",
      "Epoch 31/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.2622 - accuracy: 0.9160\n",
      "Epoch 32/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.2441 - accuracy: 0.9157\n",
      "Epoch 33/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2135 - accuracy: 0.9273\n",
      "Epoch 34/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1916 - accuracy: 0.9334\n",
      "Epoch 35/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1759 - accuracy: 0.9413\n",
      "Epoch 36/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.1685 - accuracy: 0.9401\n",
      "Epoch 37/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.1541 - accuracy: 0.9477\n",
      "Epoch 38/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.1507 - accuracy: 0.9511\n",
      "Epoch 39/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1329 - accuracy: 0.9548\n",
      "Epoch 40/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1291 - accuracy: 0.9563\n",
      "Epoch 41/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1180 - accuracy: 0.9575\n",
      "Epoch 42/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.1147 - accuracy: 0.9608\n",
      "Epoch 43/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1135 - accuracy: 0.9623\n",
      "Epoch 44/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1042 - accuracy: 0.9642\n",
      "Epoch 45/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0937 - accuracy: 0.9672\n",
      "Epoch 46/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0851 - accuracy: 0.9725\n",
      "Epoch 47/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0803 - accuracy: 0.9710\n",
      "Epoch 48/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0780 - accuracy: 0.9718\n",
      "Epoch 49/150\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0742 - accuracy: 0.9721\n",
      "Epoch 50/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0743 - accuracy: 0.9748\n",
      "Epoch 51/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0736 - accuracy: 0.9763\n",
      "Epoch 52/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0716 - accuracy: 0.9752\n",
      "Epoch 53/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0669 - accuracy: 0.9767\n",
      "Epoch 54/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0675 - accuracy: 0.9759\n",
      "Epoch 55/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0606 - accuracy: 0.9804\n",
      "Epoch 56/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0574 - accuracy: 0.9808\n",
      "Epoch 57/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0568 - accuracy: 0.9816\n",
      "Epoch 58/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0517 - accuracy: 0.9819\n",
      "Epoch 59/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0503 - accuracy: 0.9823\n",
      "Epoch 60/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0520 - accuracy: 0.9819\n",
      "Epoch 61/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0524 - accuracy: 0.9834\n",
      "Epoch 62/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0454 - accuracy: 0.9838\n",
      "Epoch 63/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0435 - accuracy: 0.9834\n",
      "Epoch 64/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0407 - accuracy: 0.9853\n",
      "Epoch 65/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0398 - accuracy: 0.9849\n",
      "Epoch 66/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0388 - accuracy: 0.9846\n",
      "Epoch 67/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0378 - accuracy: 0.9849\n",
      "Epoch 68/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0346 - accuracy: 0.9849\n",
      "Epoch 69/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0356 - accuracy: 0.9853\n",
      "Epoch 70/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0340 - accuracy: 0.9861\n",
      "Epoch 71/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0318 - accuracy: 0.9861\n",
      "Epoch 72/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0304 - accuracy: 0.9864\n",
      "Epoch 73/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0314 - accuracy: 0.9883\n",
      "Epoch 74/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0281 - accuracy: 0.9883\n",
      "Epoch 75/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0261 - accuracy: 0.9898\n",
      "Epoch 76/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0284 - accuracy: 0.9906\n",
      "Epoch 77/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0249 - accuracy: 0.9906\n",
      "Epoch 78/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0221 - accuracy: 0.9906\n",
      "Epoch 79/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0222 - accuracy: 0.9925\n",
      "Epoch 80/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0213 - accuracy: 0.9921\n",
      "Epoch 81/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0216 - accuracy: 0.9928\n",
      "Epoch 82/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0212 - accuracy: 0.9928\n",
      "Epoch 83/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0183 - accuracy: 0.9936\n",
      "Epoch 84/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0172 - accuracy: 0.9944\n",
      "Epoch 85/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0169 - accuracy: 0.9940\n",
      "Epoch 86/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0143 - accuracy: 0.9947\n",
      "Epoch 87/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0130 - accuracy: 0.9951\n",
      "Epoch 88/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0129 - accuracy: 0.9951\n",
      "Epoch 89/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0122 - accuracy: 0.9955\n",
      "Epoch 90/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0129 - accuracy: 0.9951\n",
      "Epoch 91/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0124 - accuracy: 0.9955\n",
      "Epoch 92/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0109 - accuracy: 0.9955\n",
      "Epoch 93/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0106 - accuracy: 0.9959\n",
      "Epoch 94/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0104 - accuracy: 0.9959\n",
      "Epoch 95/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0106 - accuracy: 0.9959\n",
      "Epoch 96/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0118 - accuracy: 0.9959\n",
      "Epoch 97/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0143 - accuracy: 0.9962\n",
      "Epoch 98/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0123 - accuracy: 0.9966\n",
      "Epoch 99/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0107 - accuracy: 0.9962\n",
      "Epoch 100/150\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.0097 - accuracy: 0.9962\n",
      "Epoch 101/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0090 - accuracy: 0.9962\n",
      "Epoch 102/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.0086 - accuracy: 0.9966\n",
      "Epoch 103/150\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.0085 - accuracy: 0.9966\n",
      "Epoch 104/150\n",
      "2656/2656 [==============================] - 44s 17ms/step - loss: 0.0087 - accuracy: 0.9970\n",
      "Epoch 105/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0083 - accuracy: 0.9966\n",
      "Epoch 106/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0082 - accuracy: 0.9966\n",
      "Epoch 107/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0085 - accuracy: 0.9966\n",
      "Epoch 108/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0080 - accuracy: 0.9966\n",
      "Epoch 109/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0080 - accuracy: 0.9966\n",
      "Epoch 110/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0082 - accuracy: 0.9970\n",
      "Epoch 111/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0078 - accuracy: 0.9970\n",
      "Epoch 112/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0076 - accuracy: 0.9970\n",
      "Epoch 113/150\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0075 - accuracy: 0.9970\n",
      "Epoch 114/150\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0075 - accuracy: 0.9974\n",
      "Epoch 115/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0076 - accuracy: 0.9970\n",
      "Epoch 116/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0073 - accuracy: 0.9970\n",
      "Epoch 117/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0072 - accuracy: 0.9970\n",
      "Epoch 118/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0073 - accuracy: 0.9970\n",
      "Epoch 119/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0071 - accuracy: 0.9974\n",
      "Epoch 120/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0071 - accuracy: 0.9970\n",
      "Epoch 121/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0072 - accuracy: 0.9974\n",
      "Epoch 122/150\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0076 - accuracy: 0.9970\n",
      "Epoch 123/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0069 - accuracy: 0.9970\n",
      "Epoch 124/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0068 - accuracy: 0.9974\n",
      "Epoch 125/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0067 - accuracy: 0.9970\n",
      "Epoch 126/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0067 - accuracy: 0.9974\n",
      "Epoch 127/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0067 - accuracy: 0.9970\n",
      "Epoch 128/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0066 - accuracy: 0.9974\n",
      "Epoch 129/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0066 - accuracy: 0.9974\n",
      "Epoch 130/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0065 - accuracy: 0.9974\n",
      "Epoch 131/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0064 - accuracy: 0.9977\n",
      "Epoch 132/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0064 - accuracy: 0.9974\n",
      "Epoch 133/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0063 - accuracy: 0.9974\n",
      "Epoch 134/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0063 - accuracy: 0.9977\n",
      "Epoch 135/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0063 - accuracy: 0.9977\n",
      "Epoch 136/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0066 - accuracy: 0.9974\n",
      "Epoch 137/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0058 - accuracy: 0.9977\n",
      "Epoch 138/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0055 - accuracy: 0.9977\n",
      "Epoch 139/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0054 - accuracy: 0.9977\n",
      "Epoch 140/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0053 - accuracy: 0.9977\n",
      "Epoch 141/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0061 - accuracy: 0.9974\n",
      "Epoch 142/150\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0064 - accuracy: 0.9981\n",
      "Epoch 143/150\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0053 - accuracy: 0.9981\n",
      "Epoch 144/150\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 145/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 146/150\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.0042 - accuracy: 0.9981\n",
      "Epoch 147/150\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 148/150\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 149/150\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 150/150\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0037 - accuracy: 0.9989\n",
      "2523/2523 [==============================] - 41s 16ms/step\n",
      "> 51.209\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.0005, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train, epochs=150, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('vgg19 fd12 150 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6999c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eaea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d86dc4",
   "metadata": {},
   "source": [
    "# Model starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e974cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256, 256, 3)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(19, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cad7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cca2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4922d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 24692.0932 - accuracy: 0.2997\n",
      "Epoch 2/20\n",
      "2656/2656 [==============================] - 16s 6ms/step - loss: 2.8374 - accuracy: 0.3114\n",
      "Epoch 3/20\n",
      "2656/2656 [==============================] - 15s 6ms/step - loss: 2.7655 - accuracy: 0.3114\n",
      "Epoch 4/20\n",
      "2656/2656 [==============================] - 17s 6ms/step - loss: 2.7010 - accuracy: 0.3114\n",
      "Epoch 5/20\n",
      "2656/2656 [==============================] - 18s 7ms/step - loss: 2.6437 - accuracy: 0.3114\n",
      "Epoch 6/20\n",
      "2656/2656 [==============================] - 15s 6ms/step - loss: 2.5940 - accuracy: 0.3114\n",
      "Epoch 7/20\n",
      "2656/2656 [==============================] - 14s 5ms/step - loss: 2.5514 - accuracy: 0.3114\n",
      "Epoch 8/20\n",
      "2656/2656 [==============================] - 15s 6ms/step - loss: 2.5160 - accuracy: 0.3114\n",
      "Epoch 9/20\n",
      "2656/2656 [==============================] - 14s 5ms/step - loss: 2.4877 - accuracy: 0.3114\n",
      "Epoch 10/20\n",
      "2656/2656 [==============================] - 15s 6ms/step - loss: 2.4652 - accuracy: 0.3114\n",
      "Epoch 11/20\n",
      "2656/2656 [==============================] - 17s 6ms/step - loss: 2.4480 - accuracy: 0.3114\n",
      "Epoch 12/20\n",
      "2656/2656 [==============================] - 15s 6ms/step - loss: 2.4350 - accuracy: 0.3114\n",
      "Epoch 13/20\n",
      "2656/2656 [==============================] - 15s 6ms/step - loss: 2.4249 - accuracy: 0.3114\n",
      "Epoch 14/20\n",
      "2656/2656 [==============================] - 15s 6ms/step - loss: 2.4172 - accuracy: 0.3114\n",
      "Epoch 15/20\n",
      "2656/2656 [==============================] - 14s 5ms/step - loss: 2.4112 - accuracy: 0.3114\n",
      "Epoch 16/20\n",
      "2656/2656 [==============================] - 12s 4ms/step - loss: 2.4066 - accuracy: 0.3114\n",
      "Epoch 17/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 2.4030 - accuracy: 0.3114\n",
      "Epoch 18/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 2.4002 - accuracy: 0.3114\n",
      "Epoch 19/20\n",
      "2656/2656 [==============================] - 12s 4ms/step - loss: 2.3980 - accuracy: 0.3114\n",
      "Epoch 20/20\n",
      "2656/2656 [==============================] - 12s 5ms/step - loss: 2.3962 - accuracy: 0.3114\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(x_train, y_train2, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df4953e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2523/2523 [==============================] - 16s 7ms/step\n",
      "> 29.687\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e6814",
   "metadata": {},
   "source": [
    "# Explore Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5f1f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 18,911,571\n",
      "Trainable params: 4,196,883\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "2656/2656 [==============================] - 69s 26ms/step - loss: 9.2058 - accuracy: 0.2846\n",
      "Epoch 2/20\n",
      "2656/2656 [==============================] - 60s 23ms/step - loss: 2.4233 - accuracy: 0.3166\n",
      "Epoch 3/20\n",
      "2656/2656 [==============================] - 63s 24ms/step - loss: 2.0357 - accuracy: 0.4198\n",
      "Epoch 4/20\n",
      "2656/2656 [==============================] - 85s 32ms/step - loss: 1.7418 - accuracy: 0.4703\n",
      "Epoch 5/20\n",
      "2656/2656 [==============================] - 91s 34ms/step - loss: 1.6338 - accuracy: 0.4797\n",
      "Epoch 6/20\n",
      "2656/2656 [==============================] - 85s 32ms/step - loss: 1.6042 - accuracy: 0.4823\n",
      "Epoch 7/20\n",
      "2656/2656 [==============================] - 89s 34ms/step - loss: 1.5766 - accuracy: 0.4872\n",
      "Epoch 8/20\n",
      "2656/2656 [==============================] - 70s 26ms/step - loss: 1.5636 - accuracy: 0.4880\n",
      "Epoch 9/20\n",
      "2656/2656 [==============================] - 68s 26ms/step - loss: 1.5388 - accuracy: 0.4962\n",
      "Epoch 10/20\n",
      "2656/2656 [==============================] - 62s 23ms/step - loss: 1.5187 - accuracy: 0.5200\n",
      "Epoch 11/20\n",
      "2656/2656 [==============================] - 63s 24ms/step - loss: 1.5061 - accuracy: 0.5305\n",
      "Epoch 12/20\n",
      "2656/2656 [==============================] - 67s 25ms/step - loss: 1.4943 - accuracy: 0.5384\n",
      "Epoch 13/20\n",
      "2656/2656 [==============================] - 62s 23ms/step - loss: 1.4760 - accuracy: 0.5380\n",
      "Epoch 14/20\n",
      "2656/2656 [==============================] - 66s 25ms/step - loss: 1.4674 - accuracy: 0.5444\n",
      "Epoch 15/20\n",
      "2656/2656 [==============================] - 65s 25ms/step - loss: 1.4510 - accuracy: 0.5456\n",
      "Epoch 16/20\n",
      "2656/2656 [==============================] - 64s 24ms/step - loss: 1.4400 - accuracy: 0.5456\n",
      "Epoch 17/20\n",
      "2656/2656 [==============================] - 58s 22ms/step - loss: 1.4130 - accuracy: 0.5535\n",
      "Epoch 18/20\n",
      "2656/2656 [==============================] - 64s 24ms/step - loss: 1.3953 - accuracy: 0.5595\n",
      "Epoch 19/20\n",
      "2656/2656 [==============================] - 66s 25ms/step - loss: 1.3961 - accuracy: 0.5663\n",
      "Epoch 20/20\n",
      "2656/2656 [==============================] - 59s 22ms/step - loss: 1.3770 - accuracy: 0.5629\n",
      "2656/2656 [==============================] - 63s 24ms/step\n",
      "> 56.965\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=20, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dd06c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2656/2656 [==============================] - 71s 27ms/step - loss: 7.6138 - accuracy: 0.2937\n",
      "Epoch 2/10\n",
      "2656/2656 [==============================] - 64s 24ms/step - loss: 2.6838 - accuracy: 0.3197\n",
      "Epoch 3/10\n",
      "2656/2656 [==============================] - 67s 25ms/step - loss: 2.4491 - accuracy: 0.3140\n",
      "Epoch 4/10\n",
      "2656/2656 [==============================] - 81s 31ms/step - loss: 2.1998 - accuracy: 0.3253\n",
      "Epoch 5/10\n",
      "2656/2656 [==============================] - 61s 23ms/step - loss: 2.0360 - accuracy: 0.3400\n",
      "Epoch 6/10\n",
      "2656/2656 [==============================] - 56s 21ms/step - loss: 1.8497 - accuracy: 0.4443\n",
      "Epoch 7/10\n",
      "2656/2656 [==============================] - 55s 21ms/step - loss: 1.9860 - accuracy: 0.4168\n",
      "Epoch 8/10\n",
      "2656/2656 [==============================] - 56s 21ms/step - loss: 1.8791 - accuracy: 0.4398\n",
      "Epoch 9/10\n",
      "2656/2656 [==============================] - 66s 25ms/step - loss: 1.6569 - accuracy: 0.4669\n",
      "Epoch 10/10\n",
      "2656/2656 [==============================] - 80s 30ms/step - loss: 1.5957 - accuracy: 0.4733\n",
      "2656/2656 [==============================] - 91s 34ms/step\n",
      "> 47.703\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning with 10 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=10, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "96a54581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 8.2120 - accuracy: 0.2786\n",
      "Epoch 2/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 2.7354 - accuracy: 0.3419\n",
      "Epoch 3/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 2.0737 - accuracy: 0.4262\n",
      "Epoch 4/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.7364 - accuracy: 0.4699\n",
      "Epoch 5/30\n",
      "2656/2656 [==============================] - 46s 17ms/step - loss: 1.6445 - accuracy: 0.4808\n",
      "Epoch 6/30\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 1.6232 - accuracy: 0.4846\n",
      "Epoch 7/30\n",
      "2656/2656 [==============================] - 49s 18ms/step - loss: 1.5865 - accuracy: 0.4883\n",
      "Epoch 8/30\n",
      "2656/2656 [==============================] - 49s 18ms/step - loss: 1.5778 - accuracy: 0.4913\n",
      "Epoch 9/30\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 1.5675 - accuracy: 0.4925\n",
      "Epoch 10/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.5479 - accuracy: 0.4921\n",
      "Epoch 11/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.5359 - accuracy: 0.4936\n",
      "Epoch 12/30\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 1.5339 - accuracy: 0.4974\n",
      "Epoch 13/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.5146 - accuracy: 0.5132\n",
      "Epoch 14/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.4940 - accuracy: 0.5331\n",
      "Epoch 15/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.4955 - accuracy: 0.5312\n",
      "Epoch 16/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.4810 - accuracy: 0.5335\n",
      "Epoch 17/30\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 1.4763 - accuracy: 0.5373\n",
      "Epoch 18/30\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 1.4757 - accuracy: 0.5328\n",
      "Epoch 19/30\n",
      "2656/2656 [==============================] - 50s 19ms/step - loss: 1.4802 - accuracy: 0.5335\n",
      "Epoch 20/30\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 1.4667 - accuracy: 0.5361\n",
      "Epoch 21/30\n",
      "2656/2656 [==============================] - 50s 19ms/step - loss: 1.4685 - accuracy: 0.5335\n",
      "Epoch 22/30\n",
      "2656/2656 [==============================] - 50s 19ms/step - loss: 1.4519 - accuracy: 0.5407\n",
      "Epoch 23/30\n",
      "2656/2656 [==============================] - 49s 19ms/step - loss: 1.4472 - accuracy: 0.5380\n",
      "Epoch 24/30\n",
      "2656/2656 [==============================] - 49s 18ms/step - loss: 1.4470 - accuracy: 0.5407\n",
      "Epoch 25/30\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 1.4472 - accuracy: 0.5380\n",
      "Epoch 26/30\n",
      "2656/2656 [==============================] - 50s 19ms/step - loss: 1.4341 - accuracy: 0.5437\n",
      "Epoch 27/30\n",
      "2656/2656 [==============================] - 49s 18ms/step - loss: 1.4254 - accuracy: 0.5422\n",
      "Epoch 28/30\n",
      "2656/2656 [==============================] - 50s 19ms/step - loss: 1.4190 - accuracy: 0.5437\n",
      "Epoch 29/30\n",
      "2656/2656 [==============================] - 51s 19ms/step - loss: 1.4115 - accuracy: 0.5448\n",
      "Epoch 30/30\n",
      "2656/2656 [==============================] - 50s 19ms/step - loss: 1.4120 - accuracy: 0.5399\n",
      "2656/2656 [==============================] - 53s 20ms/step\n",
      "> 53.313\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning with 30 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=30, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ef7e0f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 5.7398 - accuracy: 0.2764\n",
      "Epoch 2/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 2.5588 - accuracy: 0.3343\n",
      "Epoch 3/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 2.5571 - accuracy: 0.3475\n",
      "Epoch 4/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 2.4733 - accuracy: 0.3870\n",
      "Epoch 5/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.5098 - accuracy: 0.3825\n",
      "Epoch 6/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.3643 - accuracy: 0.4010\n",
      "Epoch 7/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.3910 - accuracy: 0.4062\n",
      "Epoch 8/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.1884 - accuracy: 0.4631\n",
      "Epoch 9/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.1493 - accuracy: 0.4639\n",
      "Epoch 10/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.0899 - accuracy: 0.4740\n",
      "Epoch 11/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.0774 - accuracy: 0.4759\n",
      "Epoch 12/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 2.0505 - accuracy: 0.4827\n",
      "Epoch 13/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 2.0230 - accuracy: 0.4868\n",
      "Epoch 14/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 2.0035 - accuracy: 0.4895\n",
      "Epoch 15/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9987 - accuracy: 0.4895\n",
      "Epoch 16/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9914 - accuracy: 0.4891\n",
      "Epoch 17/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9783 - accuracy: 0.4913\n",
      "Epoch 18/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9758 - accuracy: 0.4906\n",
      "Epoch 19/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9705 - accuracy: 0.4913\n",
      "Epoch 20/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9643 - accuracy: 0.4913\n",
      "Epoch 21/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9635 - accuracy: 0.4902\n",
      "Epoch 22/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9602 - accuracy: 0.4917\n",
      "Epoch 23/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9567 - accuracy: 0.4921\n",
      "Epoch 24/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9554 - accuracy: 0.4913\n",
      "Epoch 25/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9531 - accuracy: 0.4917\n",
      "Epoch 26/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9525 - accuracy: 0.4906\n",
      "Epoch 27/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9498 - accuracy: 0.4913\n",
      "Epoch 28/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9485 - accuracy: 0.4891\n",
      "Epoch 29/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9395 - accuracy: 0.4910\n",
      "Epoch 30/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.9259 - accuracy: 0.4853\n",
      "Epoch 31/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.9301 - accuracy: 0.4849\n",
      "Epoch 32/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9061 - accuracy: 0.4785\n",
      "Epoch 33/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9176 - accuracy: 0.4608\n",
      "Epoch 34/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9438 - accuracy: 0.4872\n",
      "Epoch 35/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9172 - accuracy: 0.4725\n",
      "Epoch 36/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.9347 - accuracy: 0.4910\n",
      "Epoch 37/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9303 - accuracy: 0.4921\n",
      "Epoch 38/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9227 - accuracy: 0.4816\n",
      "Epoch 39/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9292 - accuracy: 0.4940\n",
      "Epoch 40/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9242 - accuracy: 0.4951\n",
      "Epoch 41/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9219 - accuracy: 0.4944\n",
      "Epoch 42/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9193 - accuracy: 0.4959\n",
      "Epoch 43/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9172 - accuracy: 0.4955\n",
      "Epoch 44/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9169 - accuracy: 0.4959\n",
      "Epoch 45/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9159 - accuracy: 0.4959\n",
      "Epoch 46/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9162 - accuracy: 0.4951\n",
      "Epoch 47/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9149 - accuracy: 0.4959\n",
      "Epoch 48/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9139 - accuracy: 0.4959\n",
      "Epoch 49/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9138 - accuracy: 0.4951\n",
      "Epoch 50/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.8935 - accuracy: 0.4936\n",
      "Epoch 51/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.8707 - accuracy: 0.4770\n",
      "Epoch 52/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9135 - accuracy: 0.4928\n",
      "Epoch 53/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.8963 - accuracy: 0.4672\n",
      "Epoch 54/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.9161 - accuracy: 0.4951\n",
      "Epoch 55/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.9122 - accuracy: 0.4959\n",
      "Epoch 56/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9127 - accuracy: 0.4959\n",
      "Epoch 57/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9091 - accuracy: 0.4962\n",
      "Epoch 58/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.9095 - accuracy: 0.4959\n",
      "Epoch 59/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9096 - accuracy: 0.4959\n",
      "Epoch 60/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.9056 - accuracy: 0.4959\n",
      "Epoch 61/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.9048 - accuracy: 0.4959\n",
      "Epoch 62/200\n",
      " 992/2656 [==========>...................] - ETA: 16s - loss: 1.8896 - accuracy: 0.4970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a2ae11e25197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# entry point, run the test harness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-a2ae11e25197>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m123.68\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.779\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m103.939\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 200 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=200, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74851d87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 6.4951 - accuracy: 0.3780\n",
      "Epoch 2/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 1.6871 - accuracy: 0.4955\n",
      "Epoch 3/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.5830 - accuracy: 0.5312\n",
      "Epoch 4/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.4884 - accuracy: 0.5505\n",
      "Epoch 5/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.4758 - accuracy: 0.5471\n",
      "Epoch 6/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.4406 - accuracy: 0.5606\n",
      "Epoch 7/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.4054 - accuracy: 0.5644\n",
      "Epoch 8/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 1.3875 - accuracy: 0.5685\n",
      "Epoch 9/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3669 - accuracy: 0.5730\n",
      "Epoch 10/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3485 - accuracy: 0.5817\n",
      "Epoch 11/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3386 - accuracy: 0.5870\n",
      "Epoch 12/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.3105 - accuracy: 0.5983\n",
      "Epoch 13/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.2838 - accuracy: 0.6013\n",
      "Epoch 14/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.2372 - accuracy: 0.6111\n",
      "Epoch 15/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.2314 - accuracy: 0.6145\n",
      "Epoch 16/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.2100 - accuracy: 0.6209\n",
      "Epoch 17/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.1925 - accuracy: 0.6254\n",
      "Epoch 18/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.1985 - accuracy: 0.6246\n",
      "Epoch 19/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 1.1426 - accuracy: 0.6299\n",
      "Epoch 20/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 1.1159 - accuracy: 0.6382\n",
      "Epoch 21/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.0916 - accuracy: 0.6434\n",
      "Epoch 22/200\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 1.0733 - accuracy: 0.6404\n",
      "Epoch 23/200\n",
      "2656/2656 [==============================] - 55s 21ms/step - loss: 1.0728 - accuracy: 0.6544\n",
      "Epoch 24/200\n",
      "2656/2656 [==============================] - 51s 19ms/step - loss: 1.0143 - accuracy: 0.6570\n",
      "Epoch 25/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.0279 - accuracy: 0.6536\n",
      "Epoch 26/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 1.0024 - accuracy: 0.6691\n",
      "Epoch 27/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.9771 - accuracy: 0.6724\n",
      "Epoch 28/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.9546 - accuracy: 0.6683\n",
      "Epoch 29/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9550 - accuracy: 0.6781\n",
      "Epoch 30/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9556 - accuracy: 0.6770\n",
      "Epoch 31/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9397 - accuracy: 0.6830\n",
      "Epoch 32/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.9340 - accuracy: 0.6796\n",
      "Epoch 33/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.9010 - accuracy: 0.6894\n",
      "Epoch 34/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.8819 - accuracy: 0.6988\n",
      "Epoch 35/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.8906 - accuracy: 0.6958\n",
      "Epoch 36/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8960 - accuracy: 0.6958\n",
      "Epoch 37/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.9064 - accuracy: 0.6958\n",
      "Epoch 38/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.8955 - accuracy: 0.6992\n",
      "Epoch 39/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.9275 - accuracy: 0.7003\n",
      "Epoch 40/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.8918 - accuracy: 0.7033\n",
      "Epoch 41/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8490 - accuracy: 0.7154\n",
      "Epoch 42/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8277 - accuracy: 0.7139\n",
      "Epoch 43/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8539 - accuracy: 0.7067\n",
      "Epoch 44/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.8575 - accuracy: 0.7037\n",
      "Epoch 45/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.8755 - accuracy: 0.6999\n",
      "Epoch 46/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.9062 - accuracy: 0.7059\n",
      "Epoch 47/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.9091 - accuracy: 0.7048\n",
      "Epoch 48/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.8651 - accuracy: 0.7105\n",
      "Epoch 49/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.7993 - accuracy: 0.7267\n",
      "Epoch 50/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7741 - accuracy: 0.7334\n",
      "Epoch 51/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7649 - accuracy: 0.7323\n",
      "Epoch 52/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.8014 - accuracy: 0.7248\n",
      "Epoch 53/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7637 - accuracy: 0.7304\n",
      "Epoch 54/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7945 - accuracy: 0.7293\n",
      "Epoch 55/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.8190 - accuracy: 0.7312\n",
      "Epoch 56/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7784 - accuracy: 0.7353\n",
      "Epoch 57/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7722 - accuracy: 0.7312\n",
      "Epoch 58/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.7696 - accuracy: 0.7368\n",
      "Epoch 59/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.7872 - accuracy: 0.7316\n",
      "Epoch 60/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.7492 - accuracy: 0.7428\n",
      "Epoch 61/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.7572 - accuracy: 0.7425\n",
      "Epoch 62/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.7531 - accuracy: 0.7361\n",
      "Epoch 63/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7413 - accuracy: 0.7361\n",
      "Epoch 64/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7384 - accuracy: 0.7447\n",
      "Epoch 65/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7830 - accuracy: 0.7331\n",
      "Epoch 66/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7763 - accuracy: 0.7357\n",
      "Epoch 67/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7523 - accuracy: 0.7477\n",
      "Epoch 68/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7493 - accuracy: 0.7372\n",
      "Epoch 69/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.7330 - accuracy: 0.7451\n",
      "Epoch 70/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.7800 - accuracy: 0.7526\n",
      "Epoch 71/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.7524 - accuracy: 0.7526\n",
      "Epoch 72/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7671 - accuracy: 0.7511\n",
      "Epoch 73/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7241 - accuracy: 0.7568\n",
      "Epoch 74/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7151 - accuracy: 0.7594\n",
      "Epoch 75/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7982 - accuracy: 0.7451\n",
      "Epoch 76/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7398 - accuracy: 0.7534\n",
      "Epoch 77/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7115 - accuracy: 0.7545\n",
      "Epoch 78/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7069 - accuracy: 0.7636\n",
      "Epoch 79/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7466 - accuracy: 0.7556\n",
      "Epoch 80/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6923 - accuracy: 0.7677\n",
      "Epoch 81/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7183 - accuracy: 0.7598\n",
      "Epoch 82/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7088 - accuracy: 0.7666\n",
      "Epoch 83/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.7182 - accuracy: 0.7639\n",
      "Epoch 84/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6968 - accuracy: 0.7688\n",
      "Epoch 85/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6799 - accuracy: 0.7700\n",
      "Epoch 86/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6945 - accuracy: 0.7654\n",
      "Epoch 87/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7037 - accuracy: 0.7696\n",
      "Epoch 88/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7064 - accuracy: 0.7684\n",
      "Epoch 89/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6929 - accuracy: 0.7700\n",
      "Epoch 90/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6742 - accuracy: 0.7782\n",
      "Epoch 91/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6490 - accuracy: 0.7779\n",
      "Epoch 92/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.7035 - accuracy: 0.7651\n",
      "Epoch 93/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.7106 - accuracy: 0.7737\n",
      "Epoch 94/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6995 - accuracy: 0.7779\n",
      "Epoch 95/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6810 - accuracy: 0.7733\n",
      "Epoch 96/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6525 - accuracy: 0.7820\n",
      "Epoch 97/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6758 - accuracy: 0.7745\n",
      "Epoch 98/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.7093 - accuracy: 0.7730\n",
      "Epoch 99/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.6816 - accuracy: 0.7741\n",
      "Epoch 100/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.6614 - accuracy: 0.7805\n",
      "Epoch 101/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.6879 - accuracy: 0.7805\n",
      "Epoch 102/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.6893 - accuracy: 0.7858\n",
      "Epoch 103/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.6870 - accuracy: 0.7775\n",
      "Epoch 104/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.6569 - accuracy: 0.7910\n",
      "Epoch 105/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.6244 - accuracy: 0.7978\n",
      "Epoch 106/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6126 - accuracy: 0.7997\n",
      "Epoch 107/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6306 - accuracy: 0.7907\n",
      "Epoch 108/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6452 - accuracy: 0.7941\n",
      "Epoch 109/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6295 - accuracy: 0.7941\n",
      "Epoch 110/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6684 - accuracy: 0.7884\n",
      "Epoch 111/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6448 - accuracy: 0.7959\n",
      "Epoch 112/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.6284 - accuracy: 0.7922\n",
      "Epoch 113/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.6244 - accuracy: 0.7997\n",
      "Epoch 114/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6835 - accuracy: 0.7869\n",
      "Epoch 115/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6261 - accuracy: 0.7967\n",
      "Epoch 116/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6779 - accuracy: 0.7959\n",
      "Epoch 117/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6356 - accuracy: 0.7956\n",
      "Epoch 118/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6345 - accuracy: 0.8020\n",
      "Epoch 119/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6123 - accuracy: 0.8023\n",
      "Epoch 120/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6142 - accuracy: 0.7986\n",
      "Epoch 121/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6157 - accuracy: 0.8038\n",
      "Epoch 122/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6184 - accuracy: 0.7967\n",
      "Epoch 123/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6178 - accuracy: 0.7986\n",
      "Epoch 124/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6547 - accuracy: 0.8012\n",
      "Epoch 125/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6631 - accuracy: 0.7933\n",
      "Epoch 126/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6300 - accuracy: 0.7986\n",
      "Epoch 127/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6070 - accuracy: 0.8065\n",
      "Epoch 128/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5725 - accuracy: 0.8125\n",
      "Epoch 129/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5754 - accuracy: 0.8072\n",
      "Epoch 130/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.5843 - accuracy: 0.8035\n",
      "Epoch 131/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5656 - accuracy: 0.8102\n",
      "Epoch 132/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5730 - accuracy: 0.8106\n",
      "Epoch 133/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6875 - accuracy: 0.7824\n",
      "Epoch 134/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6316 - accuracy: 0.8023\n",
      "Epoch 135/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5767 - accuracy: 0.8095\n",
      "Epoch 136/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5932 - accuracy: 0.8117\n",
      "Epoch 137/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6023 - accuracy: 0.8061\n",
      "Epoch 138/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5997 - accuracy: 0.8087\n",
      "Epoch 139/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5861 - accuracy: 0.8110\n",
      "Epoch 140/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5975 - accuracy: 0.8076\n",
      "Epoch 141/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5857 - accuracy: 0.8106\n",
      "Epoch 142/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6736 - accuracy: 0.7971\n",
      "Epoch 143/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6436 - accuracy: 0.8046\n",
      "Epoch 144/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5763 - accuracy: 0.8099\n",
      "Epoch 145/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5757 - accuracy: 0.8133\n",
      "Epoch 146/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5568 - accuracy: 0.8189\n",
      "Epoch 147/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5817 - accuracy: 0.8106\n",
      "Epoch 148/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6046 - accuracy: 0.8084\n",
      "Epoch 149/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6321 - accuracy: 0.8065\n",
      "Epoch 150/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6345 - accuracy: 0.8087\n",
      "Epoch 151/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.6141 - accuracy: 0.8106\n",
      "Epoch 152/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5866 - accuracy: 0.8174\n",
      "Epoch 153/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5926 - accuracy: 0.8129\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5954 - accuracy: 0.8129\n",
      "Epoch 155/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5544 - accuracy: 0.8253\n",
      "Epoch 156/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5683 - accuracy: 0.8181\n",
      "Epoch 157/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5510 - accuracy: 0.8223\n",
      "Epoch 158/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5598 - accuracy: 0.8181\n",
      "Epoch 159/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5377 - accuracy: 0.8208\n",
      "Epoch 160/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5678 - accuracy: 0.8208\n",
      "Epoch 161/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5527 - accuracy: 0.8223\n",
      "Epoch 162/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5725 - accuracy: 0.8208\n",
      "Epoch 163/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.5441 - accuracy: 0.8287\n",
      "Epoch 164/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.5451 - accuracy: 0.8276\n",
      "Epoch 165/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.5323 - accuracy: 0.8264\n",
      "Epoch 166/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.5273 - accuracy: 0.8279\n",
      "Epoch 167/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.5527 - accuracy: 0.8242\n",
      "Epoch 168/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.5386 - accuracy: 0.8238\n",
      "Epoch 169/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5692 - accuracy: 0.8257\n",
      "Epoch 170/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5234 - accuracy: 0.8279\n",
      "Epoch 171/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.5169 - accuracy: 0.8287\n",
      "Epoch 172/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5207 - accuracy: 0.8272\n",
      "Epoch 173/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5099 - accuracy: 0.8317\n",
      "Epoch 174/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5120 - accuracy: 0.8298\n",
      "Epoch 175/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5635 - accuracy: 0.8234\n",
      "Epoch 176/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.5413 - accuracy: 0.8272\n",
      "Epoch 177/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5406 - accuracy: 0.8294\n",
      "Epoch 178/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.5463 - accuracy: 0.8283\n",
      "Epoch 179/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5884 - accuracy: 0.8230\n",
      "Epoch 180/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.5500 - accuracy: 0.8227\n",
      "Epoch 181/200\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.5661 - accuracy: 0.8230\n",
      "Epoch 182/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5572 - accuracy: 0.8212\n",
      "Epoch 183/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5585 - accuracy: 0.8257\n",
      "Epoch 184/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5749 - accuracy: 0.8230\n",
      "Epoch 185/200\n",
      "2656/2656 [==============================] - 26s 10ms/step - loss: 0.6257 - accuracy: 0.8181\n",
      "Epoch 186/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5834 - accuracy: 0.8197\n",
      "Epoch 187/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5719 - accuracy: 0.8257\n",
      "Epoch 188/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5580 - accuracy: 0.8245\n",
      "Epoch 189/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5794 - accuracy: 0.8219\n",
      "Epoch 190/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5437 - accuracy: 0.8268\n",
      "Epoch 191/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5487 - accuracy: 0.8287\n",
      "Epoch 192/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5473 - accuracy: 0.8257\n",
      "Epoch 193/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5739 - accuracy: 0.8302\n",
      "Epoch 194/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5785 - accuracy: 0.8268\n",
      "Epoch 195/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5693 - accuracy: 0.8249\n",
      "Epoch 196/200\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.5498 - accuracy: 0.8257\n",
      "Epoch 197/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.5280 - accuracy: 0.8268\n",
      "Epoch 198/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.5486 - accuracy: 0.8264\n",
      "Epoch 199/200\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.5929 - accuracy: 0.8261\n",
      "Epoch 200/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.5754 - accuracy: 0.8291\n",
      "2656/2656 [==============================] - 28s 10ms/step\n",
      "> 82.267\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 200 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(256, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=200, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "982b54a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-9a6b8c84b4ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'256 layer 200 blocks.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('256 layer 200 epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1be9391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 6.6768 - accuracy: 0.3791\n",
      "Epoch 2/200\n",
      " 192/2656 [=>............................] - ETA: 25s - loss: 1.6866 - accuracy: 0.4844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9b1ab13e9524>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# entry point, run the test harness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-9b1ab13e9524>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m123.68\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.779\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m103.939\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 200 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=200, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('512 layer 200 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8fa332b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-76783cbea339>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'512 layer 200 epoch.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('512 layer 200 epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7de5c42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 9.7741 - accuracy: 0.3897\n",
      "Epoch 2/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.5747 - accuracy: 0.5335\n",
      "Epoch 3/100\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 1.4160 - accuracy: 0.5723\n",
      "Epoch 4/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.2435 - accuracy: 0.6032\n",
      "Epoch 5/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.1229 - accuracy: 0.6359\n",
      "Epoch 6/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.9971 - accuracy: 0.6713\n",
      "Epoch 7/100\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.8995 - accuracy: 0.7037\n",
      "Epoch 8/100\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 0.8401 - accuracy: 0.7282\n",
      "Epoch 9/100\n",
      "2656/2656 [==============================] - 44s 17ms/step - loss: 0.7345 - accuracy: 0.7541\n",
      "Epoch 10/100\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.6717 - accuracy: 0.7782\n",
      "Epoch 11/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.5870 - accuracy: 0.7978\n",
      "Epoch 12/100\n",
      "2656/2656 [==============================] - 45s 17ms/step - loss: 0.5142 - accuracy: 0.8234\n",
      "Epoch 13/100\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4827 - accuracy: 0.8325\n",
      "Epoch 14/100\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.5147 - accuracy: 0.8189\n",
      "Epoch 15/100\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.5640 - accuracy: 0.8136\n",
      "Epoch 16/100\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.4555 - accuracy: 0.8475\n",
      "Epoch 17/100\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.3563 - accuracy: 0.8705\n",
      "Epoch 18/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3446 - accuracy: 0.8833\n",
      "Epoch 19/100\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2451 - accuracy: 0.9175\n",
      "Epoch 20/100\n",
      "2656/2656 [==============================] - 41s 16ms/step - loss: 0.2373 - accuracy: 0.9160\n",
      "Epoch 21/100\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.2120 - accuracy: 0.9247\n",
      "Epoch 22/100\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.1950 - accuracy: 0.9352\n",
      "Epoch 23/100\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.1856 - accuracy: 0.9345\n",
      "Epoch 24/100\n",
      "2656/2656 [==============================] - 49s 18ms/step - loss: 0.1756 - accuracy: 0.9398\n",
      "Epoch 25/100\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 0.1441 - accuracy: 0.9526\n",
      "Epoch 26/100\n",
      "2656/2656 [==============================] - 51s 19ms/step - loss: 0.1286 - accuracy: 0.9559\n",
      "Epoch 27/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.1475 - accuracy: 0.9522\n",
      "Epoch 28/100\n",
      "2656/2656 [==============================] - 45s 17ms/step - loss: 0.1273 - accuracy: 0.9575\n",
      "Epoch 29/100\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 0.1254 - accuracy: 0.9548\n",
      "Epoch 30/100\n",
      "2656/2656 [==============================] - 47s 18ms/step - loss: 0.1528 - accuracy: 0.9548\n",
      "Epoch 31/100\n",
      "2656/2656 [==============================] - 56s 21ms/step - loss: 0.0904 - accuracy: 0.9646\n",
      "Epoch 32/100\n",
      "2656/2656 [==============================] - 54s 20ms/step - loss: 0.0713 - accuracy: 0.9748\n",
      "Epoch 33/100\n",
      "2656/2656 [==============================] - 46s 17ms/step - loss: 0.0759 - accuracy: 0.9755\n",
      "Epoch 34/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0898 - accuracy: 0.9733\n",
      "Epoch 35/100\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.0633 - accuracy: 0.9797\n",
      "Epoch 36/100\n",
      "2656/2656 [==============================] - 41s 16ms/step - loss: 0.0587 - accuracy: 0.9800\n",
      "Epoch 37/100\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 0.0579 - accuracy: 0.9804\n",
      "Epoch 38/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0558 - accuracy: 0.9800\n",
      "Epoch 39/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0321 - accuracy: 0.9891\n",
      "Epoch 40/100\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 0.0216 - accuracy: 0.9917\n",
      "Epoch 41/100\n",
      "2656/2656 [==============================] - 44s 16ms/step - loss: 0.0192 - accuracy: 0.9928\n",
      "Epoch 42/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0200 - accuracy: 0.9928\n",
      "Epoch 43/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0209 - accuracy: 0.9928\n",
      "Epoch 44/100\n",
      "2656/2656 [==============================] - 43s 16ms/step - loss: 0.0193 - accuracy: 0.9936\n",
      "Epoch 45/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0329 - accuracy: 0.9898\n",
      "Epoch 46/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0389 - accuracy: 0.9864\n",
      "Epoch 47/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0376 - accuracy: 0.9906\n",
      "Epoch 48/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0385 - accuracy: 0.9864\n",
      "Epoch 49/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.1073 - accuracy: 0.9725\n",
      "Epoch 50/100\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2009 - accuracy: 0.9526\n",
      "Epoch 51/100\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.1473 - accuracy: 0.9605\n",
      "Epoch 52/100\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.1225 - accuracy: 0.9680\n",
      "Epoch 53/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.1683 - accuracy: 0.9582\n",
      "Epoch 54/100\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.1594 - accuracy: 0.9586\n",
      "Epoch 55/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.1138 - accuracy: 0.9729\n",
      "Epoch 56/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0657 - accuracy: 0.9827\n",
      "Epoch 57/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0623 - accuracy: 0.9842\n",
      "Epoch 58/100\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0687 - accuracy: 0.9827\n",
      "Epoch 59/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0464 - accuracy: 0.9872\n",
      "Epoch 60/100\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.0440 - accuracy: 0.9868\n",
      "Epoch 61/100\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 0.0305 - accuracy: 0.9895\n",
      "Epoch 62/100\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0330 - accuracy: 0.9906\n",
      "Epoch 63/100\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.0246 - accuracy: 0.9921\n",
      "Epoch 64/100\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.1061 - accuracy: 0.9785\n",
      "Epoch 65/100\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.2709 - accuracy: 0.9394\n",
      "Epoch 66/100\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.1744 - accuracy: 0.9608\n",
      "Epoch 67/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1435 - accuracy: 0.9657\n",
      "Epoch 68/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.1066 - accuracy: 0.9778\n",
      "Epoch 69/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0755 - accuracy: 0.9812\n",
      "Epoch 70/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1176 - accuracy: 0.9752\n",
      "Epoch 71/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0593 - accuracy: 0.9853\n",
      "Epoch 72/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.1173 - accuracy: 0.9767\n",
      "Epoch 73/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1648 - accuracy: 0.9657\n",
      "Epoch 74/100\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.1418 - accuracy: 0.9736\n",
      "Epoch 75/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0845 - accuracy: 0.9785\n",
      "Epoch 76/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0730 - accuracy: 0.9853\n",
      "Epoch 77/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0908 - accuracy: 0.9819\n",
      "Epoch 78/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1573 - accuracy: 0.9744\n",
      "Epoch 79/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.1227 - accuracy: 0.9800\n",
      "Epoch 80/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2651 - accuracy: 0.9575\n",
      "Epoch 81/100\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3030 - accuracy: 0.9608\n",
      "Epoch 82/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.3811 - accuracy: 0.9462\n",
      "Epoch 83/100\n",
      "2656/2656 [==============================] - 28s 11ms/step - loss: 0.2185 - accuracy: 0.9654\n",
      "Epoch 84/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2453 - accuracy: 0.9578\n",
      "Epoch 85/100\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.3527 - accuracy: 0.9575\n",
      "Epoch 86/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.3198 - accuracy: 0.9571\n",
      "Epoch 87/100\n",
      "2656/2656 [==============================] - 28s 10ms/step - loss: 0.2249 - accuracy: 0.9646\n",
      "Epoch 88/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2268 - accuracy: 0.9691\n",
      "Epoch 89/100\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.3696 - accuracy: 0.9575\n",
      "Epoch 90/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3512 - accuracy: 0.9518\n",
      "Epoch 91/100\n",
      "2656/2656 [==============================] - 29s 11ms/step - loss: 0.3110 - accuracy: 0.9567\n",
      "Epoch 92/100\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.3550 - accuracy: 0.9556\n",
      "Epoch 93/100\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2055 - accuracy: 0.9714\n",
      "Epoch 94/100\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0939 - accuracy: 0.9846\n",
      "Epoch 95/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0941 - accuracy: 0.9857\n",
      "Epoch 96/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1355 - accuracy: 0.9789\n",
      "Epoch 97/100\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.1141 - accuracy: 0.9827\n",
      "Epoch 98/100\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2274 - accuracy: 0.9782\n",
      "Epoch 99/100\n",
      "2656/2656 [==============================] - 27s 10ms/step - loss: 0.3087 - accuracy: 0.9642\n",
      "Epoch 100/100\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4366 - accuracy: 0.9590\n",
      "2656/2656 [==============================] - 28s 11ms/step\n",
      "> 94.767\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 100 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=100, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('512 layer 100 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e01f94b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 9.3687 - accuracy: 0.3795\n",
      "Epoch 2/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.6113 - accuracy: 0.5184\n",
      "Epoch 3/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.4630 - accuracy: 0.5553\n",
      "Epoch 4/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.3522 - accuracy: 0.5862\n",
      "Epoch 5/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.2527 - accuracy: 0.6020\n",
      "Epoch 6/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 1.1718 - accuracy: 0.6337\n",
      "Epoch 7/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.1127 - accuracy: 0.6562\n",
      "Epoch 8/50\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 1.0337 - accuracy: 0.6788\n",
      "Epoch 9/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.9339 - accuracy: 0.7007\n",
      "Epoch 10/50\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.8704 - accuracy: 0.7263\n",
      "Epoch 11/50\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.7592 - accuracy: 0.7477\n",
      "Epoch 12/50\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.6785 - accuracy: 0.7722\n",
      "Epoch 13/50\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.5991 - accuracy: 0.8008\n",
      "Epoch 14/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.5372 - accuracy: 0.8181\n",
      "Epoch 15/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.4736 - accuracy: 0.8389\n",
      "Epoch 16/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.4854 - accuracy: 0.8325\n",
      "Epoch 17/50\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.4250 - accuracy: 0.8517\n",
      "Epoch 18/50\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.3871 - accuracy: 0.8686\n",
      "Epoch 19/50\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3556 - accuracy: 0.8788\n",
      "Epoch 20/50\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2986 - accuracy: 0.8980\n",
      "Epoch 21/50\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2703 - accuracy: 0.9089\n",
      "Epoch 22/50\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2542 - accuracy: 0.9093\n",
      "Epoch 23/50\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.2428 - accuracy: 0.9209\n",
      "Epoch 24/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.2276 - accuracy: 0.9224\n",
      "Epoch 25/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1998 - accuracy: 0.9337\n",
      "Epoch 26/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1647 - accuracy: 0.9435\n",
      "Epoch 27/50\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.1572 - accuracy: 0.9469\n",
      "Epoch 28/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1326 - accuracy: 0.9559\n",
      "Epoch 29/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.1150 - accuracy: 0.9605\n",
      "Epoch 30/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1034 - accuracy: 0.9661\n",
      "Epoch 31/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1152 - accuracy: 0.9620\n",
      "Epoch 32/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.1409 - accuracy: 0.9556\n",
      "Epoch 33/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1155 - accuracy: 0.9608\n",
      "Epoch 34/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1261 - accuracy: 0.9646\n",
      "Epoch 35/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1074 - accuracy: 0.9635\n",
      "Epoch 36/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0845 - accuracy: 0.9714\n",
      "Epoch 37/50\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0854 - accuracy: 0.9725\n",
      "Epoch 38/50\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0590 - accuracy: 0.9812\n",
      "Epoch 39/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0796 - accuracy: 0.9759\n",
      "Epoch 40/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0598 - accuracy: 0.9793\n",
      "Epoch 41/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0504 - accuracy: 0.9831\n",
      "Epoch 42/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0339 - accuracy: 0.9887\n",
      "Epoch 43/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0255 - accuracy: 0.9913\n",
      "Epoch 44/50\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0271 - accuracy: 0.9928\n",
      "Epoch 45/50\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0248 - accuracy: 0.9921\n",
      "Epoch 46/50\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0201 - accuracy: 0.9932\n",
      "Epoch 47/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0174 - accuracy: 0.9936\n",
      "Epoch 48/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0173 - accuracy: 0.9940\n",
      "Epoch 49/50\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0208 - accuracy: 0.9936\n",
      "Epoch 50/50\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0209 - accuracy: 0.9936\n",
      "2656/2656 [==============================] - 35s 13ms/step\n",
      "> 99.435\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 50 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=50, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train, y_train2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('512 layer 50 epoch.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3255c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5626544",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf69470c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imsave' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-aa701b552afd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#from imageio import imsave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sample_image{0}.jpg\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'imsave' is not defined"
     ]
    }
   ],
   "source": [
    "images2 = np.load('pannuke\\\\Fold_2\\\\images\\\\images.npy')\n",
    "#import imageio\n",
    "from imageio import imsave\n",
    "for i in range(3):\n",
    "    imsave(\"sample_image{0}.jpg\".format(i), images2[i,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bc4c29ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [31.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [33.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [27.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "#import imageio\n",
    "from imageio import imsave\n",
    "for i in range(3):\n",
    "    imsave(\"sample_image{0}.jpg\".format(i), images2[i,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b831e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "types2 = np.load('pannuke\\\\Fold_2\\\\images\\\\types.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1300b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Breast' 'Breast' 'Breast' ... 'Colon' 'Colon' 'Colon']\n"
     ]
    }
   ],
   "source": [
    "print(types2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc8fdf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.0074018e-06 2.7882162e-04 6.8754198e-06 1.0007271e-02 4.8153066e-05\n",
      " 9.1148877e-01 5.0690930e-02 1.2234124e-02 4.3145178e-06 4.7352050e-06\n",
      " 2.6567755e-05 1.7423400e-06 8.7724489e-05 1.5102208e-02 6.1018717e-08\n",
      " 8.1874059e-06 1.8225512e-07 3.6764600e-08 3.6961418e-07]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for a new image.\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "\t# load the image\n",
    "\timg = load_img(filename, target_size=(256, 256))\n",
    "\t# convert to array\n",
    "\timg = img_to_array(img)\n",
    "\t# reshape into a single sample with 3 channels\n",
    "\timg = img.reshape(1, 256, 256, 3)\n",
    "\t# center pixel data\n",
    "\timg = img.astype('float32')\n",
    "\timg = img - [123.68, 116.779, 103.939]\n",
    "\treturn img\n",
    "\n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "\t# load the image\n",
    "\timg = load_image('sample_image0.jpg')\n",
    "\t# load model\n",
    "\tmodel = load_model('final_model.h5')\n",
    "\t# predict the class\n",
    "\tresult = model.predict(img)\n",
    "\tprint(result[0])\n",
    "\n",
    "# entry point, run the example\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4750fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.51204129e-07 5.95053680e-05 1.15130604e-06 3.67164254e-01\n",
      " 4.71575670e-02 4.37799603e-01 8.60867500e-02 9.16296412e-05\n",
      " 7.56945084e-08 1.62069598e-04 5.85241360e-04 3.61599289e-02\n",
      " 1.79140363e-06 6.75600086e-06 1.14883775e-07 2.44529769e-02\n",
      " 6.49470977e-10 2.57310807e-04 1.23761483e-05]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for a new image.\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "\t# load the image\n",
    "\timg = load_img(filename, target_size=(256, 256))\n",
    "\t# convert to array\n",
    "\timg = img_to_array(img)\n",
    "\t# reshape into a single sample with 3 channels\n",
    "\timg = img.reshape(1, 256, 256, 3)\n",
    "\t# center pixel data\n",
    "\timg = img.astype('float32')\n",
    "\timg = img - [123.68, 116.779, 103.939]\n",
    "\treturn img\n",
    "\n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "\t# load the image\n",
    "\timg = load_image('sample_image0.jpg')\n",
    "\t# load model\n",
    "\tmodel = load_model('512 layer 50 epoch.h5')\n",
    "\t# predict the class\n",
    "\tresult = model.predict(img)\n",
    "\tprint(result[0])\n",
    "\n",
    "# entry point, run the example\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fc350",
   "metadata": {},
   "source": [
    "### Fold - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77a36326",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = np.load('pannuke\\\\Fold_2\\\\images\\\\images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ede1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = np.load('pannuke\\\\Fold_2\\\\images\\\\types.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af624cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2523, 256, 256, 3), (2523,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2.shape, y_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d70fb3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Breast', 'Breast', 'Breast', ..., 'Colon', 'Colon', 'Colon'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0c660cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values : \n",
      "Number of unique Types in the Types1:  19\n"
     ]
    }
   ],
   "source": [
    "uniqueTypes2 = np.unique(y_train_2)\n",
    "print('Unique Values : ', )\n",
    "\n",
    "number_of_uniqueTypes2 = len(uniqueTypes2)\n",
    "\n",
    "print(\"Number of unique Types in the Types1: \", number_of_uniqueTypes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e08945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (2523,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Breast'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2ec14a992de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape before one-hot encoding: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mY_train_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m#Y_test = np_utils.to_categorical(y_test, n_classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape after one-hot encoding: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_DL_projects\\project3\\env\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Breast'"
     ]
    }
   ],
   "source": [
    "# keras imports for the dataset and building our neural network\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 19\n",
    "print(\"Shape before one-hot encoding: \", y_train_2.shape)\n",
    "Y_train_2 = np_utils.to_categorical(y_train_2, n_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154d2e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 5 5 5]\n",
      "['Breast' 'Breast' 'Breast' ... 'Colon' 'Colon' 'Colon']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder2 = LabelEncoder()\n",
    "integer_encoded2 = label_encoder2.fit_transform(y_train_2)\n",
    "print(integer_encoded2)\n",
    "\n",
    "print(y_train_2)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder2 = OneHotEncoder(sparse=False)\n",
    "integer_encoded2 = integer_encoded2.reshape(len(integer_encoded2), 1)\n",
    "onehot_encoded2 = onehot_encoder2.fit_transform(integer_encoded2)\n",
    "print(onehot_encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cfc1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = onehot_encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c714eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.DataFrame(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcd5fe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>Colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>Colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>Colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>Colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>Colon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2523 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0     Breast\n",
       "1     Breast\n",
       "2     Breast\n",
       "3     Breast\n",
       "4     Breast\n",
       "...      ...\n",
       "2518   Colon\n",
       "2519   Colon\n",
       "2520   Colon\n",
       "2521   Colon\n",
       "2522   Colon\n",
       "\n",
       "[2523 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4054a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0  Breast\n",
       "1  Breast\n",
       "2  Breast"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18fa93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c68dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1, step=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35fa1a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  3\n",
       "1  3\n",
       "2  3\n",
       "3  3\n",
       "4  3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: create a LabelEncoder object and fit it to each feature in X\n",
    "\n",
    "\n",
    "# 1. INSTANTIATE\n",
    "# encode labels with value between 0 and n_classes-1.\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "# 2/3. FIT AND TRANSFORM\n",
    "# use df.apply() to apply le.fit_transform to all columns\n",
    "dfm2 = dfm.apply(le.fit_transform)\n",
    "dfm2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdc27993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2523, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c51c4a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2523, 19)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: create a OneHotEncoder object, and fit it to all of X\n",
    "\n",
    "# 1. INSTANTIATE\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# 2. FIT\n",
    "enc.fit(dfm2)\n",
    "\n",
    "# 3. Transform\n",
    "onehotlabels = enc.transform(dfm2).toarray()\n",
    "onehotlabels.shape\n",
    "\n",
    "# as you can see, you've the same number of rows 891\n",
    "# but now you've so many more columns due to how we changed all the categorical data into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c80cee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0ccdf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(onehotlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ab8cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_22 = onehotlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a8584",
   "metadata": {},
   "source": [
    "### modeling starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bbf4666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_22' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8fe77c8db423>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# entry point, run the test harness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mrun_test_harness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-8fe77c8db423>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m123.68\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.779\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m103.939\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_22' is not defined"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 50 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_2, y_train_22, epochs=50, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_22, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('512 layer 50 epoch fold2.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34b8f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2523/2523 [==============================] - 38s 15ms/step - loss: 8.0944 - accuracy: 0.3746\n",
      "Epoch 2/100\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 1.5976 - accuracy: 0.5264\n",
      "Epoch 3/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 1.3222 - accuracy: 0.5874\n",
      "Epoch 4/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 1.1786 - accuracy: 0.6239\n",
      "Epoch 5/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 1.0702 - accuracy: 0.6465\n",
      "Epoch 6/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.9613 - accuracy: 0.6821\n",
      "Epoch 7/100\n",
      "2523/2523 [==============================] - 27s 11ms/step - loss: 0.8599 - accuracy: 0.7111\n",
      "Epoch 8/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.7769 - accuracy: 0.7451\n",
      "Epoch 9/100\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.6696 - accuracy: 0.7800\n",
      "Epoch 10/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.6127 - accuracy: 0.8022\n",
      "Epoch 11/100\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.5471 - accuracy: 0.8193\n",
      "Epoch 12/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.5129 - accuracy: 0.8339\n",
      "Epoch 13/100\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.4497 - accuracy: 0.8561\n",
      "Epoch 14/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.3775 - accuracy: 0.8732\n",
      "Epoch 15/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.3868 - accuracy: 0.8732\n",
      "Epoch 16/100\n",
      "2523/2523 [==============================] - 29s 11ms/step - loss: 0.3297 - accuracy: 0.8946\n",
      "Epoch 17/100\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.3038 - accuracy: 0.8973\n",
      "Epoch 18/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2369 - accuracy: 0.9227\n",
      "Epoch 19/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.2185 - accuracy: 0.9259\n",
      "Epoch 20/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1805 - accuracy: 0.9346\n",
      "Epoch 21/100\n",
      "2523/2523 [==============================] - 31s 12ms/step - loss: 0.1613 - accuracy: 0.9437\n",
      "Epoch 22/100\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.1488 - accuracy: 0.9445\n",
      "Epoch 23/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.1200 - accuracy: 0.9596\n",
      "Epoch 24/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.1006 - accuracy: 0.9663\n",
      "Epoch 25/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.1021 - accuracy: 0.9683\n",
      "Epoch 26/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0711 - accuracy: 0.9734\n",
      "Epoch 27/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0677 - accuracy: 0.9758\n",
      "Epoch 28/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0698 - accuracy: 0.9758\n",
      "Epoch 29/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0645 - accuracy: 0.9770\n",
      "Epoch 30/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0560 - accuracy: 0.9802\n",
      "Epoch 31/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0517 - accuracy: 0.9830\n",
      "Epoch 32/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0452 - accuracy: 0.9845\n",
      "Epoch 33/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0393 - accuracy: 0.9857\n",
      "Epoch 34/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0370 - accuracy: 0.9873\n",
      "Epoch 35/100\n",
      "2523/2523 [==============================] - 30s 12ms/step - loss: 0.0312 - accuracy: 0.9885\n",
      "Epoch 36/100\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0356 - accuracy: 0.9881\n",
      "Epoch 37/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0349 - accuracy: 0.9869\n",
      "Epoch 38/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0434 - accuracy: 0.9857\n",
      "Epoch 39/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0420 - accuracy: 0.9869\n",
      "Epoch 40/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0454 - accuracy: 0.9849\n",
      "Epoch 41/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0280 - accuracy: 0.9909\n",
      "Epoch 42/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0190 - accuracy: 0.9941\n",
      "Epoch 43/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0173 - accuracy: 0.9948\n",
      "Epoch 44/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0178 - accuracy: 0.9952\n",
      "Epoch 45/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0131 - accuracy: 0.9952\n",
      "Epoch 46/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0112 - accuracy: 0.9960\n",
      "Epoch 47/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0101 - accuracy: 0.9956\n",
      "Epoch 48/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0095 - accuracy: 0.9960\n",
      "Epoch 49/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0097 - accuracy: 0.9972\n",
      "Epoch 50/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0092 - accuracy: 0.9964\n",
      "Epoch 51/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0096 - accuracy: 0.9972\n",
      "Epoch 52/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0078 - accuracy: 0.9972\n",
      "Epoch 53/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0088 - accuracy: 0.9972\n",
      "Epoch 54/100\n",
      "2523/2523 [==============================] - 26s 10ms/step - loss: 0.0070 - accuracy: 0.9980\n",
      "Epoch 55/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0066 - accuracy: 0.9976\n",
      "Epoch 56/100\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 0.0075 - accuracy: 0.9972\n",
      "Epoch 57/100\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 0.0124 - accuracy: 0.9968\n",
      "Epoch 58/100\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 0.0087 - accuracy: 0.9972\n",
      "Epoch 59/100\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 0.0063 - accuracy: 0.9980\n",
      "Epoch 60/100\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 0.0073 - accuracy: 0.9972\n",
      "Epoch 61/100\n",
      "2523/2523 [==============================] - 24s 10ms/step - loss: 0.0062 - accuracy: 0.9984\n",
      "Epoch 62/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0053 - accuracy: 0.9984\n",
      "Epoch 63/100\n",
      "2523/2523 [==============================] - 34s 14ms/step - loss: 0.0049 - accuracy: 0.9984\n",
      "Epoch 64/100\n",
      "2523/2523 [==============================] - 33s 13ms/step - loss: 0.0058 - accuracy: 0.9984\n",
      "Epoch 65/100\n",
      "2523/2523 [==============================] - 35s 14ms/step - loss: 0.0050 - accuracy: 0.9984\n",
      "Epoch 66/100\n",
      "2523/2523 [==============================] - 29s 12ms/step - loss: 0.0045 - accuracy: 0.9984\n",
      "Epoch 67/100\n",
      "2523/2523 [==============================] - 28s 11ms/step - loss: 0.0045 - accuracy: 0.9984\n",
      "Epoch 68/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0047 - accuracy: 0.9980\n",
      "Epoch 69/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0049 - accuracy: 0.9984\n",
      "Epoch 70/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0046 - accuracy: 0.9984\n",
      "Epoch 71/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0045 - accuracy: 0.9984\n",
      "Epoch 72/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0045 - accuracy: 0.9984\n",
      "Epoch 73/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0044 - accuracy: 0.9984\n",
      "Epoch 74/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 75/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 76/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 77/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 78/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 79/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 80/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 81/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0043 - accuracy: 0.9984\n",
      "Epoch 82/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0051 - accuracy: 0.9980\n",
      "Epoch 83/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0049 - accuracy: 0.9984\n",
      "Epoch 84/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0038 - accuracy: 0.9984\n",
      "Epoch 85/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0041 - accuracy: 0.9984\n",
      "Epoch 86/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0033 - accuracy: 0.9988\n",
      "Epoch 87/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0035 - accuracy: 0.9988\n",
      "Epoch 88/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0031 - accuracy: 0.9988\n",
      "Epoch 89/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0028 - accuracy: 0.9988\n",
      "Epoch 90/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0028 - accuracy: 0.9988\n",
      "Epoch 91/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0028 - accuracy: 0.9988\n",
      "Epoch 92/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0029 - accuracy: 0.9988\n",
      "Epoch 93/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0027 - accuracy: 0.9988\n",
      "Epoch 94/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0028 - accuracy: 0.9988\n",
      "Epoch 95/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0027 - accuracy: 0.9988\n",
      "Epoch 96/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0028 - accuracy: 0.9988\n",
      "Epoch 97/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0028 - accuracy: 0.9988\n",
      "Epoch 98/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0026 - accuracy: 0.9988\n",
      "Epoch 99/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0027 - accuracy: 0.9988\n",
      "Epoch 100/100\n",
      "2523/2523 [==============================] - 25s 10ms/step - loss: 0.0028 - accuracy: 0.9988\n",
      "2523/2523 [==============================] - 25s 10ms/step\n",
      "> 99.881\n"
     ]
    }
   ],
   "source": [
    "# vgg16 model used for transfer learning  with 100 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train_2, y_train_22, epochs=100, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_22, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('512 layer 100 epoch fold2.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20315b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8790132e-12 2.5115553e-23 2.5540498e-21 7.2371035e-16 4.3835748e-09\n",
      " 7.3878944e-02 1.2142175e-22 1.7100879e-08 2.2508177e-06 7.5444537e-01\n",
      " 1.7167346e-01 7.7009822e-11 1.7317187e-15 2.5683598e-16 2.6214030e-25\n",
      " 2.1206356e-13 1.1878241e-15 7.3321333e-25 7.3668834e-14]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for a new image.\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "\t# load the image\n",
    "\timg = load_img(filename, target_size=(256, 256))\n",
    "\t# convert to array\n",
    "\timg = img_to_array(img)\n",
    "\t# reshape into a single sample with 3 channels\n",
    "\timg = img.reshape(1, 256, 256, 3)\n",
    "\t# center pixel data\n",
    "\timg = img.astype('float32')\n",
    "\timg = img - [123.68, 116.779, 103.939]\n",
    "\treturn img\n",
    "\n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "\t# load the image\n",
    "\timg = load_image('SOB_M_DC-14-2523-400-005.png')\n",
    "\t# load model\n",
    "\tmodel = load_model('512 layer 100 epoch fold2.h5')\n",
    "\t# predict the class\n",
    "\tresult = model.predict(img)\n",
    "\tprint(result[0])\n",
    "\n",
    "# entry point, run the example\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a1a04",
   "metadata": {},
   "source": [
    "# VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6261bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 287s 4us/step\n",
      "Epoch 1/20\n",
      "2656/2656 [==============================] - 45s 17ms/step - loss: 5.4325 - accuracy: 0.3742\n",
      "Epoch 2/20\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.6800 - accuracy: 0.4876\n",
      "Epoch 3/20\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 1.5583 - accuracy: 0.5113\n",
      "Epoch 4/20\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.4836 - accuracy: 0.5260\n",
      "Epoch 5/20\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.4258 - accuracy: 0.5403\n",
      "Epoch 6/20\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 1.3603 - accuracy: 0.5565\n",
      "Epoch 7/20\n",
      "2656/2656 [==============================] - 43s 16ms/step - loss: 1.3770 - accuracy: 0.5535\n",
      "Epoch 8/20\n",
      "2656/2656 [==============================] - 44s 17ms/step - loss: 1.3725 - accuracy: 0.5670\n",
      "Epoch 9/20\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 1.2415 - accuracy: 0.5870\n",
      "Epoch 10/20\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 1.1844 - accuracy: 0.5994\n",
      "Epoch 11/20\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 1.1652 - accuracy: 0.6099\n",
      "Epoch 12/20\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 1.1271 - accuracy: 0.6171\n",
      "Epoch 13/20\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.0933 - accuracy: 0.6340\n",
      "Epoch 14/20\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 1.0159 - accuracy: 0.6510\n",
      "Epoch 15/20\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.9493 - accuracy: 0.6747\n",
      "Epoch 16/20\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.9139 - accuracy: 0.6856\n",
      "Epoch 17/20\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.8644 - accuracy: 0.7029\n",
      "Epoch 18/20\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.8312 - accuracy: 0.7236\n",
      "Epoch 19/20\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.7758 - accuracy: 0.7300\n",
      "Epoch 20/20\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.7003 - accuracy: 0.7553\n",
      "2523/2523 [==============================] - 42s 17ms/step\n",
      "> 51.407\n"
     ]
    }
   ],
   "source": [
    "# vgg19 model used for transfer learning  with 20 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(256, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=20, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('256 layer 20 epoch vgg19.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ea21cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 7.0317 - accuracy: 0.3524\n",
      "Epoch 2/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.7055 - accuracy: 0.4782\n",
      "Epoch 3/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.5766 - accuracy: 0.5271\n",
      "Epoch 4/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.4823 - accuracy: 0.5580\n",
      "Epoch 5/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.4315 - accuracy: 0.5689\n",
      "Epoch 6/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.3724 - accuracy: 0.5809\n",
      "Epoch 7/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.3173 - accuracy: 0.5998\n",
      "Epoch 8/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.2720 - accuracy: 0.6096\n",
      "Epoch 9/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.2350 - accuracy: 0.6258\n",
      "Epoch 10/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.1968 - accuracy: 0.6340\n",
      "Epoch 11/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.1273 - accuracy: 0.6438\n",
      "Epoch 12/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.0898 - accuracy: 0.6623\n",
      "Epoch 13/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 1.0482 - accuracy: 0.6653\n",
      "Epoch 14/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 1.0185 - accuracy: 0.6755\n",
      "Epoch 15/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.9879 - accuracy: 0.6796\n",
      "Epoch 16/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.9654 - accuracy: 0.6913\n",
      "Epoch 17/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.9236 - accuracy: 0.7059\n",
      "Epoch 18/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.8931 - accuracy: 0.7018\n",
      "Epoch 19/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.8596 - accuracy: 0.7105\n",
      "Epoch 20/200\n",
      "2656/2656 [==============================] - 41s 16ms/step - loss: 0.8122 - accuracy: 0.7285\n",
      "Epoch 21/200\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.8127 - accuracy: 0.7319\n",
      "Epoch 22/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.7598 - accuracy: 0.7349\n",
      "Epoch 23/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.7273 - accuracy: 0.7500\n",
      "Epoch 24/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.7032 - accuracy: 0.7579\n",
      "Epoch 25/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.6761 - accuracy: 0.7658\n",
      "Epoch 26/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.6143 - accuracy: 0.7861\n",
      "Epoch 27/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.6006 - accuracy: 0.7858\n",
      "Epoch 28/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.5985 - accuracy: 0.7895\n",
      "Epoch 29/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.5612 - accuracy: 0.7952\n",
      "Epoch 30/200\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.5576 - accuracy: 0.7978\n",
      "Epoch 31/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.5502 - accuracy: 0.8035\n",
      "Epoch 32/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.5224 - accuracy: 0.8042\n",
      "Epoch 33/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.5290 - accuracy: 0.8065\n",
      "Epoch 34/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.5101 - accuracy: 0.8114\n",
      "Epoch 35/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.4935 - accuracy: 0.8178\n",
      "Epoch 36/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.5096 - accuracy: 0.8151\n",
      "Epoch 37/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.5107 - accuracy: 0.8125\n",
      "Epoch 38/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.4919 - accuracy: 0.8230\n",
      "Epoch 39/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.5161 - accuracy: 0.8136\n",
      "Epoch 40/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.4719 - accuracy: 0.8276\n",
      "Epoch 41/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4583 - accuracy: 0.8287\n",
      "Epoch 42/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4549 - accuracy: 0.8351\n",
      "Epoch 43/200\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 0.4638 - accuracy: 0.8287\n",
      "Epoch 44/200\n",
      "2656/2656 [==============================] - 43s 16ms/step - loss: 0.4412 - accuracy: 0.8355\n",
      "Epoch 45/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4282 - accuracy: 0.8404\n",
      "Epoch 46/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4254 - accuracy: 0.8438\n",
      "Epoch 47/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.4368 - accuracy: 0.8362\n",
      "Epoch 48/200\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.4234 - accuracy: 0.8407\n",
      "Epoch 49/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4191 - accuracy: 0.8407\n",
      "Epoch 50/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.4210 - accuracy: 0.8422\n",
      "Epoch 51/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.4776 - accuracy: 0.8355\n",
      "Epoch 52/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4310 - accuracy: 0.8434\n",
      "Epoch 53/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.4222 - accuracy: 0.8449\n",
      "Epoch 54/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4342 - accuracy: 0.8434\n",
      "Epoch 55/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4245 - accuracy: 0.8460\n",
      "Epoch 56/200\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.4265 - accuracy: 0.8475\n",
      "Epoch 57/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4144 - accuracy: 0.8445\n",
      "Epoch 58/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4209 - accuracy: 0.8441\n",
      "Epoch 59/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.4101 - accuracy: 0.8483\n",
      "Epoch 60/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4466 - accuracy: 0.8441\n",
      "Epoch 61/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.4389 - accuracy: 0.8426\n",
      "Epoch 62/200\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 0.4076 - accuracy: 0.8596\n",
      "Epoch 63/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.4056 - accuracy: 0.8690\n",
      "Epoch 64/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.4439 - accuracy: 0.8554\n",
      "Epoch 65/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4149 - accuracy: 0.8633\n",
      "Epoch 66/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.4126 - accuracy: 0.8611\n",
      "Epoch 67/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3753 - accuracy: 0.8735\n",
      "Epoch 68/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3632 - accuracy: 0.8754\n",
      "Epoch 69/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3785 - accuracy: 0.8709\n",
      "Epoch 70/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.3678 - accuracy: 0.8773\n",
      "Epoch 71/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3733 - accuracy: 0.8739\n",
      "Epoch 72/200\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.4548 - accuracy: 0.8566\n",
      "Epoch 73/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4116 - accuracy: 0.8690\n",
      "Epoch 74/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.4021 - accuracy: 0.8678\n",
      "Epoch 75/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3796 - accuracy: 0.8758\n",
      "Epoch 76/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3972 - accuracy: 0.8761\n",
      "Epoch 77/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.4629 - accuracy: 0.8577\n",
      "Epoch 78/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3877 - accuracy: 0.8720\n",
      "Epoch 79/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.3863 - accuracy: 0.8731\n",
      "Epoch 80/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3874 - accuracy: 0.8727\n",
      "Epoch 81/200\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.3679 - accuracy: 0.8746\n",
      "Epoch 82/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3388 - accuracy: 0.8848\n",
      "Epoch 83/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.3529 - accuracy: 0.8840\n",
      "Epoch 84/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3457 - accuracy: 0.8893\n",
      "Epoch 85/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3330 - accuracy: 0.8916\n",
      "Epoch 86/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3183 - accuracy: 0.8927\n",
      "Epoch 87/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3126 - accuracy: 0.8938\n",
      "Epoch 88/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3064 - accuracy: 0.8938\n",
      "Epoch 89/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3275 - accuracy: 0.8908\n",
      "Epoch 90/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2977 - accuracy: 0.8983\n",
      "Epoch 91/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2995 - accuracy: 0.8972\n",
      "Epoch 92/200\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.3058 - accuracy: 0.8942\n",
      "Epoch 93/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3134 - accuracy: 0.8919\n",
      "Epoch 94/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2959 - accuracy: 0.9002\n",
      "Epoch 95/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3126 - accuracy: 0.8961\n",
      "Epoch 96/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3164 - accuracy: 0.8916\n",
      "Epoch 97/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3266 - accuracy: 0.8927\n",
      "Epoch 98/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3373 - accuracy: 0.8908\n",
      "Epoch 99/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3327 - accuracy: 0.8923\n",
      "Epoch 100/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3296 - accuracy: 0.8931\n",
      "Epoch 101/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3203 - accuracy: 0.8965\n",
      "Epoch 102/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.3275 - accuracy: 0.8957\n",
      "Epoch 103/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3309 - accuracy: 0.8916\n",
      "Epoch 104/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3719 - accuracy: 0.8855\n",
      "Epoch 105/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3802 - accuracy: 0.8855\n",
      "Epoch 106/200\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 0.3858 - accuracy: 0.8840\n",
      "Epoch 107/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.3213 - accuracy: 0.8987\n",
      "Epoch 108/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3339 - accuracy: 0.8916\n",
      "Epoch 109/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2984 - accuracy: 0.9032\n",
      "Epoch 110/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.3196 - accuracy: 0.8934\n",
      "Epoch 111/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2985 - accuracy: 0.8998\n",
      "Epoch 112/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2810 - accuracy: 0.9044\n",
      "Epoch 113/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2689 - accuracy: 0.9074\n",
      "Epoch 114/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2663 - accuracy: 0.9093\n",
      "Epoch 115/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2699 - accuracy: 0.9055\n",
      "Epoch 116/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2783 - accuracy: 0.9047\n",
      "Epoch 117/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2640 - accuracy: 0.9104\n",
      "Epoch 118/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2952 - accuracy: 0.9025\n",
      "Epoch 119/200\n",
      "2656/2656 [==============================] - 41s 16ms/step - loss: 0.2705 - accuracy: 0.9111\n",
      "Epoch 120/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2600 - accuracy: 0.9093\n",
      "Epoch 121/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2601 - accuracy: 0.9089\n",
      "Epoch 122/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2571 - accuracy: 0.9089\n",
      "Epoch 123/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2769 - accuracy: 0.9085\n",
      "Epoch 124/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2882 - accuracy: 0.9081\n",
      "Epoch 125/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3065 - accuracy: 0.9021\n",
      "Epoch 126/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.4641 - accuracy: 0.8840\n",
      "Epoch 127/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3496 - accuracy: 0.8927\n",
      "Epoch 128/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3239 - accuracy: 0.9059\n",
      "Epoch 129/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3248 - accuracy: 0.9040\n",
      "Epoch 130/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3151 - accuracy: 0.9006\n",
      "Epoch 131/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.3322 - accuracy: 0.8980\n",
      "Epoch 132/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3037 - accuracy: 0.9066\n",
      "Epoch 133/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2780 - accuracy: 0.9093\n",
      "Epoch 134/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2910 - accuracy: 0.9074\n",
      "Epoch 135/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.3126 - accuracy: 0.9062\n",
      "Epoch 136/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2974 - accuracy: 0.9070\n",
      "Epoch 137/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2901 - accuracy: 0.9070\n",
      "Epoch 138/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2735 - accuracy: 0.9115\n",
      "Epoch 139/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2785 - accuracy: 0.9115\n",
      "Epoch 140/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3149 - accuracy: 0.9025\n",
      "Epoch 141/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2773 - accuracy: 0.9119\n",
      "Epoch 142/200\n",
      "2656/2656 [==============================] - 49s 19ms/step - loss: 0.2852 - accuracy: 0.9070\n",
      "Epoch 143/200\n",
      "2656/2656 [==============================] - 49s 18ms/step - loss: 0.2596 - accuracy: 0.9134\n",
      "Epoch 144/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2685 - accuracy: 0.9145\n",
      "Epoch 145/200\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.2755 - accuracy: 0.9164\n",
      "Epoch 146/200\n",
      "2656/2656 [==============================] - 43s 16ms/step - loss: 0.3269 - accuracy: 0.9029\n",
      "Epoch 147/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2949 - accuracy: 0.9078\n",
      "Epoch 148/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2771 - accuracy: 0.9085\n",
      "Epoch 149/200\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.2799 - accuracy: 0.9093\n",
      "Epoch 150/200\n",
      "2656/2656 [==============================] - 41s 16ms/step - loss: 0.3018 - accuracy: 0.9085\n",
      "Epoch 151/200\n",
      "2656/2656 [==============================] - 42s 16ms/step - loss: 0.2956 - accuracy: 0.9111\n",
      "Epoch 152/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2704 - accuracy: 0.9130\n",
      "Epoch 153/200\n",
      "2656/2656 [==============================] - 40s 15ms/step - loss: 0.2735 - accuracy: 0.9115\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2972 - accuracy: 0.9127\n",
      "Epoch 155/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2977 - accuracy: 0.9104\n",
      "Epoch 156/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2701 - accuracy: 0.9149\n",
      "Epoch 157/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2616 - accuracy: 0.9149\n",
      "Epoch 158/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2718 - accuracy: 0.9157\n",
      "Epoch 159/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2605 - accuracy: 0.9160\n",
      "Epoch 160/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2603 - accuracy: 0.9153\n",
      "Epoch 161/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2871 - accuracy: 0.9089\n",
      "Epoch 162/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3139 - accuracy: 0.9085\n",
      "Epoch 163/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.2960 - accuracy: 0.9138\n",
      "Epoch 164/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2542 - accuracy: 0.9164\n",
      "Epoch 165/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2331 - accuracy: 0.9194\n",
      "Epoch 166/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2262 - accuracy: 0.9255\n",
      "Epoch 167/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.2187 - accuracy: 0.9251\n",
      "Epoch 168/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2286 - accuracy: 0.9221\n",
      "Epoch 169/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2295 - accuracy: 0.9232\n",
      "Epoch 170/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.2484 - accuracy: 0.9187\n",
      "Epoch 171/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2749 - accuracy: 0.9168\n",
      "Epoch 172/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2683 - accuracy: 0.9145\n",
      "Epoch 173/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2815 - accuracy: 0.9168\n",
      "Epoch 174/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.2673 - accuracy: 0.9202\n",
      "Epoch 175/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2470 - accuracy: 0.9213\n",
      "Epoch 176/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2486 - accuracy: 0.9224\n",
      "Epoch 177/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2911 - accuracy: 0.9119\n",
      "Epoch 178/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2790 - accuracy: 0.9157\n",
      "Epoch 179/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.3685 - accuracy: 0.9062\n",
      "Epoch 180/200\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.3530 - accuracy: 0.9123\n",
      "Epoch 181/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.3496 - accuracy: 0.9074\n",
      "Epoch 182/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3316 - accuracy: 0.9123\n",
      "Epoch 183/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3167 - accuracy: 0.9078\n",
      "Epoch 184/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.3679 - accuracy: 0.9089\n",
      "Epoch 185/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3469 - accuracy: 0.9085\n",
      "Epoch 186/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.3408 - accuracy: 0.9036\n",
      "Epoch 187/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3030 - accuracy: 0.9160\n",
      "Epoch 188/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.3500 - accuracy: 0.9130\n",
      "Epoch 189/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.4465 - accuracy: 0.8950\n",
      "Epoch 190/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.3748 - accuracy: 0.8995\n",
      "Epoch 191/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.2778 - accuracy: 0.9153\n",
      "Epoch 192/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.2713 - accuracy: 0.9175\n",
      "Epoch 193/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.2820 - accuracy: 0.9172\n",
      "Epoch 194/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.3178 - accuracy: 0.9085\n",
      "Epoch 195/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.3164 - accuracy: 0.9130\n",
      "Epoch 196/200\n",
      "2656/2656 [==============================] - 36s 13ms/step - loss: 0.2731 - accuracy: 0.9194\n",
      "Epoch 197/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.2881 - accuracy: 0.9157\n",
      "Epoch 198/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3204 - accuracy: 0.9119\n",
      "Epoch 199/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3004 - accuracy: 0.9172\n",
      "Epoch 200/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.2403 - accuracy: 0.9228\n",
      "2523/2523 [==============================] - 44s 17ms/step\n",
      "> 50.218\n"
     ]
    }
   ],
   "source": [
    "# vgg19 model used for transfer learning  with 200 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(256, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=200, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('256 layer 200 epoch vgg19.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "227a4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2656/2656 [==============================] - 48s 18ms/step - loss: 7.0562 - accuracy: 0.3630\n",
      "Epoch 2/200\n",
      "2656/2656 [==============================] - 41s 15ms/step - loss: 1.5528 - accuracy: 0.5230\n",
      "Epoch 3/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 1.3555 - accuracy: 0.5885\n",
      "Epoch 4/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 1.1998 - accuracy: 0.6239\n",
      "Epoch 5/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 1.0723 - accuracy: 0.6570\n",
      "Epoch 6/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.9403 - accuracy: 0.6916\n",
      "Epoch 7/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.8122 - accuracy: 0.7361\n",
      "Epoch 8/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.7203 - accuracy: 0.7556\n",
      "Epoch 9/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.5935 - accuracy: 0.7925\n",
      "Epoch 10/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.5051 - accuracy: 0.8181\n",
      "Epoch 11/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.4107 - accuracy: 0.8566\n",
      "Epoch 12/200\n",
      "2656/2656 [==============================] - 36s 14ms/step - loss: 0.3622 - accuracy: 0.8769\n",
      "Epoch 13/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.2975 - accuracy: 0.8950\n",
      "Epoch 14/200\n",
      "2656/2656 [==============================] - 39s 15ms/step - loss: 0.2351 - accuracy: 0.9175\n",
      "Epoch 15/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.1614 - accuracy: 0.9480\n",
      "Epoch 16/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.1131 - accuracy: 0.9612\n",
      "Epoch 17/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0957 - accuracy: 0.9661\n",
      "Epoch 18/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0558 - accuracy: 0.9816\n",
      "Epoch 19/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0409 - accuracy: 0.9872\n",
      "Epoch 20/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0258 - accuracy: 0.9925\n",
      "Epoch 21/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0255 - accuracy: 0.9932\n",
      "Epoch 22/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0168 - accuracy: 0.9951\n",
      "Epoch 23/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0121 - accuracy: 0.9970\n",
      "Epoch 24/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 25/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0085 - accuracy: 0.9970\n",
      "Epoch 26/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 27/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0063 - accuracy: 0.9981\n",
      "Epoch 28/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 29/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0055 - accuracy: 0.9985\n",
      "Epoch 30/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 31/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0052 - accuracy: 0.9985\n",
      "Epoch 32/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0050 - accuracy: 0.9985\n",
      "Epoch 33/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0049 - accuracy: 0.9985\n",
      "Epoch 34/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0048 - accuracy: 0.9985\n",
      "Epoch 35/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0046 - accuracy: 0.9985\n",
      "Epoch 36/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0042 - accuracy: 0.9985\n",
      "Epoch 37/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0037 - accuracy: 0.9989\n",
      "Epoch 38/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 39/200\n",
      "2656/2656 [==============================] - 38s 14ms/step - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 40/200\n",
      "2656/2656 [==============================] - 37s 14ms/step - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 41/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0024 - accuracy: 0.9992\n",
      "Epoch 42/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0023 - accuracy: 0.9992\n",
      "Epoch 43/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0021 - accuracy: 0.9996\n",
      "Epoch 44/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0020 - accuracy: 0.9992\n",
      "Epoch 45/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 46/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 47/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 48/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 49/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 50/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 51/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 52/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 53/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 54/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 55/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 56/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 57/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 58/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 59/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 60/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 61/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 62/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 63/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 64/200\n",
      "2656/2656 [==============================] - 33s 13ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 65/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 66/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 67/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 68/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 69/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 70/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 71/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 72/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 73/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 74/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 75/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 76/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 77/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 78/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 79/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 80/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 81/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 82/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 83/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 84/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 85/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 86/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 87/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 88/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 89/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 90/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 91/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 92/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 93/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 94/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 95/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 96/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 97/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 98/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 99/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 100/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 101/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 102/200\n",
      "2656/2656 [==============================] - 34s 13ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 103/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 104/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 105/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 106/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 107/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 108/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 109/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 110/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 111/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 112/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 113/200\n",
      "2656/2656 [==============================] - 35s 13ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 114/200\n",
      "2656/2656 [==============================] - 33s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 115/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 116/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 117/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 118/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 119/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 120/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 121/200\n",
      "2656/2656 [==============================] - 32s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 122/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 123/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 124/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 125/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 126/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 127/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 128/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 129/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 130/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 131/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 132/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 133/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 134/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 135/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 136/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 137/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 138/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 139/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 140/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 141/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 142/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 143/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 144/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 145/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 146/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 147/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 148/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 149/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 150/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 151/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 152/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 153/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 155/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 156/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 157/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 158/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 159/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 160/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 161/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 162/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 163/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 164/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 165/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 166/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 167/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 168/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 169/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 170/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 171/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 172/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 173/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 174/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 175/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 176/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 177/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 178/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 179/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 180/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 181/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 182/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 183/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 184/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 185/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 186/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 187/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 188/200\n",
      "2656/2656 [==============================] - 31s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 189/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 190/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 191/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 192/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 193/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 194/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 195/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 196/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 197/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 198/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 199/200\n",
      "2656/2656 [==============================] - 31s 12ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "Epoch 200/200\n",
      "2656/2656 [==============================] - 30s 11ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "2523/2523 [==============================] - 40s 16ms/step\n",
      "> 56.203\n"
     ]
    }
   ],
   "source": [
    "# vgg19 model used for transfer learning  with 200 epoch\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\t# load model\n",
    "\tmodel = VGG19(include_top=False, input_shape=(256, 256, 3))\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(19, activation='softmax')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t#model.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# fit model\n",
    "\thistory = model.fit(x_train, y_train2, epochs=200, verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(x_train_2, y_train_2, verbose=1)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# save model\n",
    "\tmodel.save('512 layer 200 epoch vgg19.h5')\n",
    "\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56357433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab2478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e2450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da40ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1494e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25dad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8010f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffe53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece2b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
